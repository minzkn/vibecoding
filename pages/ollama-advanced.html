<!DOCTYPE html>
<html lang="ko" data-theme="dark-kernel">
<head>
<!-- BEGIN: Google adsense -->
<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-2110881342960271" crossorigin="anonymous"></script>
<!-- END: Google adsense -->
<!-- BEGIN: Google adsense repair -->
<script async src="https://fundingchoicesmessages.google.com/i/pub-2110881342960271?ers=1" nonce="laI6FT8gpRxugDJv5AGJRA"></script><script nonce="laI6FT8gpRxugDJv5AGJRA">(function() {function signalGooglefcPresent() {if (!window.frames['googlefcPresent']) {if (document.body) {const iframe = document.createElement('iframe'); iframe.style = 'width: 0; height: 0; border: none; z-index: -1000; left: -1000px; top: -1000px;'; iframe.style.display = 'none'; iframe.name = 'googlefcPresent'; document.body.appendChild(iframe);} else {setTimeout(signalGooglefcPresent, 0);}}}signalGooglefcPresent();})();</script>
<!-- END: Google adsense repair -->
<!-- BEGIN: Google analytics -->
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-1VWQF060SX"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-1VWQF060SX');
</script>
<!-- END: Google analytics -->

<script>
(function(){var m=document.cookie.match(/claude_theme=([^;]+)/);if(m)document.documentElement.setAttribute('data-theme',m[1]);})();
</script>
<meta charset="UTF-8">
<meta property="og:type" content="article">
<meta property="og:site_name" content="AI Vibe Coding ê°€ì´ë“œ /with MINZKN">
<meta property="og:title" content="Ollama ê³ ê¸‰ í™œìš©">
<meta property="og:description" content="Ollama ê³ ê¸‰ í™œìš©: Modelfile ì‘ì„±, ì»¤ìŠ¤í…€ ëª¨ë¸ ìƒì„±, RAG íŒŒì´í”„ë¼ì¸, ì—ì´ì „íŠ¸ êµ¬í˜„, í”„ë¡œë•ì…˜ ë°°í¬">
<meta property="og:url" content="https://minzkn.com/claude/pages/ollama-advanced.html">
<meta property="og:image" content="https://minzkn.com/claude/images/og-image.png">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="Ollama ê³ ê¸‰ í™œìš©: Modelfile ì‘ì„±, ì»¤ìŠ¤í…€ ëª¨ë¸ ìƒì„±, RAG íŒŒì´í”„ë¼ì¸, ì—ì´ì „íŠ¸ êµ¬í˜„, í”„ë¡œë•ì…˜ ë°°í¬">
<meta name="keywords" content="Claude, AI, LLM, Ollama ê³ ê¸‰ í™œìš©, Modelfile ì‘ì„±, ì»¤ìŠ¤í…€ ëª¨ë¸ ìƒì„±, RAG íŒŒì´í”„ë¼ì¸ êµ¬ì¶•, ì—ì´ì „íŠ¸ êµ¬í˜„">
<meta name="author" content="MINZKN">
<title>Ollama ê³ ê¸‰ í™œìš© - AI Vibe Coding ê°€ì´ë“œ /with MINZKN</title>
<link rel="icon" type="image/svg+xml" href="../images/favicon.svg">
<link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/joungkyun/font-d2coding/d2coding.css">
<link rel="stylesheet" href="../css/themes.css">
<link rel="stylesheet" href="../css/style.css">
<link rel="stylesheet" href="../css/responsive.css">
</head>
<body>
<div class="page-wrapper">

<header class="site-header"></header>
<nav class="side-nav" aria-label="ì‚¬ì´íŠ¸ ë‚´ë¹„ê²Œì´ì…˜"></nav>

<main class="main-content">
<nav class="breadcrumb"></nav>

<h1 id="top">Ollama ê³ ê¸‰ í™œìš©</h1>
<p class="lead">Modelfile ì‘ì„±, ì»¤ìŠ¤í…€ ëª¨ë¸ ìƒì„±, RAG íŒŒì´í”„ë¼ì¸, ì—ì´ì „íŠ¸ êµ¬í˜„, í”„ë¡œë•ì…˜ ë°°í¬</p>

<div class="info-box warning">
  <strong>ì—…ë°ì´íŠ¸ ì•ˆë‚´:</strong> ëª¨ë¸/ìš”ê¸ˆ/ë²„ì „/ì •ì±… ë“± ì‹œì ì— ë¯¼ê°í•œ ì •ë³´ëŠ” ë³€ë™ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
  ìµœì‹  ë‚´ìš©ì€ ê³µì‹ ë¬¸ì„œë¥¼ í™•ì¸í•˜ì„¸ìš”.
</div>

<div class="info-box tip">
  <div class="info-box-title">ğŸ¯ ì´ ë¬¸ì„œì—ì„œ ë°°ìš¸ ë‚´ìš©</div>
  <ol>
    <li><strong>Modelfile</strong> - ëª¨ë¸ ë™ì‘ì„ ì™„ì „íˆ ì»¤ìŠ¤í„°ë§ˆì´ì§•</li>
    <li><strong>ì»¤ìŠ¤í…€ ëª¨ë¸</strong> - íŠ¹ì • ë„ë©”ì¸ì— íŠ¹í™”ëœ ëª¨ë¸ ìƒì„±</li>
    <li><strong>RAG íŒŒì´í”„ë¼ì¸</strong> - ë¬¸ì„œ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ</li>
    <li><strong>ì—ì´ì „íŠ¸</strong> - ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ëŠ” ììœ¨ AI</li>
    <li><strong>ì„±ëŠ¥ ìµœì í™”</strong> - ì†ë„ì™€ í’ˆì§ˆ ê°œì„ </li>
    <li><strong>í”„ë¡œë•ì…˜ ë°°í¬</strong> - ì‹¤ì œ ì„œë¹„ìŠ¤ ìš´ì˜</li>
  </ol>
</div>

<section class="content-section">
  <h2 id="modelfile">Modelfile ì‘ì„±</h2>

  <p>
    <strong>Modelfile</strong>ì€ Dockerfileê³¼ ìœ ì‚¬í•˜ê²Œ, Ollama ëª¨ë¸ì˜ ë™ì‘ì„ ì •ì˜í•˜ëŠ” ì„¤ì • íŒŒì¼ì…ë‹ˆë‹¤.
    ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸, íŒŒë¼ë¯¸í„°, í…œí”Œë¦¿ ë“±ì„ ì»¤ìŠ¤í„°ë§ˆì´ì§•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
  </p>

  <h3 id="modelfile-basics">ê¸°ë³¸ êµ¬ì¡°</h3>

  <pre><code><span class="cmt"># Modelfile ì˜ˆì‹œ</span>
<span class="kw">FROM</span> llama3.2

<span class="cmt"># ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸</span>
<span class="kw">SYSTEM</span> <span class="str">"""ë‹¹ì‹ ì€ íŒŒì´ì¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ëª¨ë“  ë‹µë³€ì€ PEP 8 ìŠ¤íƒ€ì¼ ê°€ì´ë“œë¥¼ ë”°ë¥´ê³ ,
íƒ€ì… íŒíŠ¸ì™€ docstringì„ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤."""</span>

<span class="cmt"># íŒŒë¼ë¯¸í„° ì„¤ì •</span>
<span class="kw">PARAMETER</span> temperature 0.7
<span class="kw">PARAMETER</span> top_p 0.9
<span class="kw">PARAMETER</span> top_k 40

<span class="cmt"># ë©”ì‹œì§€ í…œí”Œë¦¿</span>
<span class="kw">TEMPLATE</span> <span class="str">"""{{ .System }}

User: {{ .Prompt }}
Assistant:"""</span></code></pre>

  <h3 id="modelfile-commands">Modelfile ëª…ë ¹ì–´</h3>

  <table>
    <thead>
      <tr>
        <th>ëª…ë ¹ì–´</th>
        <th>ì„¤ëª…</th>
        <th>ì˜ˆì‹œ</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><code>FROM</code></td>
        <td>ë² ì´ìŠ¤ ëª¨ë¸ ì§€ì •</td>
        <td><code>FROM llama3.2</code></td>
      </tr>
      <tr>
        <td><code>SYSTEM</code></td>
        <td>ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸</td>
        <td><code>SYSTEM "ë‹¹ì‹ ì€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤"</code></td>
      </tr>
      <tr>
        <td><code>PARAMETER</code></td>
        <td>ëª¨ë¸ íŒŒë¼ë¯¸í„°</td>
        <td><code>PARAMETER temperature 0.8</code></td>
      </tr>
      <tr>
        <td><code>TEMPLATE</code></td>
        <td>ë©”ì‹œì§€ í…œí”Œë¦¿</td>
        <td><code>TEMPLATE "{{ .Prompt }}"</code></td>
      </tr>
      <tr>
        <td><code>MESSAGE</code></td>
        <td>ëŒ€í™” íˆìŠ¤í† ë¦¬</td>
        <td><code>MESSAGE user "ì•ˆë…•"</code></td>
      </tr>
      <tr>
        <td><code>ADAPTER</code></td>
        <td>LoRA ì–´ëŒ‘í„°</td>
        <td><code>ADAPTER ./adapter.bin</code></td>
      </tr>
      <tr>
        <td><code>LICENSE</code></td>
        <td>ë¼ì´ì„ ìŠ¤ ì •ë³´</td>
        <td><code>LICENSE "MIT"</code></td>
      </tr>
    </tbody>
  </table>

  <h3 id="modelfile-parameters">íŒŒë¼ë¯¸í„° ì„¤ì •</h3>

  <table>
    <thead>
      <tr>
        <th>íŒŒë¼ë¯¸í„°</th>
        <th>ê¸°ë³¸ê°’</th>
        <th>ë²”ìœ„</th>
        <th>íš¨ê³¼</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><code>temperature</code></td>
        <td>0.8</td>
        <td>0.0 ~ 2.0</td>
        <td>ë†’ì„ìˆ˜ë¡ ì°½ì˜ì , ë‚®ì„ìˆ˜ë¡ ì¼ê´€ì </td>
      </tr>
      <tr>
        <td><code>top_p</code></td>
        <td>0.9</td>
        <td>0.0 ~ 1.0</td>
        <td>ëˆ„ì  í™•ë¥  ì„ê³„ê°’ (nucleus sampling)</td>
      </tr>
      <tr>
        <td><code>top_k</code></td>
        <td>40</td>
        <td>1 ~ 100</td>
        <td>ìƒìœ„ Kê°œ í† í°ë§Œ ì„ íƒ</td>
      </tr>
      <tr>
        <td><code>num_ctx</code></td>
        <td>2048</td>
        <td>512 ~ 8192</td>
        <td>ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° í¬ê¸° (í† í°)</td>
      </tr>
      <tr>
        <td><code>num_predict</code></td>
        <td>128</td>
        <td>-1 ~ 2048</td>
        <td>ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜ (-1 = ë¬´ì œí•œ)</td>
      </tr>
      <tr>
        <td><code>repeat_penalty</code></td>
        <td>1.1</td>
        <td>1.0 ~ 2.0</td>
        <td>ë°˜ë³µ ì–µì œ (ë†’ì„ìˆ˜ë¡ ëœ ë°˜ë³µ)</td>
      </tr>
      <tr>
        <td><code>stop</code></td>
        <td>-</td>
        <td>ë¬¸ìì—´</td>
        <td>ìƒì„± ì¤‘ë‹¨ ì‹œí€€ìŠ¤</td>
      </tr>
      <tr>
        <td><code>seed</code></td>
        <td>-1</td>
        <td>ì •ìˆ˜</td>
        <td>ë‚œìˆ˜ ì‹œë“œ (ì¬í˜„ì„±)</td>
      </tr>
    </tbody>
  </table>

  <h3 id="modelfile-examples">ì‹¤ì „ ì˜ˆì œ</h3>

  <h4>1. ì½”ë“œ ë¦¬ë·° ëª¨ë¸</h4>

  <pre><code><span class="cmt"># Modelfile.codereview</span>
<span class="kw">FROM</span> codellama:13b

<span class="kw">SYSTEM</span> <span class="str">"""ë‹¹ì‹ ì€ ì‹œë‹ˆì–´ ì†Œí”„íŠ¸ì›¨ì–´ ì—”ì§€ë‹ˆì–´ì…ë‹ˆë‹¤.
ì½”ë“œë¥¼ ë¦¬ë·°í•  ë•Œ ë‹¤ìŒ ê´€ì ì—ì„œ ë¶„ì„í•˜ì„¸ìš”:

1. ë²„ê·¸ ë° ë…¼ë¦¬ ì˜¤ë¥˜
2. ì„±ëŠ¥ ìµœì í™” ê¸°íšŒ
3. ë³´ì•ˆ ì·¨ì•½ì 
4. ì½”ë“œ ê°€ë…ì„± ë° ìœ ì§€ë³´ìˆ˜ì„±
5. í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€

ë¦¬ë·°ëŠ” ê±´ì„¤ì ì´ê³  êµ¬ì²´ì ì¸ ê°œì„  ì œì•ˆì„ í¬í•¨í•˜ì„¸ìš”."""</span>

<span class="kw">PARAMETER</span> temperature 0.3
<span class="kw">PARAMETER</span> top_p 0.95
<span class="kw">PARAMETER</span> num_ctx 4096

<span class="kw">TEMPLATE</span> <span class="str">"""{{ .System }}

ì½”ë“œ:
```
{{ .Prompt }}
```

ë¦¬ë·°:"""</span></code></pre>

  <pre><code><span class="cmt"># ëª¨ë¸ ìƒì„±</span>
ollama create codereview -f Modelfile.codereview

<span class="cmt"># ì‚¬ìš©</span>
ollama run codereview <span class="str">"def calc(a,b): return a/b"</span></code></pre>

  <h4>2. JSON ì¶œë ¥ ëª¨ë¸</h4>

  <pre><code><span class="cmt"># Modelfile.json</span>
<span class="kw">FROM</span> llama3.2

<span class="kw">SYSTEM</span> <span class="str">"""ë‹¹ì‹ ì€ JSON ìƒì„± ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ëª¨ë“  ì‘ë‹µì€ ìœ íš¨í•œ JSON í˜•ì‹ì´ì–´ì•¼ í•©ë‹ˆë‹¤.
ì¶”ê°€ ì„¤ëª…ì´ë‚˜ í…ìŠ¤íŠ¸ ì—†ì´ JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”."""</span>

<span class="kw">PARAMETER</span> temperature 0.1
<span class="kw">PARAMETER</span> stop <span class="str">"```"</span>

<span class="kw">TEMPLATE</span> <span class="str">"""{{ .System }}

ìš”ì²­: {{ .Prompt }}

JSON:
```json"""</span></code></pre>

  <pre><code>ollama create json-generator -f Modelfile.json

ollama run json-generator <span class="str">"ì‚¬ìš©ì í”„ë¡œí•„ ìŠ¤í‚¤ë§ˆë¥¼ ë§Œë“¤ì–´ì¤˜ (name, email, age)"</span>
<span class="cmt"># ì¶œë ¥:</span>
{
  <span class="str">"name"</span>: <span class="str">"string"</span>,
  <span class="str">"email"</span>: <span class="str">"string"</span>,
  <span class="str">"age"</span>: <span class="str">"number"</span>
}</code></pre>

  <h4>3. í•œêµ­ì–´ íŠ¹í™” ëª¨ë¸</h4>

  <pre><code><span class="cmt"># Modelfile.korean</span>
<span class="kw">FROM</span> llama3.2

<span class="kw">SYSTEM</span> <span class="str">"""ë‹¹ì‹ ì€ í•œêµ­ì–´ ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ëª¨ë“  ë‹µë³€ì€ ì¡´ëŒ“ë§ì„ ì‚¬ìš©í•˜ê³ , ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”.
ì™¸ë˜ì–´ë‚˜ ì˜ì–´ í‘œí˜„ì€ ìµœì†Œí™”í•˜ê³ , í•œê¸€ ìš°ì„ ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”."""</span>

<span class="kw">PARAMETER</span> temperature 0.7
<span class="kw">PARAMETER</span> repeat_penalty 1.2

<span class="kw">MESSAGE</span> user <span class="str">"ì•ˆë…•í•˜ì„¸ìš”"</span>
<span class="kw">MESSAGE</span> assistant <span class="str">"ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"</span></code></pre>

  <h4>4. SQL ìƒì„± ëª¨ë¸</h4>

  <pre><code><span class="cmt"># Modelfile.sql</span>
<span class="kw">FROM</span> codellama:7b

<span class="kw">SYSTEM</span> <span class="str">"""ë‹¹ì‹ ì€ SQL ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ìì—°ì–´ ì§ˆë¬¸ì„ PostgreSQL ì¿¼ë¦¬ë¡œ ë³€í™˜í•˜ì„¸ìš”.

ê·œì¹™:
- í‘œì¤€ SQL ë¬¸ë²• ì‚¬ìš©
- ì£¼ì„ìœ¼ë¡œ ì¿¼ë¦¬ ì„¤ëª… ì¶”ê°€
- ì¸ë±ìŠ¤ í™œìš© ê³ ë ¤
- SQL ì¸ì ì…˜ ë°©ì§€"""</span>

<span class="kw">PARAMETER</span> temperature 0.2
<span class="kw">PARAMETER</span> top_p 0.95

<span class="kw">TEMPLATE</span> <span class="str">"""{{ .System }}

ìŠ¤í‚¤ë§ˆ:
- users (id, name, email, created_at)
- orders (id, user_id, amount, status, created_at)
- products (id, name, price, stock)

ì§ˆë¬¸: {{ .Prompt }}

SQL:
```sql"""</span></code></pre>
</section>

<section class="content-section">
  <h2 id="custom-models">ì»¤ìŠ¤í…€ ëª¨ë¸ ìƒì„±</h2>

  <h3 id="gguf-import">GGUF ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°</h3>

  <p>
    Hugging Faceì—ì„œ GGUF í¬ë§· ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•˜ì—¬ Ollamaì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
  </p>

  <pre><code><span class="cmt"># 1. Hugging Faceì—ì„œ GGUF íŒŒì¼ ë‹¤ìš´ë¡œë“œ</span>
<span class="cmt"># ì˜ˆ: TheBlokeì˜ ëª¨ë¸ë“¤</span>
wget https://huggingface.co/TheBloke/Llama-2-13B-GGUF/resolve/main/llama-2-13b.Q4_K_M.gguf

<span class="cmt"># 2. Modelfile ì‘ì„±</span>
cat &gt; Modelfile.custom &lt;&lt;EOF
<span class="kw">FROM</span> ./llama-2-13b.Q4_K_M.gguf

<span class="kw">SYSTEM</span> <span class="str">"ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."</span>

<span class="kw">PARAMETER</span> temperature 0.8
EOF

<span class="cmt"># 3. Ollama ëª¨ë¸ ìƒì„±</span>
ollama create my-llama2-13b -f Modelfile.custom

<span class="cmt"># 4. ì‚¬ìš©</span>
ollama run my-llama2-13b</code></pre>

  <h3 id="lora-adapter">LoRA ì–´ëŒ‘í„° ì ìš©</h3>

  <p>
    <strong>LoRA</strong> (Low-Rank Adaptation)ëŠ” ëª¨ë¸ì˜ ì¼ë¶€ë§Œ íŒŒì¸íŠœë‹í•˜ì—¬
    íŠ¹ì • ì‘ì—…ì— íŠ¹í™”ì‹œí‚¤ëŠ” íš¨ìœ¨ì ì¸ ë°©ë²•ì…ë‹ˆë‹¤.
  </p>

  <pre><code><span class="cmt"># 1. LoRA ì–´ëŒ‘í„° í•™ìŠµ (ì™¸ë¶€ ë„êµ¬ ì‚¬ìš©)</span>
<span class="cmt"># ì˜ˆ: llama.cppì˜ finetune ê¸°ëŠ¥</span>

<span class="cmt"># 2. Modelfileì— ì–´ëŒ‘í„° ì¶”ê°€</span>
cat &gt; Modelfile.lora &lt;&lt;EOF
<span class="kw">FROM</span> llama3.2

<span class="kw">ADAPTER</span> ./my-lora-adapter.bin

<span class="kw">SYSTEM</span> <span class="str">"ë‹¹ì‹ ì€ ì˜ë£Œ ì „ë¬¸ AIì…ë‹ˆë‹¤."</span>

<span class="kw">PARAMETER</span> temperature 0.5
EOF

<span class="cmt"># 3. ëª¨ë¸ ìƒì„±</span>
ollama create medical-llama -f Modelfile.lora</code></pre>

  <h3 id="multi-shot">Few-Shot í•™ìŠµ ëª¨ë¸</h3>

  <pre><code><span class="cmt"># Modelfile.fewshot</span>
<span class="kw">FROM</span> llama3.2

<span class="kw">SYSTEM</span> <span class="str">"ë‹¹ì‹ ì€ ê°ì„± ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. í…ìŠ¤íŠ¸ì˜ ê°ì •ì„ positive/negative/neutralë¡œ ë¶„ë¥˜í•˜ì„¸ìš”."</span>

<span class="cmt"># Few-shot ì˜ˆì œ ì¶”ê°€</span>
<span class="kw">MESSAGE</span> user <span class="str">"ì´ ì˜í™” ì •ë§ ìµœê³ ì˜ˆìš”!"</span>
<span class="kw">MESSAGE</span> assistant <span class="str">"positive"</span>

<span class="kw">MESSAGE</span> user <span class="str">"ëˆ ì•„ê¹Œì›Œìš”. ì‹œê°„ ë‚­ë¹„ì˜€ì–´ìš”."</span>
<span class="kw">MESSAGE</span> assistant <span class="str">"negative"</span>

<span class="kw">MESSAGE</span> user <span class="str">"ê·¸ëƒ¥ í‰ë²”í•œ ì˜í™”ì˜€ì–´ìš”."</span>
<span class="kw">MESSAGE</span> assistant <span class="str">"neutral"</span>

<span class="kw">PARAMETER</span> temperature 0.3</code></pre>

  <pre><code>ollama create sentiment-analyzer -f Modelfile.fewshot

ollama run sentiment-analyzer <span class="str">"ê¸°ëŒ€ ì´ìƒì´ì—ˆì–´ìš”. ê°•ì¶”í•©ë‹ˆë‹¤!"</span>
<span class="cmt"># positive</span></code></pre>
</section>

<section class="content-section">
  <h2 id="rag-pipeline">RAG íŒŒì´í”„ë¼ì¸ êµ¬ì¶•</h2>

  <p>
    <strong>RAG</strong> (Retrieval-Augmented Generation)ëŠ” ì™¸ë¶€ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ì—¬
    LLMì˜ ì‘ë‹µì„ ë³´ê°•í•˜ëŠ” ê¸°ë²•ì…ë‹ˆë‹¤. í™˜ê°(hallucination)ì„ ì¤„ì´ê³  ìµœì‹  ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
  </p>

  <h3 id="rag-architecture">RAG ì•„í‚¤í…ì²˜</h3>

  <svg viewBox="0 0 800 500" class="diagram" role="img" aria-label="RAG íŒŒì´í”„ë¼ì¸">
    <defs>
      <marker id="arrowhead-rag" markerWidth="10" markerHeight="10" refX="9" refY="3" orient="auto">
        <polygon points="0 0, 10 3, 0 6" fill="var(--svg-arrow)" />
      </marker>
    </defs>

    <!-- User Query -->
    <rect x="50" y="50" width="120" height="60" fill="var(--svg-box)" stroke="var(--svg-border)" stroke-width="2" rx="8"/>
    <text x="110" y="85" fill="var(--svg-text)" text-anchor="middle" font-size="14" font-weight="bold">ì‚¬ìš©ì ì§ˆë¬¸</text>

    <!-- Embedding -->
    <rect x="50" y="160" width="120" height="60" fill="var(--svg-highlight)" stroke="var(--svg-accent)" stroke-width="2" rx="8"/>
    <text x="110" y="195" fill="var(--svg-text)" text-anchor="middle" font-size="14" font-weight="bold">ì„ë² ë”© ìƒì„±</text>

    <!-- Vector DB -->
    <rect x="250" y="160" width="150" height="60" fill="var(--svg-box)" stroke="var(--svg-border)" stroke-width="2" rx="8"/>
    <text x="325" y="185" fill="var(--svg-text)" text-anchor="middle" font-size="14" font-weight="bold">ë²¡í„° DB ê²€ìƒ‰</text>
    <text x="325" y="205" fill="var(--svg-text-secondary)" text-anchor="middle" font-size="11">(Chroma, FAISS)</text>

    <!-- Retrieved Docs -->
    <rect x="250" y="270" width="150" height="60" fill="var(--svg-box)" stroke="var(--svg-border)" stroke-width="2" rx="8"/>
    <text x="325" y="295" fill="var(--svg-text)" text-anchor="middle" font-size="14" font-weight="bold">ê´€ë ¨ ë¬¸ì„œ</text>
    <text x="325" y="315" fill="var(--svg-text-secondary)" text-anchor="middle" font-size="11">Top K ë¬¸ì„œ</text>

    <!-- LLM -->
    <rect x="480" y="160" width="150" height="170" fill="var(--svg-highlight)" stroke="var(--svg-accent)" stroke-width="2" rx="8"/>
    <text x="555" y="190" fill="var(--svg-text)" text-anchor="middle" font-size="14" font-weight="bold">LLM (Ollama)</text>
    <text x="555" y="220" fill="var(--svg-text-secondary)" text-anchor="middle" font-size="12">ì§ˆë¬¸ + ì»¨í…ìŠ¤íŠ¸</text>
    <text x="555" y="240" fill="var(--svg-text-secondary)" text-anchor="middle" font-size="12">â†’</text>
    <text x="555" y="260" fill="var(--svg-text-secondary)" text-anchor="middle" font-size="12">ë‹µë³€ ìƒì„±</text>

    <!-- Response -->
    <rect x="680" y="210" width="100" height="60" fill="var(--svg-box)" stroke="var(--svg-border)" stroke-width="2" rx="8"/>
    <text x="730" y="245" fill="var(--svg-text)" text-anchor="middle" font-size="14" font-weight="bold">ì‘ë‹µ</text>

    <!-- Arrows -->
    <line x1="110" y1="110" x2="110" y2="160" stroke="var(--svg-arrow)" stroke-width="2" marker-end="url(#arrowhead-rag)"/>
    <line x1="170" y1="190" x2="250" y2="190" stroke="var(--svg-arrow)" stroke-width="2" marker-end="url(#arrowhead-rag)"/>
    <line x1="325" y1="220" x2="325" y2="270" stroke="var(--svg-arrow)" stroke-width="2" marker-end="url(#arrowhead-rag)"/>
    <line x1="400" y1="300" x2="480" y2="245" stroke="var(--svg-arrow)" stroke-width="2" marker-end="url(#arrowhead-rag)"/>
    <line x1="630" y1="240" x2="680" y2="240" stroke="var(--svg-arrow)" stroke-width="2" marker-end="url(#arrowhead-rag)"/>

    <!-- Document Store -->
    <rect x="250" y="380" width="150" height="60" fill="var(--svg-section)" stroke="var(--svg-border)" stroke-width="2" rx="8"/>
    <text x="325" y="405" fill="var(--svg-text)" text-anchor="middle" font-size="12" font-weight="bold">ë¬¸ì„œ ì €ì¥ì†Œ</text>
    <text x="325" y="425" fill="var(--svg-text-secondary)" text-anchor="middle" font-size="10">PDF, DOCX, TXT</text>
    <line x1="325" y1="380" x2="325" y2="330" stroke="var(--svg-arrow)" stroke-width="2" stroke-dasharray="5,5"/>
  </svg>

  <h3 id="rag-simple">ê°„ë‹¨í•œ RAG êµ¬í˜„ (Python)</h3>

  <pre><code><span class="kw">import</span> requests
<span class="kw">from</span> typing <span class="kw">import</span> List
<span class="kw">import</span> numpy <span class="kw">as</span> np

<span class="kw">class</span> <span class="type">SimpleRAG</span>:
    <span class="kw">def</span> <span class="fn">__init__</span>(<span class="kw">self</span>, documents: List[<span class="type">str</span>]):
        <span class="kw">self</span>.documents = documents
        <span class="kw">self</span>.embeddings = [<span class="kw">self</span>._embed(doc) <span class="kw">for</span> doc <span class="kw">in</span> documents]

    <span class="kw">def</span> <span class="fn">_embed</span>(<span class="kw">self</span>, text: <span class="type">str</span>) -&gt; List[<span class="type">float</span>]:
        <span class="str">"""Ollama ì„ë² ë”© API ì‚¬ìš©"""</span>
        response = requests.post(
            <span class="str">'http://localhost:11434/api/embeddings'</span>,
            json={<span class="str">'model'</span>: <span class="str">'nomic-embed-text'</span>, <span class="str">'prompt'</span>: text}
        )
        <span class="kw">return</span> response.json()[<span class="str">'embedding'</span>]

    <span class="kw">def</span> <span class="fn">_cosine_similarity</span>(<span class="kw">self</span>, a: List[<span class="type">float</span>], b: List[<span class="type">float</span>]) -&gt; <span class="type">float</span>:
        <span class="str">"""ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""</span>
        a = np.array(a)
        b = np.array(b)
        <span class="kw">return</span> np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

    <span class="kw">def</span> <span class="fn">retrieve</span>(<span class="kw">self</span>, query: <span class="type">str</span>, k: <span class="type">int</span> = <span class="num">3</span>) -&gt; List[<span class="type">str</span>]:
        <span class="str">"""ê°€ì¥ ê´€ë ¨ ìˆëŠ” ë¬¸ì„œ Kê°œ ê²€ìƒ‰"""</span>
        query_emb = <span class="kw">self</span>._embed(query)

        <span class="cmt"># ìœ ì‚¬ë„ ê³„ì‚°</span>
        scores = [
            <span class="kw">self</span>._cosine_similarity(query_emb, doc_emb)
            <span class="kw">for</span> doc_emb <span class="kw">in</span> <span class="kw">self</span>.embeddings
        ]

        <span class="cmt"># ìƒìœ„ Kê°œ ì¸ë±ìŠ¤</span>
        top_indices = np.argsort(scores)[-k:][::-<span class="num">1</span>]

        <span class="kw">return</span> [<span class="kw">self</span>.documents[i] <span class="kw">for</span> i <span class="kw">in</span> top_indices]

    <span class="kw">def</span> <span class="fn">generate</span>(<span class="kw">self</span>, query: <span class="type">str</span>) -&gt; <span class="type">str</span>:
        <span class="str">"""RAG ì‘ë‹µ ìƒì„±"""</span>
        <span class="cmt"># 1. ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰</span>
        context_docs = <span class="kw">self</span>.retrieve(query, k=<span class="num">3</span>)
        context = <span class="str">"\n\n"</span>.join(context_docs)

        <span class="cmt"># 2. í”„ë¡¬í”„íŠ¸ êµ¬ì„±</span>
        prompt = <span class="str">f"""ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.

ì»¨í…ìŠ¤íŠ¸:
{context}

ì§ˆë¬¸: {query}

ë‹µë³€:"""</span>

        <span class="cmt"># 3. LLM í˜¸ì¶œ</span>
        response = requests.post(
            <span class="str">'http://localhost:11434/api/generate'</span>,
            json={
                <span class="str">'model'</span>: <span class="str">'llama3.2'</span>,
                <span class="str">'prompt'</span>: prompt,
                <span class="str">'stream'</span>: <span class="kw">False</span>
            }
        )

        <span class="kw">return</span> response.json()[<span class="str">'response'</span>]


<span class="cmt"># ì‚¬ìš© ì˜ˆì‹œ</span>
documents = [
    <span class="str">"FastAPIëŠ” Python ì›¹ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. ë¹ ë¥´ê³  í˜„ëŒ€ì ì…ë‹ˆë‹¤."</span>,
    <span class="str">"FlaskëŠ” ë§ˆì´í¬ë¡œ ì›¹ í”„ë ˆì„ì›Œí¬ë¡œ ê°„ë‹¨í•˜ê³  ìœ ì—°í•©ë‹ˆë‹¤."</span>,
    <span class="str">"DjangoëŠ” í’€ìŠ¤íƒ í”„ë ˆì„ì›Œí¬ë¡œ admin íŒ¨ë„ì„ ì œê³µí•©ë‹ˆë‹¤."</span>,
]

rag = SimpleRAG(documents)
answer = rag.generate(<span class="str">"FastAPIì˜ íŠ¹ì§•ì€?"</span>)
<span class="kw">print</span>(answer)</code></pre>

  <h3 id="rag-chromadb">ChromaDBë¥¼ ì‚¬ìš©í•œ í”„ë¡œë•ì…˜ RAG</h3>

  <pre><code><span class="kw">from</span> langchain_community.vectorstores <span class="kw">import</span> Chroma
<span class="kw">from</span> langchain_community.embeddings <span class="kw">import</span> OllamaEmbeddings
<span class="kw">from</span> langchain_community.llms <span class="kw">import</span> Ollama
<span class="kw">from</span> langchain.text_splitter <span class="kw">import</span> RecursiveCharacterTextSplitter
<span class="kw">from</span> langchain.chains <span class="kw">import</span> RetrievalQA
<span class="kw">from</span> langchain_community.document_loaders <span class="kw">import</span> DirectoryLoader, PyPDFLoader

<span class="kw">class</span> <span class="type">ProductionRAG</span>:
    <span class="kw">def</span> <span class="fn">__init__</span>(<span class="kw">self</span>, docs_dir: <span class="type">str</span>, persist_dir: <span class="type">str</span> = <span class="str">"./chroma_db"</span>):
        <span class="kw">self</span>.embeddings = OllamaEmbeddings(model=<span class="str">"nomic-embed-text"</span>)
        <span class="kw">self</span>.llm = Ollama(model=<span class="str">"llama3.2"</span>, temperature=<span class="num">0.3</span>)
        <span class="kw">self</span>.persist_dir = persist_dir

        <span class="cmt"># ë¬¸ì„œ ë¡œë“œ ë° ë²¡í„° DB ìƒì„±</span>
        <span class="kw">self</span>.vectorstore = <span class="kw">self</span>._build_vectorstore(docs_dir)
        <span class="kw">self</span>.qa_chain = <span class="kw">self</span>._create_qa_chain()

    <span class="kw">def</span> <span class="fn">_build_vectorstore</span>(<span class="kw">self</span>, docs_dir: <span class="type">str</span>):
        <span class="str">"""ë¬¸ì„œ ë¡œë“œ ë° ë²¡í„° DB ìƒì„±"""</span>
        <span class="cmt"># PDF ë¡œë”</span>
        loader = DirectoryLoader(
            docs_dir,
            glob=<span class="str">"**/*.pdf"</span>,
            loader_cls=PyPDFLoader
        )
        documents = loader.load()

        <span class="cmt"># í…ìŠ¤íŠ¸ ë¶„í• </span>
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=<span class="num">1000</span>,
            chunk_overlap=<span class="num">200</span>,
            length_function=len
        )
        chunks = text_splitter.split_documents(documents)

        <span class="cmt"># ë²¡í„° DB ìƒì„±</span>
        vectorstore = Chroma.from_documents(
            documents=chunks,
            embedding=<span class="kw">self</span>.embeddings,
            persist_directory=<span class="kw">self</span>.persist_dir
        )

        <span class="kw">return</span> vectorstore

    <span class="kw">def</span> <span class="fn">_create_qa_chain</span>(<span class="kw">self</span>):
        <span class="str">"""QA ì²´ì¸ ìƒì„±"""</span>
        <span class="kw">return</span> RetrievalQA.from_chain_type(
            llm=<span class="kw">self</span>.llm,
            chain_type=<span class="str">"stuff"</span>,
            retriever=<span class="kw">self</span>.vectorstore.as_retriever(
                search_type=<span class="str">"similarity"</span>,
                search_kwargs={<span class="str">"k"</span>: <span class="num">5</span>}
            ),
            return_source_documents=<span class="kw">True</span>
        )

    <span class="kw">def</span> <span class="fn">query</span>(<span class="kw">self</span>, question: <span class="type">str</span>):
        <span class="str">"""ì§ˆë¬¸í•˜ê³  ë‹µë³€ ë°›ê¸°"""</span>
        result = <span class="kw">self</span>.qa_chain.invoke({<span class="str">"query"</span>: question})

        <span class="kw">print</span>(<span class="str">f"ì§ˆë¬¸: {question}"</span>)
        <span class="kw">print</span>(<span class="str">f"ë‹µë³€: {result['result']}"</span>)
        <span class="kw">print</span>(<span class="str">f"\nì¶œì²˜ ë¬¸ì„œ {len(result['source_documents'])}ê°œ:"</span>)

        <span class="kw">for</span> i, doc <span class="kw">in</span> enumerate(result[<span class="str">'source_documents'</span>], <span class="num">1</span>):
            <span class="kw">print</span>(<span class="str">f"  {i}. {doc.metadata.get('source', 'Unknown')}"</span>)

        <span class="kw">return</span> result


<span class="cmt"># ì‚¬ìš©</span>
rag = ProductionRAG(<span class="str">"./company_docs"</span>)
rag.query(<span class="str">"íšŒì‚¬ì˜ íœ´ê°€ ì •ì±…ì€ ë¬´ì—‡ì¸ê°€ìš”?"</span>)</code></pre>

  <h3 id="rag-optimization">RAG ìµœì í™” ê¸°ë²•</h3>

  <h4>1. í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (í‚¤ì›Œë“œ + ë²¡í„°)</h4>

  <pre><code><span class="kw">from</span> langchain.retrievers <span class="kw">import</span> EnsembleRetriever
<span class="kw">from</span> langchain_community.retrievers <span class="kw">import</span> BM25Retriever

<span class="cmt"># BM25 (í‚¤ì›Œë“œ ê²€ìƒ‰)</span>
bm25_retriever = BM25Retriever.from_documents(documents)
bm25_retriever.k = <span class="num">3</span>

<span class="cmt"># ë²¡í„° ê²€ìƒ‰</span>
vector_retriever = vectorstore.as_retriever(search_kwargs={<span class="str">"k"</span>: <span class="num">3</span>})

<span class="cmt"># ì•™ìƒë¸” (í•˜ì´ë¸Œë¦¬ë“œ)</span>
ensemble_retriever = EnsembleRetriever(
    retrievers=[bm25_retriever, vector_retriever],
    weights=[<span class="num">0.4</span>, <span class="num">0.6</span>]  <span class="cmt"># ë²¡í„° ê²€ìƒ‰ 60% ê°€ì¤‘ì¹˜</span>
)</code></pre>

  <h4>2. Reranking (ì¬ìˆœìœ„í™”)</h4>

  <pre><code><span class="kw">from</span> langchain.retrievers <span class="kw">import</span> ContextualCompressionRetriever
<span class="kw">from</span> langchain.retrievers.document_compressors <span class="kw">import</span> LLMChainExtractor

<span class="cmt"># LLMìœ¼ë¡œ ê´€ë ¨ì„± ì¬í‰ê°€</span>
compressor = LLMChainExtractor.from_llm(llm)

compression_retriever = ContextualCompressionRetriever(
    base_compressor=compressor,
    base_retriever=vector_retriever
)

<span class="cmt"># ë” ì •í™•í•œ ë¬¸ì„œ ê²€ìƒ‰</span>
compressed_docs = compression_retriever.get_relevant_documents(<span class="str">"ì§ˆë¬¸"</span>)</code></pre>

  <h4>3. Parent Document Retriever</h4>

  <pre><code><span class="kw">from</span> langchain.retrievers <span class="kw">import</span> ParentDocumentRetriever
<span class="kw">from</span> langchain.storage <span class="kw">import</span> InMemoryStore

<span class="cmt"># ì‘ì€ ì²­í¬ë¡œ ê²€ìƒ‰, í° ì²­í¬ë¡œ ì»¨í…ìŠ¤íŠ¸ ì œê³µ</span>
parent_splitter = RecursiveCharacterTextSplitter(chunk_size=<span class="num">2000</span>)
child_splitter = RecursiveCharacterTextSplitter(chunk_size=<span class="num">400</span>)

store = InMemoryStore()

retriever = ParentDocumentRetriever(
    vectorstore=vectorstore,
    docstore=store,
    child_splitter=child_splitter,
    parent_splitter=parent_splitter
)</code></pre>
</section>

<section class="content-section">
  <h2 id="agents">ì—ì´ì „íŠ¸ êµ¬í˜„</h2>

  <p>
    <strong>ì—ì´ì „íŠ¸</strong>ëŠ” ë„êµ¬(Tools)ë¥¼ ì‚¬ìš©í•˜ì—¬ ë³µì¡í•œ ì‘ì—…ì„ ììœ¨ì ìœ¼ë¡œ ìˆ˜í–‰í•˜ëŠ” AI ì‹œìŠ¤í…œì…ë‹ˆë‹¤.
  </p>

  <h3 id="agent-simple">ê°„ë‹¨í•œ ì—ì´ì „íŠ¸</h3>

  <pre><code><span class="kw">from</span> langchain.agents <span class="kw">import</span> initialize_agent, Tool, AgentType
<span class="kw">from</span> langchain_community.llms <span class="kw">import</span> Ollama
<span class="kw">import</span> requests

<span class="cmt"># ë„êµ¬ ì •ì˜</span>
<span class="kw">def</span> <span class="fn">search_wikipedia</span>(query: <span class="type">str</span>) -&gt; <span class="type">str</span>:
    <span class="str">"""ìœ„í‚¤í”¼ë””ì•„ ê²€ìƒ‰"""</span>
    url = <span class="str">f"https://en.wikipedia.org/api/rest_v1/page/summary/{query}"</span>
    response = requests.get(url)
    <span class="kw">if</span> response.status_code == <span class="num">200</span>:
        <span class="kw">return</span> response.json().get(<span class="str">"extract"</span>, <span class="str">"No results"</span>)
    <span class="kw">return</span> <span class="str">"Error"</span>

<span class="kw">def</span> <span class="fn">calculate</span>(expression: <span class="type">str</span>) -&gt; <span class="type">str</span>:
    <span class="str">"""ìˆ˜í•™ ê³„ì‚°"""</span>
    <span class="kw">try</span>:
        <span class="kw">return</span> str(eval(expression))
    <span class="kw">except</span> <span class="type">Exception</span> <span class="kw">as</span> e:
        <span class="kw">return</span> <span class="str">f"Error: {e}"</span>

<span class="kw">def</span> <span class="fn">get_weather</span>(city: <span class="type">str</span>) -&gt; <span class="type">str</span>:
    <span class="str">"""ë‚ ì”¨ ì •ë³´ (ê°€ì§œ ë°ì´í„°)"""</span>
    <span class="kw">import</span> random
    temp = random.randint(<span class="num">15</span>, <span class="num">30</span>)
    <span class="kw">return</span> <span class="str">f"{city}ì˜ í˜„ì¬ ì˜¨ë„ëŠ” {temp}Â°Cì…ë‹ˆë‹¤."</span>

<span class="cmt"># ë„êµ¬ ë¦¬ìŠ¤íŠ¸</span>
tools = [
    Tool(
        name=<span class="str">"Wikipedia"</span>,
        func=search_wikipedia,
        description=<span class="str">"ìœ„í‚¤í”¼ë””ì•„ì—ì„œ ì •ë³´ë¥¼ ê²€ìƒ‰í•  ë•Œ ì‚¬ìš©. ì…ë ¥: ê²€ìƒ‰ì–´"</span>
    ),
    Tool(
        name=<span class="str">"Calculator"</span>,
        func=calculate,
        description=<span class="str">"ìˆ˜í•™ ê³„ì‚°. ì…ë ¥: íŒŒì´ì¬ í‘œí˜„ì‹ (ì˜ˆ: '2+2', '10*5')"</span>
    ),
    Tool(
        name=<span class="str">"Weather"</span>,
        func=get_weather,
        description=<span class="str">"ë„ì‹œì˜ ë‚ ì”¨ ì •ë³´. ì…ë ¥: ë„ì‹œ ì´ë¦„"</span>
    )
]

<span class="cmt"># LLM ë° ì—ì´ì „íŠ¸ ì´ˆê¸°í™”</span>
llm = Ollama(model=<span class="str">"llama3.2"</span>, temperature=<span class="num">0</span>)
agent = initialize_agent(
    tools,
    llm,
    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
    verbose=<span class="kw">True</span>,
    max_iterations=<span class="num">5</span>
)

<span class="cmt"># ì‹¤í–‰</span>
result = agent.run(<span class="str">"ì„œìš¸ì˜ ë‚ ì”¨ë¥¼ ì•Œë ¤ì£¼ê³ , í™”ì”¨ë¡œ ë³€í™˜í•´ì¤˜"</span>)
<span class="kw">print</span>(result)</code></pre>

  <h3 id="agent-custom">ì»¤ìŠ¤í…€ ë„êµ¬ ì—ì´ì „íŠ¸</h3>

  <pre><code><span class="kw">from</span> langchain.tools <span class="kw">import</span> BaseTool
<span class="kw">from</span> pydantic <span class="kw">import</span> Field

<span class="kw">class</span> <span class="type">FileReadTool</span>(BaseTool):
    name = <span class="str">"file_reader"</span>
    description = <span class="str">"íŒŒì¼ ë‚´ìš©ì„ ì½ëŠ” ë„êµ¬. ì…ë ¥: íŒŒì¼ ê²½ë¡œ"</span>

    <span class="kw">def</span> <span class="fn">_run</span>(<span class="kw">self</span>, file_path: <span class="type">str</span>) -&gt; <span class="type">str</span>:
        <span class="kw">try</span>:
            <span class="kw">with</span> open(file_path, <span class="str">'r'</span>) <span class="kw">as</span> f:
                <span class="kw">return</span> f.read()
        <span class="kw">except</span> <span class="type">Exception</span> <span class="kw">as</span> e:
            <span class="kw">return</span> <span class="str">f"Error: {e}"</span>

    <span class="kw">async</span> <span class="kw">def</span> <span class="fn">_arun</span>(<span class="kw">self</span>, file_path: <span class="type">str</span>) -&gt; <span class="type">str</span>:
        <span class="kw">raise</span> NotImplementedError(<span class="str">"Async not supported"</span>)


<span class="kw">class</span> <span class="type">DatabaseQueryTool</span>(BaseTool):
    name = <span class="str">"database_query"</span>
    description = <span class="str">"SQLite DB ì¿¼ë¦¬ ì‹¤í–‰. ì…ë ¥: SQL ì¿¼ë¦¬"</span>
    db_path: <span class="type">str</span> = Field(default=<span class="str">"./data.db"</span>)

    <span class="kw">def</span> <span class="fn">_run</span>(<span class="kw">self</span>, query: <span class="type">str</span>) -&gt; <span class="type">str</span>:
        <span class="kw">import</span> sqlite3
        <span class="kw">try</span>:
            conn = sqlite3.connect(<span class="kw">self</span>.db_path)
            cursor = conn.cursor()
            cursor.execute(query)
            results = cursor.fetchall()
            conn.close()
            <span class="kw">return</span> str(results)
        <span class="kw">except</span> <span class="type">Exception</span> <span class="kw">as</span> e:
            <span class="kw">return</span> <span class="str">f"Error: {e}"</span>


<span class="cmt"># ì‚¬ìš©</span>
tools = [FileReadTool(), DatabaseQueryTool(db_path=<span class="str">"./users.db"</span>)]
agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION)</code></pre>

  <h3 id="agent-conversational">ëŒ€í™”í˜• ì—ì´ì „íŠ¸</h3>

  <pre><code><span class="kw">from</span> langchain.agents <span class="kw">import</span> AgentType
<span class="kw">from</span> langchain.memory <span class="kw">import</span> ConversationBufferMemory

<span class="cmt"># ëŒ€í™” ë©”ëª¨ë¦¬</span>
memory = ConversationBufferMemory(
    memory_key=<span class="str">"chat_history"</span>,
    return_messages=<span class="kw">True</span>
)

<span class="cmt"># ëŒ€í™”í˜• ì—ì´ì „íŠ¸</span>
agent = initialize_agent(
    tools,
    llm,
    agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION,
    memory=memory,
    verbose=<span class="kw">True</span>
)

<span class="cmt"># ëŒ€í™”</span>
agent.run(<span class="str">"ì„œìš¸ì˜ ë‚ ì”¨ë¥¼ ì•Œë ¤ì¤˜"</span>)
agent.run(<span class="str">"ê±°ê¸° ì¸êµ¬ëŠ” ì–¼ë§ˆì•¼?"</span>)  <span class="cmt"># "ê±°ê¸°" = ì„œìš¸ (ë©”ëª¨ë¦¬ ìœ ì§€)</span></code></pre>
</section>

<section class="content-section">
  <h2 id="optimization">ì„±ëŠ¥ ìµœì í™”</h2>

  <h3 id="opt-model-selection">ëª¨ë¸ ì„ íƒ ìµœì í™”</h3>

  <table>
    <thead>
      <tr>
        <th>ì‘ì—…</th>
        <th>ì¶”ì²œ ëª¨ë¸</th>
        <th>ì´ìœ </th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>ì½”ë“œ ìƒì„±</td>
        <td>CodeLlama 13B, Qwen2.5-Coder 14B</td>
        <td>ì½”ë“œ íŠ¹í™”, ë†’ì€ ì •í™•ë„</td>
      </tr>
      <tr>
        <td>ì¼ë°˜ ëŒ€í™”</td>
        <td>Llama 3.2 7B, Mistral 7B</td>
        <td>ê· í˜•ì¡íŒ ì„±ëŠ¥/ì†ë„</td>
      </tr>
      <tr>
        <td>ë¹ ë¥¸ ìë™ì™„ì„±</td>
        <td>Starcoder2 3B, Phi-3 Mini</td>
        <td>ì‘ê³  ë¹ ë¦„</td>
      </tr>
      <tr>
        <td>ì„ë² ë”©</td>
        <td>nomic-embed-text</td>
        <td>ë¹ ë¥´ê³  ì •í™•</td>
      </tr>
      <tr>
        <td>ìš”ì•½</td>
        <td>Llama 3.2 3B</td>
        <td>ë¹ ë¥´ê³  íš¨ìœ¨ì </td>
      </tr>
      <tr>
        <td>ë³µì¡í•œ ì¶”ë¡ </td>
        <td>Llama 3.1 70B, Mixtral 8x22B</td>
        <td>ìµœê³  ì„±ëŠ¥ (GPU í•„ìˆ˜)</td>
      </tr>
    </tbody>
  </table>

  <h3 id="opt-context">ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° ìµœì í™”</h3>

  <pre><code><span class="cmt"># Modelfileì—ì„œ ì„¤ì •</span>
<span class="kw">PARAMETER</span> num_ctx 8192    <span class="cmt"># ê¸°ë³¸ 2048 â†’ 8192ë¡œ ì¦ê°€</span>
<span class="kw">PARAMETER</span> num_predict 512  <span class="cmt"># ì‘ë‹µ ê¸¸ì´ ì œí•œ</span>

<span class="cmt"># API í˜¸ì¶œ ì‹œ</span>
requests.post(<span class="str">'http://localhost:11434/api/generate'</span>, json={
    <span class="str">'model'</span>: <span class="str">'llama3.2'</span>,
    <span class="str">'prompt'</span>: prompt,
    <span class="str">'options'</span>: {
        <span class="str">'num_ctx'</span>: <span class="num">8192</span>,
        <span class="str">'num_predict'</span>: <span class="num">512</span>
    }
})</code></pre>

  <h3 id="opt-batch">ë°°ì¹˜ ì²˜ë¦¬</h3>

  <pre><code><span class="kw">import</span> asyncio
<span class="kw">import</span> aiohttp

<span class="kw">async</span> <span class="kw">def</span> <span class="fn">generate_async</span>(session, prompt):
    <span class="kw">async</span> <span class="kw">with</span> session.post(
        <span class="str">'http://localhost:11434/api/generate'</span>,
        json={<span class="str">'model'</span>: <span class="str">'llama3.2'</span>, <span class="str">'prompt'</span>: prompt, <span class="str">'stream'</span>: <span class="kw">False</span>}
    ) <span class="kw">as</span> response:
        data = <span class="kw">await</span> response.json()
        <span class="kw">return</span> data[<span class="str">'response'</span>]

<span class="kw">async</span> <span class="kw">def</span> <span class="fn">batch_generate</span>(prompts):
    <span class="kw">async</span> <span class="kw">with</span> aiohttp.ClientSession() <span class="kw">as</span> session:
        tasks = [generate_async(session, p) <span class="kw">for</span> p <span class="kw">in</span> prompts]
        <span class="kw">return</span> <span class="kw">await</span> asyncio.gather(*tasks)

<span class="cmt"># ì‚¬ìš©</span>
prompts = [<span class="str">f"ìˆ«ì {i}ë¥¼ ì„¤ëª…í•´ì¤˜"</span> <span class="kw">for</span> i <span class="kw">in</span> range(<span class="num">10</span>)]
results = asyncio.run(batch_generate(prompts))</code></pre>

  <h3 id="opt-caching">ì‘ë‹µ ìºì‹±</h3>

  <pre><code><span class="kw">from</span> functools <span class="kw">import</span> lru_cache
<span class="kw">import</span> hashlib

<span class="kw">class</span> <span class="type">CachedOllama</span>:
    <span class="kw">def</span> <span class="fn">__init__</span>(<span class="kw">self</span>, model: <span class="type">str</span>):
        <span class="kw">self</span>.model = model
        <span class="kw">self</span>.cache = {}

    <span class="kw">def</span> <span class="fn">_hash_prompt</span>(<span class="kw">self</span>, prompt: <span class="type">str</span>) -&gt; <span class="type">str</span>:
        <span class="kw">return</span> hashlib.md5(prompt.encode()).hexdigest()

    <span class="kw">def</span> <span class="fn">generate</span>(<span class="kw">self</span>, prompt: <span class="type">str</span>) -&gt; <span class="type">str</span>:
        cache_key = <span class="kw">self</span>._hash_prompt(prompt)

        <span class="cmt"># ìºì‹œ í™•ì¸</span>
        <span class="kw">if</span> cache_key <span class="kw">in</span> <span class="kw">self</span>.cache:
            <span class="kw">print</span>(<span class="str">"Cache hit!"</span>)
            <span class="kw">return</span> <span class="kw">self</span>.cache[cache_key]

        <span class="cmt"># ìƒì„±</span>
        response = requests.post(
            <span class="str">'http://localhost:11434/api/generate'</span>,
            json={<span class="str">'model'</span>: <span class="kw">self</span>.model, <span class="str">'prompt'</span>: prompt, <span class="str">'stream'</span>: <span class="kw">False</span>}
        )
        result = response.json()[<span class="str">'response'</span>]

        <span class="cmt"># ìºì‹œ ì €ì¥</span>
        <span class="kw">self</span>.cache[cache_key] = result
        <span class="kw">return</span> result


ollama = CachedOllama(<span class="str">"llama3.2"</span>)
ollama.generate(<span class="str">"ì•ˆë…•"</span>)  <span class="cmt"># LLM í˜¸ì¶œ</span>
ollama.generate(<span class="str">"ì•ˆë…•"</span>)  <span class="cmt"># ìºì‹œ ì‚¬ìš© (ì¦‰ì‹œ ë°˜í™˜)</span></code></pre>

  <h3 id="opt-gpu">GPU í™œìš© ìµœì í™”</h3>

  <pre><code><span class="cmt"># í™˜ê²½ ë³€ìˆ˜ë¡œ GPU ë ˆì´ì–´ ìˆ˜ ì¡°ì •</span>
<span class="kw">export</span> OLLAMA_NUM_GPU=<span class="num">99</span>  <span class="cmt"># ëª¨ë“  ë ˆì´ì–´ë¥¼ GPUì—</span>
<span class="kw">export</span> OLLAMA_NUM_GPU=<span class="num">20</span>  <span class="cmt"># ì¼ë¶€ë§Œ GPU (í•˜ì´ë¸Œë¦¬ë“œ)</span>

<span class="cmt"># ë‹¤ì¤‘ GPU ì‚¬ìš©</span>
<span class="kw">export</span> CUDA_VISIBLE_DEVICES=0,1  <span class="cmt"># GPU 0, 1 ì‚¬ìš©</span>

<span class="cmt"># VRAM ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§</span>
watch -n 1 nvidia-smi</code></pre>
</section>

<section class="content-section">
  <h2 id="production">í”„ë¡œë•ì…˜ ë°°í¬</h2>

  <h3 id="prod-docker">Docker í”„ë¡œë•ì…˜ ë°°í¬</h3>

  <pre><code><span class="cmt"># Dockerfile</span>
<span class="kw">FROM</span> ollama/ollama:latest

<span class="cmt"># ëª¨ë¸ ë¯¸ë¦¬ ë‹¤ìš´ë¡œë“œ</span>
<span class="kw">RUN</span> ollama serve &amp; sleep 5 &amp;&amp; \
    ollama pull llama3.2 &amp;&amp; \
    ollama pull nomic-embed-text &amp;&amp; \
    pkill ollama

<span class="cmt"># í™˜ê²½ ë³€ìˆ˜</span>
<span class="kw">ENV</span> OLLAMA_HOST=0.0.0.0:11434
<span class="kw">ENV</span> OLLAMA_KEEP_ALIVE=24h
<span class="kw">ENV</span> OLLAMA_NUM_PARALLEL=4

<span class="kw">EXPOSE</span> 11434

<span class="kw">CMD</span> [<span class="str">"ollama"</span>, <span class="str">"serve"</span>]</code></pre>

  <pre><code><span class="cmt"># ë¹Œë“œ ë° ì‹¤í–‰</span>
docker build -t my-ollama .
docker run -d -p 11434:11434 --gpus all --name ollama-prod my-ollama</code></pre>

  <h3 id="prod-kubernetes">Kubernetes ë°°í¬</h3>

  <pre><code><span class="cmt"># ollama-deployment.yaml</span>
<span class="kw">apiVersion</span>: apps/v1
<span class="kw">kind</span>: Deployment
<span class="kw">metadata</span>:
  <span class="kw">name</span>: ollama
<span class="kw">spec</span>:
  <span class="kw">replicas</span>: 2
  <span class="kw">selector</span>:
    <span class="kw">matchLabels</span>:
      <span class="kw">app</span>: ollama
  <span class="kw">template</span>:
    <span class="kw">metadata</span>:
      <span class="kw">labels</span>:
        <span class="kw">app</span>: ollama
    <span class="kw">spec</span>:
      <span class="kw">containers</span>:
      - <span class="kw">name</span>: ollama
        <span class="kw">image</span>: ollama/ollama:latest
        <span class="kw">ports</span>:
        - <span class="kw">containerPort</span>: 11434
        <span class="kw">env</span>:
        - <span class="kw">name</span>: OLLAMA_HOST
          <span class="kw">value</span>: <span class="str">"0.0.0.0:11434"</span>
        <span class="kw">resources</span>:
          <span class="kw">limits</span>:
            <span class="kw">nvidia.com/gpu</span>: 1
            <span class="kw">memory</span>: <span class="str">"16Gi"</span>
          <span class="kw">requests</span>:
            <span class="kw">memory</span>: <span class="str">"8Gi"</span>
        <span class="kw">volumeMounts</span>:
        - <span class="kw">name</span>: ollama-data
          <span class="kw">mountPath</span>: /root/.ollama
      <span class="kw">volumes</span>:
      - <span class="kw">name</span>: ollama-data
        <span class="kw">persistentVolumeClaim</span>:
          <span class="kw">claimName</span>: ollama-pvc
---
<span class="kw">apiVersion</span>: v1
<span class="kw">kind</span>: Service
<span class="kw">metadata</span>:
  <span class="kw">name</span>: ollama
<span class="kw">spec</span>:
  <span class="kw">selector</span>:
    <span class="kw">app</span>: ollama
  <span class="kw">ports</span>:
  - <span class="kw">protocol</span>: TCP
    <span class="kw">port</span>: 11434
    <span class="kw">targetPort</span>: 11434
  <span class="kw">type</span>: LoadBalancer</code></pre>

  <h3 id="prod-nginx">NGINX ë¦¬ë²„ìŠ¤ í”„ë¡ì‹œ</h3>

  <pre><code><span class="cmt"># nginx.conf</span>
upstream ollama_backend {
    server 127.0.0.1:11434;
    keepalive 32;
}

server {
    listen 80;
    server_name ollama.example.com;

    <span class="cmt"># HTTPS ë¦¬ë‹¤ì´ë ‰íŠ¸</span>
    return 301 https://$server_name$request_uri;
}

server {
    listen 443 ssl http2;
    server_name ollama.example.com;

    ssl_certificate /etc/ssl/certs/ollama.crt;
    ssl_certificate_key /etc/ssl/private/ollama.key;

    <span class="cmt"># íƒ€ì„ì•„ì›ƒ ì„¤ì • (ê¸´ ì‘ë‹µ ëŒ€ë¹„)</span>
    proxy_read_timeout 300s;
    proxy_connect_timeout 75s;

    location / {
        proxy_pass http://ollama_backend;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;

        <span class="cmt"># Rate limiting</span>
        limit_req zone=ollama_limit burst=10 nodelay;
    }
}

<span class="cmt"># Rate limit zone</span>
limit_req_zone $binary_remote_addr zone=ollama_limit:10m rate=10r/s;</code></pre>

  <h3 id="prod-monitoring">ëª¨ë‹ˆí„°ë§</h3>

  <pre><code><span class="cmt"># Prometheus ë©”íŠ¸ë¦­ ìˆ˜ì§‘ (Python ì˜ˆì‹œ)</span>
<span class="kw">from</span> prometheus_client <span class="kw">import</span> Counter, Histogram, start_http_server
<span class="kw">import</span> time

<span class="cmt"># ë©”íŠ¸ë¦­ ì •ì˜</span>
request_count = Counter(<span class="str">'ollama_requests_total'</span>, <span class="str">'Total requests'</span>)
response_time = Histogram(<span class="str">'ollama_response_seconds'</span>, <span class="str">'Response time'</span>)

<span class="kw">def</span> <span class="fn">generate_with_metrics</span>(prompt):
    request_count.inc()
    start = time.time()

    <span class="cmt"># Ollama í˜¸ì¶œ</span>
    response = requests.post(<span class="str">'http://localhost:11434/api/generate'</span>, json={
        <span class="str">'model'</span>: <span class="str">'llama3.2'</span>,
        <span class="str">'prompt'</span>: prompt,
        <span class="str">'stream'</span>: <span class="kw">False</span>
    })

    duration = time.time() - start
    response_time.observe(duration)

    <span class="kw">return</span> response.json()[<span class="str">'response'</span>]

<span class="cmt"># Prometheus ì„œë²„ ì‹œì‘ (í¬íŠ¸ 8000)</span>
start_http_server(<span class="num">8000</span>)</code></pre>

  <h3 id="prod-load-balancing">ë¡œë“œ ë°¸ëŸ°ì‹±</h3>

  <pre><code><span class="cmt"># Python ë¼ìš´ë“œ ë¡œë¹ˆ ë¡œë“œ ë°¸ëŸ°ì„œ</span>
<span class="kw">import</span> requests
<span class="kw">from</span> itertools <span class="kw">import</span> cycle

<span class="kw">class</span> <span class="type">OllamaLoadBalancer</span>:
    <span class="kw">def</span> <span class="fn">__init__</span>(<span class="kw">self</span>, servers: list):
        <span class="kw">self</span>.servers = cycle(servers)

    <span class="kw">def</span> <span class="fn">generate</span>(<span class="kw">self</span>, prompt: <span class="type">str</span>) -&gt; <span class="type">str</span>:
        server = next(<span class="kw">self</span>.servers)
        response = requests.post(
            <span class="str">f'{server}/api/generate'</span>,
            json={<span class="str">'model'</span>: <span class="str">'llama3.2'</span>, <span class="str">'prompt'</span>: prompt, <span class="str">'stream'</span>: <span class="kw">False</span>}
        )
        <span class="kw">return</span> response.json()[<span class="str">'response'</span>]


lb = OllamaLoadBalancer([
    <span class="str">'http://ollama1:11434'</span>,
    <span class="str">'http://ollama2:11434'</span>,
    <span class="str">'http://ollama3:11434'</span>
])

result = lb.generate(<span class="str">"ì•ˆë…•"</span>)  <span class="cmt"># ìë™ìœ¼ë¡œ ì„œë²„ ë¶„ì‚°</span></code></pre>
</section>

<section class="content-section">
  <h2 id="best-practices">ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤</h2>

  <h3 id="bp-security">ë³´ì•ˆ</h3>

  <ul>
    <li><strong>ì¸ì¦ ì¶”ê°€</strong>: NGINXì—ì„œ Basic Auth ë˜ëŠ” JWT ê²€ì¦</li>
    <li><strong>Rate Limiting</strong>: ë‚¨ìš© ë°©ì§€ (NGINX limit_req)</li>
    <li><strong>HTTPS</strong>: ëª¨ë“  í†µì‹  ì•”í˜¸í™”</li>
    <li><strong>ë°©í™”ë²½</strong>: í¬íŠ¸ 11434ë¥¼ ì™¸ë¶€ì— ì§ì ‘ ë…¸ì¶œí•˜ì§€ ë§ ê²ƒ</li>
    <li><strong>ì…ë ¥ ê²€ì¦</strong>: í”„ë¡¬í”„íŠ¸ ì¸ì ì…˜ ë°©ì§€</li>
  </ul>

  <h3 id="bp-prompt">í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§</h3>

  <ul>
    <li><strong>êµ¬ì²´ì ìœ¼ë¡œ</strong>: ëª¨í˜¸í•œ ì§€ì‹œ ëŒ€ì‹  ëª…í™•í•œ ìš”êµ¬ì‚¬í•­</li>
    <li><strong>ì˜ˆì‹œ ì œê³µ</strong>: Few-shot learning (2-3ê°œ ì˜ˆì œ)</li>
    <li><strong>í¬ë§· ì§€ì •</strong>: JSON, Markdown ë“± ì¶œë ¥ í˜•ì‹ ëª…ì‹œ</li>
    <li><strong>ë‹¨ê³„ë³„ ì§€ì‹œ</strong>: "ë¨¼ì € Aë¥¼ í•˜ê³ , ê·¸ ë‹¤ìŒ Bë¥¼ í•´"</li>
    <li><strong>ì—­í•  ë¶€ì—¬</strong>: "ë‹¹ì‹ ì€ X ì „ë¬¸ê°€ì…ë‹ˆë‹¤"</li>
  </ul>

  <h3 id="bp-error-handling">ì—ëŸ¬ ì²˜ë¦¬</h3>

  <pre><code><span class="kw">import</span> requests
<span class="kw">from</span> tenacity <span class="kw">import</span> retry, stop_after_attempt, wait_exponential

<span class="pp">@retry</span>(
    stop=stop_after_attempt(<span class="num">3</span>),
    wait=wait_exponential(multiplier=<span class="num">1</span>, min=<span class="num">2</span>, max=<span class="num">10</span>)
)
<span class="kw">def</span> <span class="fn">generate_with_retry</span>(prompt: <span class="type">str</span>) -&gt; <span class="type">str</span>:
    <span class="kw">try</span>:
        response = requests.post(
            <span class="str">'http://localhost:11434/api/generate'</span>,
            json={<span class="str">'model'</span>: <span class="str">'llama3.2'</span>, <span class="str">'prompt'</span>: prompt, <span class="str">'stream'</span>: <span class="kw">False</span>},
            timeout=<span class="num">30</span>
        )
        response.raise_for_status()
        <span class="kw">return</span> response.json()[<span class="str">'response'</span>]
    <span class="kw">except</span> requests.exceptions.Timeout:
        <span class="kw">print</span>(<span class="str">"Timeout, retrying..."</span>)
        <span class="kw">raise</span>
    <span class="kw">except</span> requests.exceptions.HTTPError <span class="kw">as</span> e:
        <span class="kw">print</span>(<span class="str">f"HTTP error: {e}"</span>)
        <span class="kw">raise</span></code></pre>

  <h3 id="bp-testing">í…ŒìŠ¤íŠ¸</h3>

  <pre><code><span class="kw">import</span> pytest

<span class="kw">def</span> <span class="fn">test_ollama_connection</span>():
    <span class="str">"""Ollama ì„œë²„ ì—°ê²° í…ŒìŠ¤íŠ¸"""</span>
    response = requests.get(<span class="str">'http://localhost:11434/api/tags'</span>)
    <span class="kw">assert</span> response.status_code == <span class="num">200</span>

<span class="kw">def</span> <span class="fn">test_model_generation</span>():
    <span class="str">"""ëª¨ë¸ ìƒì„± í…ŒìŠ¤íŠ¸"""</span>
    result = generate_with_retry(<span class="str">"2+2ëŠ”?"</span>)
    <span class="kw">assert</span> <span class="str">"4"</span> <span class="kw">in</span> result

<span class="kw">def</span> <span class="fn">test_rag_retrieval</span>():
    <span class="str">"""RAG ê²€ìƒ‰ í…ŒìŠ¤íŠ¸"""</span>
    rag = SimpleRAG([<span class="str">"Python is a programming language"</span>])
    docs = rag.retrieve(<span class="str">"What is Python?"</span>)
    <span class="kw">assert</span> len(docs) &gt; <span class="num">0</span></code></pre>
</section>

<section class="content-section">
  <h2 id="next-steps">ë‹¤ìŒ ë‹¨ê³„</h2>

  <p>ê³ ê¸‰ í™œìš©ë²•ì„ ë§ˆìŠ¤í„°í–ˆìŠµë‹ˆë‹¤! ì´ì œ ë¬¸ì œ í•´ê²° ë°©ë²•ì„ ë°°ì›Œë´…ì‹œë‹¤.</p>

  <div class="info-box info">
    <div class="info-box-title">ğŸ“š ê³„ì† í•™ìŠµí•˜ê¸°</div>
    <ol>
      <li><a href="ollama-troubleshooting.html">íŠ¸ëŸ¬ë¸”ìŠˆíŒ…</a> - ì¼ë°˜ì ì¸ ë¬¸ì œ í•´ê²°</li>
      <li><a href="ollama-integration.html">ë„êµ¬ ì—°ë™</a> - Continue, Aider ì„¤ì •</li>
      <li><a href="ollama-models.html">ëª¨ë¸ ì„ íƒ</a> - ìš©ë„ë³„ ìµœì  ëª¨ë¸</li>
    </ol>
  </div>

  <div class="info-box tip">
    <div class="info-box-title">ğŸš€ ì‹¤ì „ í”„ë¡œì íŠ¸</div>
    <ul>
      <li><strong>íšŒì‚¬ ë¬¸ì„œ RAG</strong>: ë‚´ë¶€ ë¬¸ì„œ ê¸°ë°˜ Q&A ì±—ë´‡ (í”„ë¼ì´ë²„ì‹œ ë³´ì¥)</li>
      <li><strong>ì½”ë“œ ë¦¬ë·° ë´‡</strong>: Git ì»¤ë°‹ ìë™ ë¦¬ë·° (Modelfile ì»¤ìŠ¤í„°ë§ˆì´ì§•)</li>
      <li><strong>ë‹¤êµ­ì–´ ë²ˆì—­ API</strong>: Ollama + FastAPI í”„ë¡œë•ì…˜ ë°°í¬</li>
      <li><strong>AI ì—ì´ì „íŠ¸</strong>: ìë™í™” ì›Œí¬í”Œë¡œìš° (Slack, Jira í†µí•©)</li>
    </ul>
  </div>
</section>

<section class="content-section">
  <h2 id="summary">í•µì‹¬ ì •ë¦¬</h2>
  <ul>
    <li>Ollama ê³ ê¸‰ í™œìš©ì˜ í•µì‹¬ ê°œë…ê³¼ íë¦„ì„ ì •ë¦¬í•©ë‹ˆë‹¤.</li>
    <li>Modelfile ì‘ì„±ë¥¼ ë‹¨ê³„ë³„ë¡œ ì´í•´í•©ë‹ˆë‹¤.</li>
    <li>ì‹¤ì „ ì ìš© ì‹œ ê¸°ì¤€ê³¼ ì£¼ì˜ì ì„ í™•ì¸í•©ë‹ˆë‹¤.</li>
  </ul>
</section>

<section class="content-section">
  <h2 id="practice-tips">ì‹¤ë¬´ íŒ</h2>
  <ul>
    <li>ì…ë ¥/ì¶œë ¥ ì˜ˆì‹œë¥¼ ê³ ì •í•´ ì¬í˜„ì„±ì„ í™•ë³´í•˜ì„¸ìš”.</li>
    <li>Ollama ê³ ê¸‰ í™œìš© ë²”ìœ„ë¥¼ ì‘ê²Œ ì¡ê³  ë‹¨ê³„ì ìœ¼ë¡œ í™•ì¥í•˜ì„¸ìš”.</li>
    <li>Modelfile ì‘ì„± ì¡°ê±´ì„ ë¬¸ì„œí™”í•´ ëŒ€ì‘ ì‹œê°„ì„ ì¤„ì´ì„¸ìš”.</li>
  </ul>
</section>
<nav class="page-nav"></nav>
</main>

<aside class="inline-toc">
  <div class="toc-title">ëª©ì°¨</div>
  <div class="toc-nav"></div>
</aside>
<footer class="site-footer"></footer>

</div>
<script src="../js/main.js"></script>
</body>
</html>
