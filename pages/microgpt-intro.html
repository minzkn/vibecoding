<!DOCTYPE html>
<html lang="ko" data-theme="dark-kernel">
<head>
<!-- BEGIN: Google adsense -->
<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-2110881342960271" crossorigin="anonymous"></script>
<!-- END: Google adsense -->
<!-- BEGIN: Google adsense repair -->
<script async src="https://fundingchoicesmessages.google.com/i/pub-2110881342960271?ers=1" nonce="laI6FT8gpRxugDJv5AGJRA"></script><script nonce="laI6FT8gpRxugDJv5AGJRA">(function() {function signalGooglefcPresent() {if (!window.frames['googlefcPresent']) {if (document.body) {const iframe = document.createElement('iframe'); iframe.style = 'width: 0; height: 0; border: none; z-index: -1000; left: -1000px; top: -1000px;'; iframe.style.display = 'none'; iframe.name = 'googlefcPresent'; document.body.appendChild(iframe);} else {setTimeout(signalGooglefcPresent, 0);}}}signalGooglefcPresent();})();</script>
<!-- END: Google adsense repair -->
<!-- BEGIN: Google analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-1VWQF060SX"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-1VWQF060SX');
</script>
<!-- END: Google analytics -->
<script>
(function(){var m=document.cookie.match(/claude_theme=([^;]+)/);if(m)document.documentElement.setAttribute('data-theme',m[1]);})();
</script>
<meta charset="UTF-8">
<meta property="og:type" content="article">
<meta property="og:site_name" content="AI Vibe Coding 가이드 /with MINZKN">
<meta property="og:title" content="GPT를 밑바닥부터 (1): 저자·데이터·Autograd — microgpt.py 완전 해설">
<meta property="og:description" content="Andrej Karpathy의 microgpt.py — 순수 파이썬 200줄, 4,192개 파라미터. 저자 소개, 데이터 파이프라인, 토크나이저, Autograd 엔진 완전 해설.">
<meta property="og:url" content="https://minzkn.com/claude/pages/microgpt-intro.html">
<meta property="og:image" content="https://minzkn.com/claude/images/og-image.png">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="Andrej Karpathy의 microgpt.py — 순수 파이썬 200줄, 외부 ML 라이브러리 없이 GPT 완전 구현. 저자 소개, 데이터 파이프라인, 토크나이저, Autograd 엔진을 상세 해설합니다.">
<meta name="keywords" content="microgpt karpathy gpt 밑바닥 autograd value 역전파 토크나이저 순수파이썬 micrograd names.txt bos">
<meta name="author" content="MINZKN">
<title>GPT를 밑바닥부터 (1): 저자·데이터·Autograd - AI Vibe Coding 가이드 /with MINZKN</title>
<link rel="icon" type="image/svg+xml" href="../images/favicon.svg">
<link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/joungkyun/font-d2coding/d2coding.css">
<link rel="stylesheet" href="../css/themes.css">
<link rel="stylesheet" href="../css/style.css">
<link rel="stylesheet" href="../css/responsive.css">
</head>
<body>
<div class="page-wrapper">
<header class="site-header"></header>
<nav class="side-nav" aria-label="사이트 내비게이션"></nav>
<main class="main-content">
<div class="breadcrumb"></div>

<h1 id="top">GPT를 밑바닥부터 (1): 저자·데이터·Autograd</h1>
<p class="page-description">Andrej Karpathy의 microgpt.py — 순수 파이썬(os, math, random) 200줄, 외부 ML 라이브러리 0개. 저자 소개, 전체 코드 흐름, 데이터 파이프라인, 토크나이저, Autograd 엔진을 완전 해설합니다.</p>

<section class="content-section">
  <h2 id="overview">개요</h2>
  <p><strong>microgpt.py</strong>는 Andrej Karpathy가 2026년 2월 공개한 "가장 원자적인 GPT 구현"입니다.
  <code>os</code>, <code>math</code>, <code>random</code> 세 표준 모듈만으로 GPT를 완성합니다.</p>
  <div class="info-box tip">
    <strong>핵심 철학:</strong> "This file contains the full algorithmic content of what is needed:
    dataset · tokenizer · autograd engine · GPT-2-like architecture · Adam optimizer · training loop · inference loop.
    Everything else is just efficiency. I cannot simplify this any further."
    — Andrej Karpathy, 2026.02.12
  </div>
  <table>
    <thead><tr><th>항목</th><th>값</th><th>의미</th></tr></thead>
    <tbody>
      <tr><td>파라미터</td><td><strong>4,192개</strong></td><td>학습 가능한 숫자 전체</td></tr>
      <tr><td>코드 줄 수</td><td>~200줄</td><td>데이터부터 추론까지 한 파일</td></tr>
      <tr><td>학습 문서(이름)</td><td>32,033개</td><td>names.txt 영어 이름 데이터셋</td></tr>
      <tr><td>외부 ML 라이브러리</td><td><strong>0개</strong></td><td>os, math, random만 사용</td></tr>
      <tr><td>학습 결과</td><td>loss 3.30 → 2.37</td><td>Perplexity 27 → 10.7 (약 2.5배 향상)</td></tr>
    </tbody>
  </table>
  <p>이 시리즈는 3부로 구성됩니다:</p>
  <ul>
    <li><strong>1부 (현재)</strong>: 저자 소개, 전체 흐름, 데이터 파이프라인, 토크나이저, Autograd 엔진</li>
    <li><a href="microgpt-architecture.html">2부</a>: 모델 설계도, 임베딩, Attention, 트랜스포머 블록, GPT 파이프라인</li>
    <li><a href="microgpt-training.html">3부</a>: 손실 계산, Adam 옵티마이저, 추론, LLM 비교, 요약</li>
  </ul>
</section>

<section class="content-section">
  <h2 id="author">저자 — Andrej Karpathy</h2>
  <dl>
    <dt>학력</dt>
    <dd><strong>Stanford PhD</strong> — Fei-Fei Li 지도 하에 Computer Vision · Deep Learning 전공.
    박사 논문 주제: "Connecting Images and Natural Language" (이미지-언어 연결).
    재학 중 Google Brain · DeepMind에서 인턴.</dd>

    <dt>커리어</dt>
    <dd>
      <strong>OpenAI 창립 멤버(2015)</strong> — GPT 시리즈 초기 연구에 직접 참여.<br>
      <strong>Tesla AI 디렉터(2017~2022)</strong> — Autopilot의 신경망 인식 시스템 총괄.
      "Data Engine" 개념(모델이 스스로 학습 데이터를 생성)을 실제 자율주행에 적용.<br>
      <strong>OpenAI 복귀(2023)</strong> — AGI 안전성 연구.<br>
      <strong>독립(2024~)</strong> — AI 교육 콘텐츠에 집중.
    </dd>

    <dt>교육 철학</dt>
    <dd><strong>"Everything else is just efficiency"</strong> —
    복잡한 AI를 의존성 없는 가장 원자적 형태로 단순화함으로써, 블랙박스를 투명하게 만드는 교육자.
    "이해하려면 직접 만들어봐야 한다"는 원칙으로 수만 명에게 딥러닝의 문을 열었습니다.</dd>
  </dl>

  <h3 id="lineage">프로젝트 계보 — 10년의 단순화 여정</h3>
  <p>각 프로젝트는 "한 가지를 더 단순하게"라는 목표로 이어집니다.</p>
  <table>
    <thead>
      <tr><th>프로젝트</th><th>연도</th><th>핵심 아이디어</th><th>microgpt와의 관계</th></tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>micrograd</strong></td>
        <td>2020</td>
        <td>스칼라 Autograd — Value 클래스로 역전파 구현</td>
        <td>microgpt <code>Value</code> 클래스의 직접적 원형</td>
      </tr>
      <tr>
        <td><strong>makemore</strong></td>
        <td>2022</td>
        <td>문자 수준 언어 모델, names.txt 데이터셋</td>
        <td>데이터셋·토크나이저 구조 계승</td>
      </tr>
      <tr>
        <td><strong>nanoGPT</strong></td>
        <td>2022</td>
        <td>PyTorch+GPU 기반 실용 GPT 훈련 코드베이스 — GPT-2(124M) 완전 재현, Shakespeare 예제, DDP 분산학습, Flash Attention</td>
        <td>GPT 아키텍처의 "Production 최소 구현" — microgpt가 배운 원리를 실제 규모로 확장</td>
      </tr>
      <tr>
        <td><strong>llm.c</strong></td>
        <td>2024</td>
        <td>C/CUDA로 LLM 훈련 — NumPy·PyTorch 없이</td>
        <td>의존성 제거 철학 계승</td>
      </tr>
      <tr>
        <td><strong>microgpt</strong></td>
        <td>2026</td>
        <td>순수 파이썬 200줄 — 모든 것을 하나의 파일에</td>
        <td>집대성</td>
      </tr>
    </tbody>
  </table>
</section>

<section class="content-section">
  <h2 id="flow">전체 코드 흐름</h2>
  <pre><code>【입력】 names.txt (32,033개 이름)
    │
    ▼
【Phase 1 — 데이터 파이프라인】 (~20줄)
    ├─ import os/math/random
    ├─ random.shuffle(docs)          ← 학습 순서 무작위화
    ├─ uchars = sorted(set(''.join(docs)))  ← 고유 문자 추출
    └─ vocab_size = 27 (a~z + BOS)

    │  각 이름 "emma" → [26, 4, 12, 12, 0, 26]
    ▼
【Phase 2 — Autograd 엔진】 (~40줄)
    ├─ class Value: data, grad, _children, _local_grads
    ├─ 6가지 연산: add, mul, pow, log, exp, relu
    └─ backward(): 위상 정렬 → 연쇄 법칙 적용

    │  모든 숫자가 미분 가능한 Value 객체가 됨
    ▼
【Phase 3 — 신경망 정의】 (~50줄)
    ├─ state_dict: 4,192개 Value 파라미터
    ├─ linear(), softmax(), rmsnorm()
    └─ gpt(token_id, pos_id, keys, values) → logits[27]

    │  토큰 → 다음 토큰 확률 분포
    ▼
【Phase 4 — 학습 루프 (1,000 스텝)】 (~30줄)
    ├─ 순전파: loss = CrossEntropy(gpt(tokens), targets)
    ├─ 역전파: loss.backward()
    ├─ Adam: p.data -= lr * m_hat / (v_hat**0.5 + ε)
    └─ lr 선형 감쇠: 0.01 → 0.00001

    │  loss: 3.30 → 2.37
    ▼
【Phase 5 — 추론 (이름 생성)】
    BOS → 't' → 'o' → 'm' → BOS  →  "tom"</code></pre>

  <div class="info-box info">
    <strong>왜 이 순서인가?</strong> 데이터 없이는 토크나이저가 없고, Autograd 없이는 학습이 없고,
    모델 없이는 추론이 없습니다. 각 블록이 다음 블록의 토대가 되는 완벽한 의존성 체인입니다.
    GPT-4도 이 4단계 구조를 따릅니다 — 규모만 다를 뿐입니다.
  </div>
</section>

<section class="content-section">
  <h2 id="data">임포트 &amp; 데이터 로딩</h2>
  <p>microgpt.py가 사용하는 임포트는 단 세 줄입니다.
  ML 라이브러리가 없다는 것 자체가 이 프로젝트의 핵심 철학입니다.</p>

  <pre><code><span class="cmt"># 수학적 원리를 라이브러리 뒤에 숨기지 않고 직접 보여주기 위함</span>
<span class="kw">import</span> os        <span class="cmt"># os.path.exists — 파일 존재 여부 확인</span>
<span class="kw">import</span> math      <span class="cmt"># math.log, math.exp — 수학 함수</span>
<span class="kw">import</span> random    <span class="cmt"># random.seed, random.gauss, random.choices</span>

random.<span class="fn">seed</span>(<span class="num">42</span>)  <span class="cmt"># 재현 가능한 결과 — 동일한 random.seed → 동일한 학습 결과</span></code></pre>

  <h3 id="dataset">데이터셋 — makemore 저장소</h3>
  <p><strong>데이터 출처:</strong> <code>names.txt</code>는 Karpathy의 makemore 프로젝트의 영어 이름 데이터셋입니다.
  약 <strong>32,000개</strong>의 고유 이름이 포함되어 있습니다.</p>
  <pre><code><span class="kw">if not</span> os.path.<span class="fn">exists</span>(<span class="str">'input.txt'</span>):
    <span class="kw">import</span> urllib.request
    names_url = <span class="str">'https://raw.githubusercontent.com/karpathy/makemore/988aa59/names.txt'</span>
    urllib.request.<span class="fn">urlretrieve</span>(names_url, <span class="str">'input.txt'</span>)

docs = [line.<span class="fn">strip</span>() <span class="kw">for</span> line <span class="kw">in</span> <span class="fn">open</span>(<span class="str">'input.txt'</span>) <span class="kw">if</span> line.<span class="fn">strip</span>()]
random.<span class="fn">shuffle</span>(docs)  <span class="cmt"># 순서 섞기 — 학습 편향 방지</span>
<span class="fn">print</span>(<span class="str">f"num docs: {len(docs)}"</span>)  <span class="cmt"># → num docs: 32033</span></code></pre>

  <p>input.txt — 각 줄이 하나의 이름(= 하나의 훈련 문서):</p>
  <pre><code>emma        ← 4글자
olivia      ← 6글자
ava         ← 3글자
isabella    ← 8글자
sophia      ← 6글자
...
(총 32,033줄)</code></pre>

  <table>
    <thead><tr><th>특성</th><th>값</th><th>이유</th></tr></thead>
    <tbody>
      <tr><td>총 이름 수</td><td>32,033개</td><td>미국 신생아 이름 통계 기반</td></tr>
      <tr><td>문자 종류</td><td>소문자 a~z (26종)</td><td>대소문자·숫자·특수문자 없음</td></tr>
      <tr><td>최단 이름</td><td>2글자 (예: "jo")</td><td>–</td></tr>
      <tr><td>최장 이름</td><td>15글자</td><td>block_size=16 설정 근거</td></tr>
      <tr><td>평균 길이</td><td>약 5~6글자</td><td>–</td></tr>
    </tbody>
  </table>

  <h3 id="shuffle-reason">random.shuffle — 왜 섞는가?</h3>
  <p>학습 루프는 <code>docs[step % len(docs)]</code>로 순환합니다.
  섞지 않으면 처음 1,000 스텝 동안 항상 같은 이름 순서로 학습하게 됩니다.
  데이터셋의 앞부분에 특정 패턴(예: 짧은 이름, 특정 알파벳으로 시작하는 이름)이
  몰려 있으면 모델이 그 패턴에 편향됩니다.</p>
  <pre><code>random.<span class="fn">seed</span>(<span class="num">42</span>)       <span class="cmt"># ← 먼저 시드를 설정한 뒤</span>
random.<span class="fn">shuffle</span>(docs)  <span class="cmt"># ← 섞어야 재현 가능한 순서가 됩니다</span></code></pre>

  <h3 id="batch-size-1">배치 크기 1 — 왜 이름 하나씩 학습하는가?</h3>
  <p>microgpt는 한 번에 이름 하나(batch_size=1)를 학습합니다.
  배치 크기가 크면 GPU 메모리와 병렬 연산이 필요하지만,
  microgpt는 CPU에서 스칼라 연산으로 작동하므로 배치 크기 1이 가장 단순합니다.</p>
  <div class="info-box info">
    <strong>학습 vs 추론의 차이:</strong>
    학습 시에는 이름 전체 토큰을 한 번에 순전파합니다(각 위치에서 다음 토큰 예측).
    추론 시에는 BOS부터 시작해서 한 번에 토큰 하나씩 생성합니다.
  </div>

  <h3 id="why-names">왜 이름 데이터인가?</h3>
  <ul>
    <li><strong>검증이 쉽다:</strong> 생성된 이름이 발음 가능한지 사람이 즉시 판단 가능.</li>
    <li><strong>규모가 적당하다:</strong> 32,033개 × 평균 5글자 = 약 160,000 토큰 — 노트북으로 1분 내 학습.</li>
    <li><strong>패턴이 명확하다:</strong> 이름은 모음-자음 교대, 특정 접미사(-a, -on, -er) 등 학습 가능한 통계 패턴이 있음.</li>
    <li><strong>범용성:</strong> "이름"이나 "대화"나 GPT 입장에서는 모두 토큰 시퀀스. 원리는 동일.</li>
  </ul>
</section>

<section class="content-section">
  <h2 id="tokenizer">토크나이저 구축</h2>
  <p>텍스트를 숫자로, 숫자를 다시 텍스트로 변환하는 번역기를 만듭니다.
  microgpt.py는 <strong>문자 수준(character-level)</strong> 토크나이저를 사용합니다.</p>

  <pre><code><span class="cmt"># 모든 이름을 합쳐 고유한 문자만 추출 (정렬)</span>
uchars = <span class="fn">sorted</span>(<span class="fn">set</span>(<span class="str">''</span>.<span class="fn">join</span>(docs)))
<span class="cmt"># 결과: ['a', 'b', 'c', ..., 'z'] → 26개 소문자</span>

BOS = <span class="fn">len</span>(uchars)            <span class="cmt"># = 26, Beginning of Sequence 특수 토큰 ID</span>
vocab_size = <span class="fn">len</span>(uchars) + <span class="num">1</span>  <span class="cmt"># = 27 (a~z 26개 + BOS 1개)</span>

<span class="cmt"># 인코딩: uchars.index(ch)  디코딩: uchars[token_id]</span>

<span class="cmt"># 예시: "emma" → [BOS, e, m, m, a, BOS]</span>
name = <span class="str">"emma"</span>
tokens = [BOS] + [uchars.<span class="fn">index</span>(ch) <span class="kw">for</span> ch <span class="kw">in</span> name] + [BOS]
<span class="cmt"># → [26, 4, 12, 12, 0, 26]</span>

<span class="cmt"># 디코딩: 정수 → 문자 (BOS 제외)</span>
decoded = <span class="str">''</span>.<span class="fn">join</span>(uchars[t] <span class="kw">for</span> t <span class="kw">in</span> tokens <span class="kw">if</span> t != BOS)
<span class="cmt"># → "emma"</span></code></pre>

  <h3 id="bos-wrap">BOS 래핑 — 문서 경계 표현</h3>
  <pre><code><span class="str">"emma"</span> → tokens = [<span class="num">26</span>, <span class="num">4</span>, <span class="num">12</span>, <span class="num">12</span>, <span class="num">0</span>, <span class="num">26</span>]

<span class="cmt">【학습 입력/타겟 쌍】</span>
pos 0: 입력=BOS(26)  → 타겟=<span class="str">'e'</span>(4)   ← "이름의 시작엔 무엇이 오는가?"
pos 1: 입력=<span class="str">'e'</span>(4)   → 타겟=<span class="str">'m'</span>(12)  ← "e 다음엔 무엇이 오는가?"
pos 2: 입력=<span class="str">'m'</span>(12)  → 타겟=<span class="str">'m'</span>(12)  ← "em 다음엔?"
pos 3: 입력=<span class="str">'m'</span>(12)  → 타겟=<span class="str">'a'</span>(0)   ← "emm 다음엔?"
pos 4: 입력=<span class="str">'a'</span>(0)   → 타겟=BOS(<span class="num">26</span>)  ← "emma 다음엔? → 끝!"</code></pre>

  <div class="info-box info">
    <strong>BOS를 양쪽에 붙이는 이유:</strong>
    앞에 BOS → 모델이 "이름이 시작된다"는 신호를 받고 첫 글자를 예측.
    뒤에 BOS → 모델이 "이름이 끝났다"는 것을 학습(추론 시 생성 종료 조건).
    BOS 하나가 시작 신호이자 종료 신호 두 역할을 모두 합니다.
  </div>

  <h3 id="token-ids">토큰 ID 구조 — 27개 심볼</h3>
  <table>
    <thead><tr><th>ID</th><th>심볼</th><th>ID</th><th>심볼</th><th>ID</th><th>심볼</th></tr></thead>
    <tbody>
      <tr><td>0</td><td>'a'</td><td>9</td><td>'j'</td><td>18</td><td>'s'</td></tr>
      <tr><td>1</td><td>'b'</td><td>10</td><td>'k'</td><td>19</td><td>'t'</td></tr>
      <tr><td>2</td><td>'c'</td><td>11</td><td>'l'</td><td>20</td><td>'u'</td></tr>
      <tr><td>3</td><td>'d'</td><td>12</td><td>'m'</td><td>21</td><td>'v'</td></tr>
      <tr><td>4</td><td>'e'</td><td>13</td><td>'n'</td><td>22</td><td>'w'</td></tr>
      <tr><td>5</td><td>'f'</td><td>14</td><td>'o'</td><td>23</td><td>'x'</td></tr>
      <tr><td>6</td><td>'g'</td><td>15</td><td>'p'</td><td>24</td><td>'y'</td></tr>
      <tr><td>7</td><td>'h'</td><td>16</td><td>'q'</td><td>25</td><td>'z'</td></tr>
      <tr><td>8</td><td>'i'</td><td>17</td><td>'r'</td><td><strong>26</strong></td><td><strong>BOS</strong></td></tr>
    </tbody>
  </table>

  <table>
    <thead><tr><th>항목</th><th>microgpt (문자 수준)</th><th>tiktoken / BPE (프로덕션)</th></tr></thead>
    <tbody>
      <tr><td>어휘집 크기</td><td>27개 (a~z + BOS)</td><td>~100,000개</td></tr>
      <tr><td>특징</td><td>구현 단순, 이해 쉬움</td><td>위치당 더 많은 의미 압축</td></tr>
      <tr><td>인코딩 방식</td><td><code>uchars.index(ch)</code> 사용</td><td>자주 쓰이는 문자열 병합(BPE)</td></tr>
      <tr><td>인코딩 복잡도</td><td>O(n) 선형 탐색</td><td>O(1) 딕셔너리 룩업</td></tr>
      <tr><td>용도</td><td>소규모 교육용에 적합</td><td>GPT-4 등 실제 LLM에서 사용</td></tr>
    </tbody>
  </table>

  <h3 id="tokenizer-pitfalls">흔한 실수들</h3>
  <ul>
    <li>
      <strong>Off-by-One 오류:</strong>
      어휘집 크기를 26으로 잘못 지정하면 BOS(ID=26)를 임베딩 테이블에서 찾지 못해 IndexError 발생.
      <code>wte = matrix(vocab_size, n_embd)</code>에서 <code>vocab_size=27</code>이어야 합니다.
    </li>
    <li>
      <strong>일관성 없는 어휘집:</strong>
      소문자로 훈련된 모델에 대문자("Emma")를 입력하면 <code>uchars.index('E')</code>가 ValueError를 냅니다.
      모든 이름은 <strong>소문자 전처리</strong> 후 토크나이저에 입력해야 합니다.
    </li>
    <li>
      <strong>uchars.index() 시간 복잡도:</strong>
      <code>list.index()</code>는 O(n) 선형 탐색입니다. 어휘 크기가 27개로 작기 때문에 문제없지만,
      vocab_size가 50,000이라면 반드시 dict 기반 룩업테이블로 바꿔야 합니다.
    </li>
  </ul>
</section>

<section class="content-section">
  <h2 id="autograd">Autograd 엔진 — 학습의 심장</h2>
  <p>PyTorch의 핵심 기능인 <strong>자동 미분(Automatic Differentiation)</strong>을 밑바닥부터 직접 구현합니다.
  모든 숫자가 그냥 숫자가 아니라, <em>자신이 어디서 왔는지</em>를 기억하는 <code>Value</code> 객체가 됩니다.</p>

  <div class="info-box tip">
    <strong>핵심 아이디어:</strong> 순전파(forward pass) 중에 계산 그래프를 동적으로 구성하고,
    역전파(backward pass) 시 연쇄 법칙(Chain Rule)을 적용해 모든 파라미터의 기울기를 자동으로 계산합니다.
  </div>

  <h3 id="chain-rule">연쇄 법칙 (Chain Rule)</h3>
  <p><strong>dy/dx = (dy/du) · (du/dx)</strong> — 미적분학의 핵심 — 복합 함수의 미분</p>
  <dl>
    <dt><code>self.grad</code> — 전역 미분값</dt>
    <dd>d(Loss)/d(self) — 역전파 시 계산됨</dd>
    <dt><code>_local_grads</code> — 지역 미분값</dt>
    <dd>d(self)/d(child) — 순전파 시 계산됨</dd>
  </dl>

  <h3 id="ops-table">6가지 연산의 지역 기울기 — 한눈에 보기</h3>
  <table>
    <thead>
      <tr><th>연산</th><th>순전파 f(a, b)</th><th>지역 기울기 ∂f/∂a</th><th>직관</th></tr>
    </thead>
    <tbody>
      <tr><td>add</td><td>a + b</td><td>1 (∂f/∂b = 1)</td><td>덧셈은 기울기를 그대로 흘림</td></tr>
      <tr><td>mul</td><td>a × b</td><td>b (∂f/∂b = a)</td><td>상대방 값이 기울기 배율</td></tr>
      <tr><td>pow</td><td>a<sup>n</sup></td><td>n · a<sup>n−1</sup></td><td>지수 법칙</td></tr>
      <tr><td>log</td><td>ln(a)</td><td>1 / a</td><td>로그의 도함수</td></tr>
      <tr><td>exp</td><td>e<sup>a</sup></td><td>e<sup>a</sup> (자기 자신!)</td><td>e^x의 도함수 = e^x</td></tr>
      <tr><td>relu</td><td>max(0, a)</td><td>1 if a &gt; 0 else 0</td><td>양수만 기울기 통과</td></tr>
    </tbody>
  </table>
  <div class="info-box info">
    <strong>연쇄 법칙 직관 — 속도 비유:</strong>
    자동차가 자전거보다 <strong>2배</strong> 빠르고, 자전거가 사람보다 <strong>4배</strong> 빠르다면 →
    자동차는 사람보다 <strong>2 × 4 = 8배</strong> 빠릅니다.
    dL/dx = dL/dy · dy/dx — 각 단계의 기울기를 <strong>곱하면</strong> 전체 기울기가 됩니다.
  </div>

  <h3 id="value-class">Value 클래스 전체 코드</h3>
  <pre><code><span class="kw">class</span> <span class="type">Value</span>:
    __slots__ = (<span class="str">'data'</span>, <span class="str">'grad'</span>, <span class="str">'_children'</span>, <span class="str">'_local_grads'</span>)

    <span class="kw">def</span> <span class="fn">__init__</span>(self, data, children=(), local_grads=()):
        self.data = data          <span class="cmt"># float: 순전파 결과값</span>
        self.grad = <span class="num">0</span>             <span class="cmt"># float: d(Loss)/d(self), 역전파 시 채워짐</span>
        self._children = children         <span class="cmt"># tuple[Value]: 이 노드를 만든 입력들</span>
        self._local_grads = local_grads   <span class="cmt"># tuple[float]: d(self)/d(child)</span>

    <span class="cmt"># ── 핵심 이진 연산 두 개 (나머지는 모두 이 두 개의 조합) ──</span>
    <span class="kw">def</span> <span class="fn">__add__</span>(self, other):
        other = other <span class="kw">if</span> <span class="fn">isinstance</span>(other, <span class="type">Value</span>) <span class="kw">else</span> <span class="type">Value</span>(other)
        <span class="cmt"># f = a + b  →  ∂f/∂a = 1,  ∂f/∂b = 1</span>
        <span class="kw">return</span> <span class="type">Value</span>(self.data + other.data, (self, other), (<span class="num">1</span>, <span class="num">1</span>))

    <span class="kw">def</span> <span class="fn">__mul__</span>(self, other):
        other = other <span class="kw">if</span> <span class="fn">isinstance</span>(other, <span class="type">Value</span>) <span class="kw">else</span> <span class="type">Value</span>(other)
        <span class="cmt"># f = a × b  →  ∂f/∂a = b,  ∂f/∂b = a</span>
        <span class="kw">return</span> <span class="type">Value</span>(self.data * other.data, (self, other), (other.data, self.data))

    <span class="cmt"># ── 나머지 기본 연산 ──</span>
    <span class="kw">def</span> <span class="fn">__pow__</span>(self, other):
        <span class="cmt"># f = a^n  →  ∂f/∂a = n·a^(n-1)</span>
        <span class="kw">return</span> <span class="type">Value</span>(self.data**other, (self,), (other * self.data**(other-<span class="num">1</span>),))

    <span class="kw">def</span> <span class="fn">log</span>(self):
        <span class="cmt"># f = ln(a)  →  ∂f/∂a = 1/a</span>
        <span class="kw">return</span> <span class="type">Value</span>(math.<span class="fn">log</span>(self.data), (self,), (<span class="num">1</span>/self.data,))

    <span class="kw">def</span> <span class="fn">exp</span>(self):
        <span class="cmt"># f = e^a  →  ∂f/∂a = e^a  (도함수가 자기 자신!)</span>
        <span class="kw">return</span> <span class="type">Value</span>(math.<span class="fn">exp</span>(self.data), (self,), (math.<span class="fn">exp</span>(self.data),))

    <span class="kw">def</span> <span class="fn">relu</span>(self):
        <span class="cmt"># f = max(0, a)  →  ∂f/∂a = 1 if a>0 else 0</span>
        <span class="kw">return</span> <span class="type">Value</span>(<span class="fn">max</span>(<span class="num">0</span>, self.data), (self,), (<span class="fn">float</span>(self.data &gt; <span class="num">0</span>),))

    <span class="cmt"># ── 파생 연산 (위 연산들의 조합으로 자동 처리) ──</span>
    <span class="kw">def</span> <span class="fn">__neg__</span>(self):   <span class="kw">return</span> self * -<span class="num">1</span>           <span class="cmt"># -a = a × (-1)</span>
    <span class="kw">def</span> <span class="fn">__sub__</span>(self, other):  <span class="kw">return</span> self + (-other)    <span class="cmt"># a - b = a + (-b)</span>
    <span class="kw">def</span> <span class="fn">__rsub__</span>(self, other): <span class="kw">return</span> other + (-self)    <span class="cmt"># b - a</span>
    <span class="kw">def</span> <span class="fn">__radd__</span>(self, other): <span class="kw">return</span> self + other       <span class="cmt"># b + a (sum()에서 필요)</span>
    <span class="kw">def</span> <span class="fn">__rmul__</span>(self, other): <span class="kw">return</span> self * other       <span class="cmt"># b × a</span>
    <span class="kw">def</span> <span class="fn">__truediv__</span>(self, other): <span class="kw">return</span> self * other**-<span class="num">1</span>  <span class="cmt"># a/b = a × b^(-1)</span>
    <span class="kw">def</span> <span class="fn">__rtruediv__</span>(self, other): <span class="kw">return</span> other * self**-<span class="num">1</span> <span class="cmt"># b/a</span></code></pre>

  <h3 id="slots">__slots__ — 메모리 최적화</h3>
  <p>파이썬 객체는 기본적으로 <code>__dict__</code>라는 딕셔너리로 속성을 저장합니다.
  <code>__slots__</code>를 선언하면 이 딕셔너리 대신 고정된 슬롯(C 배열)을 사용합니다.</p>
  <table>
    <thead><tr><th>방식</th><th>인스턴스당 메모리</th><th>4,192개 기준</th></tr></thead>
    <tbody>
      <tr><td>__dict__ (기본)</td><td>~200~400 bytes</td><td>~1.5 MB</td></tr>
      <tr><td>__slots__ (microgpt)</td><td>~100~150 bytes</td><td>~0.6 MB</td></tr>
    </tbody>
  </table>
  <p>학습 중에는 순전파 시 파라미터 수의 수십~수백 배의 중간 Value 노드가 생성됩니다.
  메모리 절약 외에도 속성 접근 속도도 향상됩니다.</p>

  <h3 id="radd">__radd__ 가 필요한 이유</h3>
  <pre><code><span class="cmt"># Python의 sum()은 내부적으로 0 + first_element 를 실행합니다</span>
<span class="cmt"># → 0.__add__(Value) → int는 Value를 모르므로 NotImplemented 반환</span>
<span class="cmt"># → Python이 Value.__radd__(0) 를 대신 호출</span>
<span class="cmt"># → self + other = Value + 0 → 정상 작동</span>

total = <span class="fn">sum</span>(losses)   <span class="cmt"># sum([Value, Value, ...]) → 내부적으로 0 + Value(...) 실행</span>
<span class="cmt"># __radd__ 없으면 TypeError!</span></code></pre>

  <h3 id="backward">역전파 — backward()</h3>
  <pre><code><span class="kw">def</span> <span class="fn">backward</span>(self):
    <span class="cmt"># 1) 위상 정렬(Topological Sort)로 그래프 순회</span>
    topo = []
    visited = <span class="fn">set</span>()

    <span class="kw">def</span> <span class="fn">build_topo</span>(v):
        <span class="kw">if</span> v <span class="kw">not in</span> visited:
            visited.<span class="fn">add</span>(v)
            <span class="kw">for</span> child <span class="kw">in</span> v._children:
                <span class="fn">build_topo</span>(child)
            topo.<span class="fn">append</span>(v)   <span class="cmt"># 자식 처리 후 자신을 추가 (후위 순회)</span>

    <span class="fn">build_topo</span>(self)

    <span class="cmt"># 2) 역순으로 기울기 전파</span>
    self.grad = <span class="num">1</span>  <span class="cmt"># d(Loss)/d(Loss) = 1 (역전파 출발점)</span>
    <span class="kw">for</span> v <span class="kw">in</span> <span class="fn">reversed</span>(topo):
        <span class="kw">for</span> child, local_grad <span class="kw">in</span> <span class="fn">zip</span>(v._children, v._local_grads):
            child.grad += local_grad * v.grad  <span class="cmt"># += 누적! (다변수 연쇄 법칙)</span></code></pre>

  <div class="info-box tip">
    <strong>위상 정렬(Topological Sort)의 비밀:</strong> 계산 그래프를 역순으로 순회함으로써,
    모든 child 노드의 기울기를 정확히 계산할 수 있습니다. 이것이 PyTorch의 <code>torch.autograd</code>가
    내부적으로 수행하는 작업과 동일한 원리입니다.
  </div>

  <h3 id="backprop-example">역전파 단계별 추적 — <code>L = (a * b) + a</code></h3>
  <p>a = 2.0, b = 3.0인 경우. 수학적으로 dL/da = b + 1 = 4.0, dL/db = a = 2.0</p>
  <pre><code><span class="cmt"># ── 순전파 ──</span>
a = <span class="type">Value</span>(<span class="num">2.0</span>)   <span class="cmt"># 파라미터</span>
b = <span class="type">Value</span>(<span class="num">3.0</span>)   <span class="cmt"># 파라미터</span>
c = a * b        <span class="cmt"># c.data=6.0, c._children=(a,b), c._local_grads=(3.0, 2.0)</span>
                 <span class="cmt">#   ∂c/∂a = b.data = 3.0,  ∂c/∂b = a.data = 2.0</span>
L = c + a        <span class="cmt"># L.data=8.0, L._children=(c,a), L._local_grads=(1, 1)</span>
                 <span class="cmt">#   ∂L/∂c = 1,  ∂L/∂a = 1</span>

<span class="cmt"># ── 위상 정렬 결과 (DFS 후위 순회) ──</span>
<span class="cmt"># topo = [a, b, c, L]  ← 의존성 순서</span>
<span class="cmt"># reversed(topo) = [L, c, b, a]  ← 역전파 순서</span>

<span class="cmt"># ── 역전파 ──</span>
L.grad = <span class="num">1</span>   <span class="cmt"># d(L)/d(L) = 1 (출발점)</span>

<span class="cmt"># v=L: children=(c,a), local_grads=(1, 1)</span>
<span class="cmt"># c.grad += 1 * L.grad  = 1 * 1 = 1.0</span>
<span class="cmt"># a.grad += 1 * L.grad  = 1 * 1 = 1.0   (L = c + a 에서 a 경로)</span>

<span class="cmt"># v=c: children=(a,b), local_grads=(3.0, 2.0)</span>
<span class="cmt"># a.grad += 3.0 * c.grad = 3.0 * 1.0    → a.grad = 1.0 + 3.0 = 4.0</span>
<span class="cmt"># b.grad += 2.0 * c.grad = 2.0 * 1.0    → b.grad = 0.0 + 2.0 = 2.0</span>

<span class="cmt"># 최종: a.grad=4.0 ✓, b.grad=2.0 ✓ (수학 계산과 일치)</span></code></pre>

  <h3 id="comp-graph">계산 그래프 시각화</h3>
  <pre><code>       a(2.0)    b(3.0)
        │  \      │
        │   ×─────┘  ← __mul__: local_grads=(b, a)=(3.0, 2.0)
        │    \
        │     c(6.0)
        │      │
        └──────+      ← __add__: local_grads=(1, 1)
               │
              L(8.0)   ← loss.backward() 시작점, L.grad=1

역전파 방향: L → c,a → a,b  (화살표 반대 방향으로 grad 전파)</code></pre>

  <div class="diagram-container">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 700 260"
         style="width:100%;max-width:700px;display:block;margin:1.5em auto;background:var(--diagram-fill);border-radius:8px;padding:20px;">
      <defs>
        <marker id="microgpt-intro-arrow" viewBox="0 0 10 7" refX="10" refY="3.5"
                markerWidth="10" markerHeight="7" orient="auto">
          <polygon points="0 0, 10 3.5, 0 7" fill="var(--diagram-arrow)"/>
        </marker>
      </defs>
      <rect x="20" y="80" width="80" height="40" rx="6"
            fill="var(--bg-secondary)" stroke="var(--accent-primary)" stroke-width="2"/>
      <text x="60" y="97" text-anchor="middle" fill="var(--diagram-text)" font-size="12" font-family="monospace">a=2.0</text>
      <text x="60" y="113" text-anchor="middle" fill="var(--accent-secondary)" font-size="10">grad=4.0</text>
      <rect x="20" y="160" width="80" height="40" rx="6"
            fill="var(--bg-secondary)" stroke="var(--border-color)" stroke-width="1.5"/>
      <text x="60" y="177" text-anchor="middle" fill="var(--diagram-text)" font-size="12" font-family="monospace">b=3.0</text>
      <text x="60" y="193" text-anchor="middle" fill="var(--accent-secondary)" font-size="10">grad=2.0</text>
      <rect x="230" y="120" width="100" height="40" rx="6"
            fill="var(--bg-secondary)" stroke="var(--border-color)" stroke-width="1.5"/>
      <text x="280" y="137" text-anchor="middle" fill="var(--diagram-text)" font-size="11" font-family="monospace">c=a×b=6.0</text>
      <text x="280" y="153" text-anchor="middle" fill="var(--accent-secondary)" font-size="10">grad=1.0</text>
      <rect x="460" y="100" width="120" height="40" rx="6"
            fill="var(--bg-secondary)" stroke="var(--accent-primary)" stroke-width="2"/>
      <text x="520" y="117" text-anchor="middle" fill="var(--diagram-text)" font-size="11" font-family="monospace">L=c+a=8.0</text>
      <text x="520" y="133" text-anchor="middle" fill="var(--accent-secondary)" font-size="10">grad=1.0 (시작)</text>
      <line x1="100" y1="100" x2="228" y2="135" stroke="var(--diagram-arrow)" stroke-width="1.5" marker-end="url(#microgpt-intro-arrow)"/>
      <line x1="100" y1="180" x2="228" y2="148" stroke="var(--diagram-arrow)" stroke-width="1.5" marker-end="url(#microgpt-intro-arrow)"/>
      <line x1="330" y1="140" x2="458" y2="123" stroke="var(--diagram-arrow)" stroke-width="1.5" marker-end="url(#microgpt-intro-arrow)"/>
      <line x1="100" y1="90" x2="458" y2="114" stroke="var(--diagram-arrow)" stroke-width="1.5" stroke-dasharray="4,3" marker-end="url(#microgpt-intro-arrow)"/>
      <text x="160" y="112" text-anchor="middle" fill="var(--text-muted)" font-size="10">∂c/∂a=3.0</text>
      <text x="160" y="173" text-anchor="middle" fill="var(--text-muted)" font-size="10">∂c/∂b=2.0</text>
      <text x="400" y="122" text-anchor="middle" fill="var(--text-muted)" font-size="10">∂L/∂c=1</text>
      <text x="320" y="97" text-anchor="middle" fill="var(--text-muted)" font-size="10">∂L/∂a=1</text>
    </svg>
    <p class="diagram-caption">L = (a×b) + a 의 계산 그래프와 역전파. a는 두 경로에서 기울기를 받아 grad=4.0(누적)</p>
  </div>

  <h3 id="topo-sort">재귀 위상 정렬의 동작 원리</h3>
  <pre><code><span class="kw">def</span> <span class="fn">build_topo</span>(v):
    <span class="kw">if</span> v <span class="kw">not in</span> visited:
        visited.<span class="fn">add</span>(v)
        <span class="kw">for</span> child <span class="kw">in</span> v._children:
            <span class="fn">build_topo</span>(child)  <span class="cmt"># 자식들을 먼저 처리</span>
        topo.<span class="fn">append</span>(v)         <span class="cmt"># 자식 처리 후 자신을 추가 (후위 순회)</span>

<span class="cmt"># L의 경우:</span>
<span class="cmt"># build_topo(L)</span>
<span class="cmt">#   build_topo(c)   ← L의 child</span>
<span class="cmt">#     build_topo(a) ← c의 child → visited, topo=[a]</span>
<span class="cmt">#     build_topo(b) ← c의 child → visited, topo=[a,b]</span>
<span class="cmt">#   topo=[a,b,c]</span>
<span class="cmt">#   build_topo(a)   ← L의 child → already visited, skip</span>
<span class="cmt"># topo=[a, b, c, L]</span>
<span class="cmt"># reversed → [L, c, b, a] ← 역전파 순서</span></code></pre>

  <div class="info-box info">
    <strong>+= 누적의 핵심 이유:</strong>
    변수 <code>a</code>가 두 연산(<code>a*b</code>와 <code>c+a</code>)에서 사용될 때,
    각 경로에서 오는 기울기를 <strong>더해야(+=)</strong> 합니다.
    이것이 미적분의 "다변수 연쇄 법칙(multivariable chain rule)"입니다.
    PyTorch에서도 동일하게 <code>.grad</code>는 누적됩니다
    (이 때문에 매 스텝마다 <code>optimizer.zero_grad()</code>를 호출해야 합니다).
  </div>
</section>

<section class="content-section">
  <h2 id="next">다음 파트</h2>
  <ul>
    <li><a href="microgpt-architecture.html">2부: 모델 설계도, 임베딩, Attention, 트랜스포머 블록, 전체 GPT 파이프라인</a></li>
    <li><a href="microgpt-training.html">3부: 손실 계산, Adam 옵티마이저, 추론, LLM 비교, 요약</a></li>
  </ul>
</section>

<section class="content-section">
  <h2 id="references">참고 자료</h2>

  <h3>원본 소스</h3>
  <ul>
    <li><a href="https://karpathy.github.io/2026/02/12/microgpt/" target="_blank" rel="noopener">Andrej Karpathy — microgpt 블로그 포스트 (2026.02.12)</a> — 저자 직접 작성 공식 가이드</li>
    <li><a href="https://gist.github.com/karpathy/8627fe009c40f57531cb18360106ce95" target="_blank" rel="noopener">microgpt.py 원본 소스 코드 (GitHub Gist)</a> — ~200줄 순수 파이썬 전체 알고리즘</li>
    <li><a href="https://github.com/karpathy" target="_blank" rel="noopener">@karpathy GitHub</a> — 저자의 모든 오픈소스 교육용 AI 프로젝트</li>
    <li><a href="https://x.com/karpathy" target="_blank" rel="noopener">@karpathy X(Twitter)</a> — 최신 연구·강의 업데이트</li>
  </ul>

  <h3>관련 Karpathy 프로젝트</h3>
  <ul>
    <li><a href="https://github.com/karpathy/micrograd" target="_blank" rel="noopener">micrograd</a> — 스칼라 autograd 엔진 (~150줄). microgpt의 <code>Value</code> 클래스 직접 원형</li>
    <li><a href="https://github.com/karpathy/makemore" target="_blank" rel="noopener">makemore</a> — names.txt 데이터셋 출처. Bigram → MLP → Transformer 단계별 언어 모델 강의</li>
    <li><a href="https://github.com/karpathy/nanoGPT" target="_blank" rel="noopener">nanoGPT</a> — microgpt 원리의 실용적 확장. PyTorch·GPU·DDP·GPT-2(124M) 재현</li>
    <li><a href="https://github.com/karpathy/llm.c" target="_blank" rel="noopener">llm.c</a> — C/CUDA로만 LLM 훈련. 의존성 제거 철학의 시스템 레벨 극한</li>
  </ul>

  <h3>학습 영상 (Karpathy YouTube)</h3>
  <ul>
    <li><a href="https://www.youtube.com/watch?v=VMj-3S1tku0" target="_blank" rel="noopener">The spelled-out intro to neural networks and backpropagation: building micrograd</a> — micrograd + 역전파 강의 (2시간25분)</li>
    <li><a href="https://www.youtube.com/watch?v=kCc8FmEb1nY" target="_blank" rel="noopener">Let's build GPT: from scratch, in code, spelled out</a> — nanoGPT 라이브 코딩 (2시간)</li>
    <li><a href="https://www.youtube.com/watch?v=PaCmpygFfXo" target="_blank" rel="noopener">The spelled-out intro to language modeling: building makemore</a> — names.txt + 문자 수준 LM</li>
    <li><a href="https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ" target="_blank" rel="noopener">Neural Networks: Zero to Hero (재생목록)</a> — 위 강의 전체 시리즈</li>
  </ul>

  <h3>핵심 논문</h3>
  <ul>
    <li><a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">Attention Is All You Need (Vaswani et al., 2017)</a> — Transformer 원논문. Attention·Multi-head·Residual·Norm의 출발점</li>
    <li><a href="https://arxiv.org/abs/1412.6980" target="_blank" rel="noopener">Adam: A Method for Stochastic Optimization (Kingma &amp; Ba, 2014)</a> — microgpt가 구현한 Adam 옵티마이저 원논문</li>
    <li><a href="https://arxiv.org/abs/1910.07467" target="_blank" rel="noopener">Root Mean Square Layer Normalization (Zhang &amp; Sennrich, 2019)</a> — microgpt의 <code>rmsnorm</code> 수식 근거</li>
    <li><a href="https://openai.com/research/language-unsupervised" target="_blank" rel="noopener">GPT-2: Language Models are Unsupervised Multitask Learners (OpenAI, 2019)</a> — microgpt가 "follow GPT-2"라고 명시한 기준 아키텍처</li>
  </ul>

  <h3>관련 vibecoding 문서</h3>
  <ul>
    <li><a href="llm-theory-foundations.html">LLM 이론 기초</a></li>
    <li><a href="llm-theory-advanced.html">LLM 이론 심화 (Transformer, Attention 상세)</a></li>
    <li><a href="llm-handbook-training.html">LLM 핸드북: 학습·정렬·추론</a></li>
    <li><a href="llm-theory-math-appendix.html">LLM 이론 수학 부록 (어텐션 수식 유도)</a></li>
  </ul>
</section>

<div class="page-nav"></div>
</main>

<aside class="inline-toc">
  <div class="toc-title">목차</div>
  <div class="toc-nav"></div>
</aside>
<footer class="site-footer"></footer>
</div>
<script src="../js/main.js"></script>
</body>
</html>
