<!DOCTYPE html>
<html lang="ko" data-theme="dark-kernel">
<head>
<script>
(function(){var m=document.cookie.match(/claude_theme=([^;]+)/);if(m)document.documentElement.setAttribute('data-theme',m[1]);})();
</script>
<meta charset="UTF-8">
<meta property="og:type" content="article">
<meta property="og:site_name" content="AI Vibe Coding ê°€ì´ë“œ /with MINZKN">
<meta property="og:title" content="Ollama ë„êµ¬ ì—°ë™ - Continue, Aider, LangChain, Open WebUI">
<meta property="og:description" content="Ollamaë¥¼ VS Code, LangChain, LlamaIndex, APIì™€ í†µí•©í•˜ì—¬ ê°•ë ¥í•œ AI ê°œë°œ í™˜ê²½ êµ¬ì¶•">
<meta property="og:url" content="https://minzkn.com/claude/pages/ollama-integration.html">
<meta property="og:image" content="https://minzkn.com/claude/images/og-image.png">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="Ollama í†µí•© ê°€ì´ë“œ. Continue.dev, Aider, LangChain, LlamaIndex, Open WebUI ì—°ë™ ë°©ë²•.">
<meta name="keywords" content="ollama ì—°ë™, continue dev, aider, langchain, llamaindex, open webui, api integration">
<meta name="author" content="MINZKN">
<title>Ollama ë„êµ¬ ì—°ë™ - AI Vibe Coding ê°€ì´ë“œ /with MINZKN</title>
<link rel="icon" type="image/svg+xml" href="../images/favicon.svg">
<link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/joungkyun/font-d2coding/d2coding.css">
<link rel="stylesheet" href="../css/themes.css">
<link rel="stylesheet" href="../css/style.css">
<link rel="stylesheet" href="../css/responsive.css">
</head>
<body>
<div class="page-wrapper">

<header class="site-header"></header>
<nav class="side-nav" aria-label="ì‚¬ì´íŠ¸ ë‚´ë¹„ê²Œì´ì…˜"></nav>

<main class="main-content">
<nav class="breadcrumb"></nav>

<h1 id="top">Ollama ë„êµ¬ ì—°ë™</h1>
<p class="lead">ë¡œì»¬ LLMì„ ê°œë°œ ë„êµ¬ì™€ í†µí•©í•˜ì—¬ ì™„ì „í•œ AI ì½”ë”© í™˜ê²½ êµ¬ì¶•í•˜ê¸°</p>

<div class="info-box tip">
  <div class="info-box-title">ğŸš€ ë¹ ë¥¸ ì‹œì‘</div>
  <p><strong>ê°€ì¥ ì¸ê¸° ìˆëŠ” í†µí•© 3ê°€ì§€:</strong></p>
  <ol>
    <li><strong>Continue.dev</strong> - VS Codeì—ì„œ GitHub Copilotì²˜ëŸ¼ ì‚¬ìš© (ë¬´ë£Œ)</li>
    <li><strong>Aider</strong> - CLI ê¸°ë°˜ AI í˜ì–´ í”„ë¡œê·¸ë˜ë°</li>
    <li><strong>Open WebUI</strong> - ChatGPT ìŠ¤íƒ€ì¼ ì›¹ ì¸í„°í˜ì´ìŠ¤</li>
  </ol>
  <p>ì´ 3ê°€ì§€ë§Œ ì„¤ì •í•´ë„ 90% ì‘ì—…ì„ ë¡œì»¬ì—ì„œ ë¬´ë£Œë¡œ ì²˜ë¦¬!</p>
</div>

<section class="content-section">
  <h2 id="continue-dev">Continue.dev + Ollama (VS Code)</h2>

  <p>
    <strong>Continue</strong>ëŠ” VS Code/JetBrainsì—ì„œ GitHub Copilotì²˜ëŸ¼ ë™ì‘í•˜ëŠ” ì˜¤í”ˆì†ŒìŠ¤ AI ì½”ë”© ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
    Ollamaì™€ ì™„ë²½í•˜ê²Œ í†µí•©ë˜ì–´ ë¬´ë£Œë¡œ ë¬´ì œí•œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
  </p>

  <h3 id="continue-install">ì„¤ì¹˜</h3>

  <pre><code><span class="cmt"># 1. VS Code í™•ì¥ ë§ˆì¼“í”Œë ˆì´ìŠ¤ì—ì„œ ì„¤ì¹˜</span>
<span class="str">1. VS Code ì—´ê¸°</span>
<span class="str">2. Extensions (Ctrl+Shift+X)</span>
<span class="str">3. "Continue" ê²€ìƒ‰</span>
<span class="str">4. Install í´ë¦­</span>

<span class="cmt"># ë˜ëŠ” CLIë¡œ ì„¤ì¹˜</span>
code --install-extension continue.continue</code></pre>

  <h3 id="continue-config">ì„¤ì •</h3>

  <p>Continue ì‚¬ì´ë“œë°”ì—ì„œ ì„¤ì • íŒŒì¼ì„ ì—´ê±°ë‚˜ <code>~/.continue/config.json</code> ì§ì ‘ í¸ì§‘:</p>

  <pre><code><span class="cmt">// ~/.continue/config.json</span>
{
  <span class="str">"models"</span>: [
    {
      <span class="str">"title"</span>: <span class="str">"CodeLlama 13B"</span>,
      <span class="str">"provider"</span>: <span class="str">"ollama"</span>,
      <span class="str">"model"</span>: <span class="str">"codellama:13b"</span>,
      <span class="str">"apiBase"</span>: <span class="str">"http://localhost:11434"</span>
    },
    {
      <span class="str">"title"</span>: <span class="str">"Llama 3.2"</span>,
      <span class="str">"provider"</span>: <span class="str">"ollama"</span>,
      <span class="str">"model"</span>: <span class="str">"llama3.2"</span>
    }
  ],
  <span class="str">"tabAutocompleteModel"</span>: {
    <span class="str">"title"</span>: <span class="str">"Starcoder2 3B"</span>,
    <span class="str">"provider"</span>: <span class="str">"ollama"</span>,
    <span class="str">"model"</span>: <span class="str">"starcoder2:3b"</span>
  },
  <span class="str">"embeddingsProvider"</span>: {
    <span class="str">"provider"</span>: <span class="str">"ollama"</span>,
    <span class="str">"model"</span>: <span class="str">"nomic-embed-text"</span>
  }
}</code></pre>

  <h3 id="continue-usage">ì‚¬ìš©ë²•</h3>

  <h4>1. ì¸ë¼ì¸ ì½”ë“œ ìƒì„± (Ctrl+I)</h4>

  <pre><code><span class="cmt"># ì½”ë“œ í¸ì§‘ê¸°ì—ì„œ Ctrl+I ëˆ„ë¥´ê³  ìš”ì²­</span>
<span class="str">"í”¼ë³´ë‚˜ì¹˜ ìˆ˜ì—´ ìƒì„± í•¨ìˆ˜ë¥¼ ì¬ê·€ë¡œ ì‘ì„±í•´ì¤˜"</span>

<span class="cmt"># Continueê°€ ìë™ìœ¼ë¡œ ì½”ë“œ ìƒì„± ë° ì‚½ì…</span>
<span class="kw">def</span> <span class="fn">fibonacci</span>(n):
    <span class="kw">if</span> n &lt;= <span class="num">1</span>:
        <span class="kw">return</span> n
    <span class="kw">return</span> fibonacci(n-<span class="num">1</span>) + fibonacci(n-<span class="num">2</span>)</code></pre>

  <h4>2. ì½”ë“œ ì„¤ëª… (Ctrl+L)</h4>

  <pre><code><span class="cmt"># ì½”ë“œ ë¸”ë¡ ì„ íƒ í›„ Ctrl+L</span>
<span class="str">"ì´ ì½”ë“œê°€ ë¬´ì—‡ì„ í•˜ëŠ”ì§€ ì„¤ëª…í•´ì¤˜"</span>

<span class="cmt"># ì‚¬ì´ë“œë°”ì— ì„¤ëª… í‘œì‹œ</span></code></pre>

  <h4>3. ë¦¬íŒ©í† ë§</h4>

  <pre><code><span class="cmt"># ì½”ë“œ ì„ íƒ í›„ Ctrl+I</span>
<span class="str">"ì´ í•¨ìˆ˜ë¥¼ íƒ€ì… íŒíŠ¸ë¥¼ ì¶”ê°€í•˜ê³  docstring ì‘ì„±í•´ì¤˜"</span>

<span class="kw">def</span> <span class="fn">fibonacci</span>(n: <span class="type">int</span>) -&gt; <span class="type">int</span>:
    <span class="str">"""
    Calculate the nth Fibonacci number recursively.

    Args:
        n: The position in the Fibonacci sequence (0-indexed)

    Returns:
        The nth Fibonacci number
    """</span>
    <span class="kw">if</span> n &lt;= <span class="num">1</span>:
        <span class="kw">return</span> n
    <span class="kw">return</span> fibonacci(n-<span class="num">1</span>) + fibonacci(n-<span class="num">2</span>)</code></pre>

  <h4>4. íƒ­ ìë™ì™„ì„± (ì‹¤í—˜ì )</h4>

  <pre><code><span class="cmt"># ì½”ë“œ ì‘ì„± ì¤‘ íƒ­ ëˆŒëŸ¬ ìë™ì™„ì„±</span>
<span class="kw">def</span> <span class="fn">sort_list</span>(items):
    <span class="cmt"># Tab ëˆ„ë¥´ë©´ ìë™ìœ¼ë¡œ ì œì•ˆ</span>
    <span class="kw">return</span> sorted(items)  <span class="cmt"># â† Continueê°€ ìƒì„±</span></code></pre>

  <h3 id="continue-tips">íŒ & íŠ¸ë¦­</h3>

  <div class="info-box tip">
    <div class="info-box-title">ğŸ’¡ Continue ìµœì í™”</div>
    <ul>
      <li><strong>ì½”ë“œ ìƒì„±</strong>: CodeLlama 13B ë˜ëŠ” Qwen2.5-Coder 14B</li>
      <li><strong>ëŒ€í™”/ì„¤ëª…</strong>: Llama 3.2 7B ë˜ëŠ” Mistral 7B</li>
      <li><strong>ìë™ì™„ì„±</strong>: Starcoder2 3B (ë¹ ë¦„, ê°€ë²¼ì›€)</li>
      <li><strong>ì„ë² ë”©</strong>: nomic-embed-text (ì½”ë“œë² ì´ìŠ¤ ê²€ìƒ‰ìš©)</li>
    </ul>
  </div>

  <pre><code><span class="cmt"># ëª¨ë¸ ë¯¸ë¦¬ ë‹¤ìš´ë¡œë“œ</span>
ollama pull codellama:13b
ollama pull llama3.2
ollama pull starcoder2:3b
ollama pull nomic-embed-text</code></pre>

  <h3 id="continue-keyboard">í‚¤ë³´ë“œ ë‹¨ì¶•í‚¤</h3>

  <table>
    <thead>
      <tr>
        <th>ë‹¨ì¶•í‚¤</th>
        <th>ê¸°ëŠ¥</th>
        <th>ì„¤ëª…</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><code>Ctrl+I</code></td>
        <td>ì¸ë¼ì¸ í¸ì§‘</td>
        <td>ì„ íƒ ì˜ì—­ì— ì½”ë“œ ìƒì„±/ìˆ˜ì •</td>
      </tr>
      <tr>
        <td><code>Ctrl+L</code></td>
        <td>ì±„íŒ… ì—´ê¸°</td>
        <td>ì‚¬ì´ë“œë°”ì—ì„œ ëŒ€í™” ì‹œì‘</td>
      </tr>
      <tr>
        <td><code>Tab</code></td>
        <td>ìë™ì™„ì„±</td>
        <td>ì œì•ˆ ìˆ˜ë½ (ì„¤ì • í•„ìš”)</td>
      </tr>
      <tr>
        <td><code>Ctrl+Shift+R</code></td>
        <td>ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€</td>
        <td>í˜„ì¬ íŒŒì¼ì„ ì»¨í…ìŠ¤íŠ¸ì— ì¶”ê°€</td>
      </tr>
    </tbody>
  </table>
</section>

<section class="content-section">
  <h2 id="aider">Aider + Ollama (CLI)</h2>

  <p>
    <strong>Aider</strong>ëŠ” í„°ë¯¸ë„ì—ì„œ ë™ì‘í•˜ëŠ” AI í˜ì–´ í”„ë¡œê·¸ë˜ë° ë„êµ¬ë¡œ, Gitê³¼ ì™„ë²½í•˜ê²Œ í†µí•©ë©ë‹ˆë‹¤.
    ì½”ë“œ ë³€ê²½ì‚¬í•­ì„ ìë™ìœ¼ë¡œ ì»¤ë°‹í•˜ê³ , ì—¬ëŸ¬ íŒŒì¼ì„ ë™ì‹œì— ìˆ˜ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
  </p>

  <h3 id="aider-install">ì„¤ì¹˜</h3>

  <pre><code><span class="cmt"># Python 3.8+ í•„ìš”</span>
pip install aider-chat

<span class="cmt"># ë˜ëŠ” pipxë¡œ ê²©ë¦¬ ì„¤ì¹˜</span>
pipx install aider-chat

<span class="cmt"># ì„¤ì¹˜ í™•ì¸</span>
aider --version
<span class="cmt"># aider version 0.x.x</span></code></pre>

  <h3 id="aider-config">Ollama ì—°ë™ ì„¤ì •</h3>

  <pre><code><span class="cmt"># ê¸°ë³¸ ì‚¬ìš© (ëª¨ë¸ ì§€ì •)</span>
aider --model ollama/codellama:13b

<span class="cmt"># í™˜ê²½ ë³€ìˆ˜ë¡œ ê¸°ë³¸ ëª¨ë¸ ì„¤ì •</span>
<span class="kw">export</span> AIDER_MODEL=ollama/llama3.2
<span class="kw">export</span> OLLAMA_API_BASE=http://localhost:11434

<span class="cmt"># ~/.aider.conf.yml ìƒì„± (ì˜êµ¬ ì„¤ì •)</span>
cat &gt; ~/.aider.conf.yml &lt;&lt;EOF
model: ollama/codellama:13b
edit-format: whole
auto-commits: true
dirty-commits: false
EOF</code></pre>

  <h3 id="aider-usage">ì‚¬ìš©ë²•</h3>

  <h4>1. í”„ë¡œì íŠ¸ì—ì„œ Aider ì‹œì‘</h4>

  <pre><code><span class="cmt"># Git ì €ì¥ì†Œ ë£¨íŠ¸ì—ì„œ ì‹¤í–‰</span>
cd ~/my-project
aider --model ollama/codellama:13b

<span class="cmt"># íŠ¹ì • íŒŒì¼ ì§€ì •</span>
aider src/main.py tests/test_main.py --model ollama/llama3.2

<span class="cmt"># ëª¨ë“  Python íŒŒì¼</span>
aider src/*.py</code></pre>

  <h4>2. ì½”ë“œ ìƒì„±</h4>

  <pre><code><span class="cmt"># Aider í”„ë¡¬í”„íŠ¸</span>
<span class="str">&gt; main.pyì— FastAPI ì•±ì„ ìƒì„±í•˜ê³ , /health ì—”ë“œí¬ì¸íŠ¸ë¥¼ ì¶”ê°€í•´ì¤˜</span>

<span class="cmt"># Aiderê°€ ìë™ìœ¼ë¡œ ì½”ë“œ ìƒì„± ë° íŒŒì¼ ì‘ì„±</span>
<span class="cmt"># main.py</span>
<span class="kw">from</span> fastapi <span class="kw">import</span> FastAPI

app = FastAPI()

<span class="pp">@app.get</span>(<span class="str">"/health"</span>)
<span class="kw">async</span> <span class="kw">def</span> <span class="fn">health_check</span>():
    <span class="kw">return</span> {<span class="str">"status"</span>: <span class="str">"healthy"</span>}

<span class="cmt"># Aiderê°€ ìë™ìœ¼ë¡œ Git ì»¤ë°‹</span>
Applied edit to main.py
Commit 3a8f7e1 Add FastAPI health endpoint</code></pre>

  <h4>3. ë²„ê·¸ ìˆ˜ì •</h4>

  <pre><code><span class="str">&gt; user.pyì˜ validate_email í•¨ìˆ˜ì— ë²„ê·¸ê°€ ìˆì–´. ì˜ëª»ëœ ì´ë©”ì¼ì„ í—ˆìš©í•˜ê³  ìˆì–´.</span>

<span class="cmt"># Aiderê°€ ì½”ë“œ ë¶„ì„ í›„ ìˆ˜ì •</span>
I found the issue. The regex pattern is missing the @ symbol check.

<span class="cmt"># ìˆ˜ì •ëœ ì½”ë“œ</span>
<span class="kw">import</span> re

<span class="kw">def</span> <span class="fn">validate_email</span>(email: <span class="type">str</span>) -&gt; <span class="type">bool</span>:
    pattern = <span class="str">r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'</span>
    <span class="kw">return</span> bool(re.match(pattern, email))

Applied edit to user.py
Commit b4c2d9f Fix email validation regex</code></pre>

  <h4>4. ë¦¬íŒ©í† ë§</h4>

  <pre><code><span class="str">&gt; database.pyì˜ ëª¨ë“  í•¨ìˆ˜ì— íƒ€ì… íŒíŠ¸ì™€ docstringì„ ì¶”ê°€í•˜ê³ , ì—ëŸ¬ í•¸ë“¤ë§ì„ ê°œì„ í•´ì¤˜</span>

<span class="cmt"># ì—¬ëŸ¬ í•¨ìˆ˜ ë™ì‹œ ë¦¬íŒ©í† ë§</span>
Applied edit to database.py
- Added type hints to 5 functions
- Added docstrings following Google style
- Added proper exception handling
Commit 7e1a5c3 Refactor database.py with type hints and docs</code></pre>

  <h3 id="aider-commands">Aider ëª…ë ¹ì–´</h3>

  <table>
    <thead>
      <tr>
        <th>ëª…ë ¹ì–´</th>
        <th>ì„¤ëª…</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><code>/add &lt;file&gt;</code></td>
        <td>íŒŒì¼ì„ ì±„íŒ…ì— ì¶”ê°€</td>
      </tr>
      <tr>
        <td><code>/drop &lt;file&gt;</code></td>
        <td>íŒŒì¼ì„ ì±„íŒ…ì—ì„œ ì œê±°</td>
      </tr>
      <tr>
        <td><code>/ls</code></td>
        <td>í˜„ì¬ ì±„íŒ…ì— í¬í•¨ëœ íŒŒì¼ ëª©ë¡</td>
      </tr>
      <tr>
        <td><code>/run &lt;cmd&gt;</code></td>
        <td>ì‰˜ ëª…ë ¹ì–´ ì‹¤í–‰</td>
      </tr>
      <tr>
        <td><code>/undo</code></td>
        <td>ë§ˆì§€ë§‰ ë³€ê²½ ì·¨ì†Œ</td>
      </tr>
      <tr>
        <td><code>/commit</code></td>
        <td>ìˆ˜ë™ ì»¤ë°‹</td>
      </tr>
      <tr>
        <td><code>/diff</code></td>
        <td>ë³€ê²½ì‚¬í•­ í™•ì¸</td>
      </tr>
      <tr>
        <td><code>/help</code></td>
        <td>ë„ì›€ë§</td>
      </tr>
      <tr>
        <td><code>/exit</code></td>
        <td>ì¢…ë£Œ</td>
      </tr>
    </tbody>
  </table>

  <h3 id="aider-advanced">ê³ ê¸‰ ì‚¬ìš©</h3>

  <h4>ì „ì²´ ì½”ë“œë² ì´ìŠ¤ ê²€ìƒ‰</h4>

  <pre><code><span class="cmt"># ì €ì¥ì†Œ ë§µ í™œì„±í™” (--map-tokens ì˜µì…˜)</span>
aider --model ollama/llama3.2 --map-tokens 2048

<span class="str">&gt; ì¸ì¦ ê´€ë ¨ ëª¨ë“  í•¨ìˆ˜ë¥¼ ì°¾ì•„ì„œ JWT í† í° ê²€ì¦ ë¡œì§ì„ ì¶”ê°€í•´ì¤˜</span>

<span class="cmt"># Aiderê°€ ì „ì²´ ì½”ë“œë² ì´ìŠ¤ ê²€ìƒ‰ í›„ ê´€ë ¨ íŒŒì¼ ìë™ ìˆ˜ì •</span></code></pre>

  <h4>í…ŒìŠ¤íŠ¸ ìë™ ìƒì„±</h4>

  <pre><code>aider src/calculator.py

<span class="str">&gt; calculator.pyì˜ ëª¨ë“  í•¨ìˆ˜ì— ëŒ€í•œ pytest í…ŒìŠ¤íŠ¸ë¥¼ tests/test_calculator.pyì— ìƒì„±í•´ì¤˜</span>

Applied edit to tests/test_calculator.py
Created 8 test functions
Commit a3d8e4f Add comprehensive tests for calculator</code></pre>

  <div class="info-box tip">
    <div class="info-box-title">ğŸ’¡ Aider ì‚¬ìš© íŒ</div>
    <ul>
      <li><strong>--watch-files</strong>: íŒŒì¼ ë³€ê²½ ê°ì§€ ìë™ ì ìš©</li>
      <li><strong>--auto-commits false</strong>: ìˆ˜ë™ ì»¤ë°‹ ì œì–´</li>
      <li><strong>--edit-format whole</strong>: ì „ì²´ íŒŒì¼ í¸ì§‘ ëª¨ë“œ (ì‘ì€ íŒŒì¼)</li>
      <li><strong>--edit-format diff</strong>: diff íŒ¨ì¹˜ ëª¨ë“œ (í° íŒŒì¼, ê¸°ë³¸ê°’)</li>
      <li><strong>--message "..."</strong>: ëŒ€í™” ì—†ì´ ë‹¨ì¼ ëª…ë ¹ ì‹¤í–‰</li>
    </ul>
  </div>
</section>

<section class="content-section">
  <h2 id="open-webui">Open WebUI (ChatGPT ìŠ¤íƒ€ì¼ ì¸í„°í˜ì´ìŠ¤)</h2>

  <p>
    <strong>Open WebUI</strong>ëŠ” Ollamaë¥¼ ìœ„í•œ ì™„ì „í•œ ê¸°ëŠ¥ì„ ê°–ì¶˜ ChatGPT ìŠ¤íƒ€ì¼ ì›¹ ì¸í„°í˜ì´ìŠ¤ì…ë‹ˆë‹¤.
    ë¸Œë¼ìš°ì €ì—ì„œ ëŒ€í™”, ë¬¸ì„œ ì—…ë¡œë“œ, RAG, íŒ€ í˜‘ì—…, ë©€í‹°ëª¨ë‹¬, ìŒì„± ì¸ì‹ ë“± ëª¨ë“  ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
    ê°œì¸ ì‚¬ìš©ë¶€í„° ê¸°ì—… ë°°í¬ê¹Œì§€ ë‹¤ì–‘í•œ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ì§€ì›í•©ë‹ˆë‹¤.
  </p>

  <div class="info-box tip">
    <div class="info-box-title">ğŸŒŸ Open WebUIë¥¼ ì„ íƒí•´ì•¼ í•˜ëŠ” ì´ìœ </div>
    <ul>
      <li><strong>ì™„ì „í•œ í”„ë¼ì´ë²„ì‹œ</strong>: ëª¨ë“  ë°ì´í„°ê°€ ë¡œì»¬ì— ì €ì¥, ì™¸ë¶€ ì „ì†¡ ì—†ìŒ</li>
      <li><strong>ë¬´ë£Œ & ì˜¤í”ˆì†ŒìŠ¤</strong>: MIT ë¼ì´ì„¼ìŠ¤, í‰ìƒ ë¬´ë£Œ</li>
      <li><strong>ChatGPT ìˆ˜ì¤€ UI</strong>: ëŒ€í™” íˆìŠ¤í† ë¦¬, ë§ˆí¬ë‹¤ìš´, ì½”ë“œ í•˜ì´ë¼ì´íŒ…</li>
      <li><strong>íŒ€ í˜‘ì—…</strong>: ì‚¬ìš©ì ê´€ë¦¬, ê¶Œí•œ, ëŒ€í™” ê³µìœ </li>
      <li><strong>í™•ì¥ì„±</strong>: í”ŒëŸ¬ê·¸ì¸, API, ì»¤ìŠ¤í…€ ëª¨ë¸</li>
    </ul>
  </div>

  <h3 id="webui-install">ì„¤ì¹˜ ë°©ë²•</h3>

  <h4>ë°©ë²• 1: Docker (ê¶Œì¥ - ê°€ì¥ ì‰¬ì›€)</h4>

  <pre><code><span class="cmt"># CPU ì „ìš© (MacBook, ì¼ë°˜ PC)</span>
docker run -d -p 3000:8080 \
  -v open-webui:/app/backend/data \
  -e OLLAMA_BASE_URL=http://host.docker.internal:11434 \
  --name open-webui \
  --restart always \
  ghcr.io/open-webui/open-webui:main

<span class="cmt"># GPU ì§€ì› (NVIDIA RTX, A100 ë“±)</span>
docker run -d -p 3000:8080 \
  --gpus all \
  -v open-webui:/app/backend/data \
  -v ollama:/root/.ollama \
  -e OLLAMA_BASE_URL=http://host.docker.internal:11434 \
  --name open-webui \
  --restart always \
  ghcr.io/open-webui/open-webui:cuda

<span class="cmt"># Apple Silicon (M1/M2/M3)</span>
docker run -d -p 3000:8080 \
  --platform linux/arm64 \
  -v open-webui:/app/backend/data \
  -e OLLAMA_BASE_URL=http://host.docker.internal:11434 \
  --name open-webui \
  --restart always \
  ghcr.io/open-webui/open-webui:main

<span class="cmt"># ë¸Œë¼ìš°ì €ì—ì„œ ì ‘ì†</span>
<span class="str">http://localhost:3000</span></code></pre>

  <h4>ë°©ë²• 2: Docker Compose (í”„ë¡œë•ì…˜ ì¶”ì²œ)</h4>

  <pre><code><span class="cmt"># docker-compose.yml</span>
<span class="kw">version</span>: <span class="str">'3.8'</span>

<span class="kw">services</span>:
  <span class="kw">ollama</span>:
    <span class="kw">image</span>: ollama/ollama:latest
    <span class="kw">container_name</span>: ollama
    <span class="kw">ports</span>:
      - <span class="str">"11434:11434"</span>
    <span class="kw">volumes</span>:
      - ollama:/root/.ollama
    <span class="kw">restart</span>: always
    <span class="kw">deploy</span>:
      <span class="kw">resources</span>:
        <span class="kw">reservations</span>:
          <span class="kw">devices</span>:
            - <span class="kw">driver</span>: nvidia
              <span class="kw">count</span>: 1
              <span class="kw">capabilities</span>: [gpu]

  <span class="kw">open-webui</span>:
    <span class="kw">image</span>: ghcr.io/open-webui/open-webui:main
    <span class="kw">container_name</span>: open-webui
    <span class="kw">ports</span>:
      - <span class="str">"3000:8080"</span>
    <span class="kw">volumes</span>:
      - open-webui:/app/backend/data
    <span class="kw">environment</span>:
      - OLLAMA_BASE_URL=http://ollama:11434
      - WEBUI_SECRET_KEY=<span class="str">"your-secret-key-change-this"</span>
      - WEBUI_AUTH=<span class="kw">true</span>
      - ENABLE_SIGNUP=<span class="kw">true</span>
      - DEFAULT_MODELS=llama3.2,codellama:13b
      - ENABLE_RAG_WEB_SEARCH=<span class="kw">true</span>
      - RAG_EMBEDDING_MODEL=nomic-embed-text
    <span class="kw">depends_on</span>:
      - ollama
    <span class="kw">restart</span>: always

<span class="kw">volumes</span>:
  <span class="kw">ollama</span>:
  <span class="kw">open-webui</span>:</code></pre>

  <pre><code><span class="cmt"># ì‹¤í–‰</span>
docker-compose up -d

<span class="cmt"># ë¡œê·¸ í™•ì¸</span>
docker-compose logs -f open-webui

<span class="cmt"># ì¤‘ì§€</span>
docker-compose down

<span class="cmt"># ì¬ì‹œì‘</span>
docker-compose restart</code></pre>

  <h4>ë°©ë²• 3: Pythonìœ¼ë¡œ ì„¤ì¹˜ (ê°œë°œììš©)</h4>

  <pre><code><span class="cmt"># Python 3.11+ í•„ìš”</span>
git clone https://github.com/open-webui/open-webui.git
cd open-webui

<span class="cmt"># ê°€ìƒ í™˜ê²½ ìƒì„±</span>
python -m venv venv
source venv/bin/activate  <span class="cmt"># Windows: venv\Scripts\activate</span>

<span class="cmt"># ì˜ì¡´ì„± ì„¤ì¹˜</span>
pip install -r requirements.txt

<span class="cmt"># í™˜ê²½ ë³€ìˆ˜ ì„¤ì •</span>
<span class="kw">export</span> OLLAMA_BASE_URL=http://localhost:11434
<span class="kw">export</span> WEBUI_SECRET_KEY=<span class="str">"your-secret-key"</span>

<span class="cmt"># ì‹¤í–‰</span>
bash start.sh

<span class="cmt"># ë˜ëŠ” ê°œë°œ ëª¨ë“œ</span>
npm install
npm run dev  <span class="cmt"># http://localhost:5173</span></code></pre>

  <h3 id="webui-first-setup">ì´ˆê¸° ì„¤ì •</h3>

  <h4>1. ì²« ë²ˆì§¸ ê³„ì • ìƒì„± (ê´€ë¦¬ì)</h4>

  <pre><code><span class="cmt"># ë¸Œë¼ìš°ì €ì—ì„œ http://localhost:3000 ì ‘ì†</span>
<span class="str">1. "Sign up" í´ë¦­</span>
<span class="str">2. ì´ë¦„, ì´ë©”ì¼, ë¹„ë°€ë²ˆí˜¸ ì…ë ¥</span>
<span class="str">3. ì²« ë²ˆì§¸ ì‚¬ìš©ìëŠ” ìë™ìœ¼ë¡œ ê´€ë¦¬ì ê¶Œí•œ ë¶€ì—¬</span>

<span class="cmt"># ì´í›„ ì‚¬ìš©ìëŠ” ì¼ë°˜ ê¶Œí•œìœ¼ë¡œ ìƒì„±ë¨</span></code></pre>

  <h4>2. ê¸°ë³¸ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ</h4>

  <pre><code><span class="cmt"># ì›¹ UIì—ì„œ ì§ì ‘ ë‹¤ìš´ë¡œë“œ</span>
<span class="str">Settings â†’ Models â†’ Pull a model from Ollama.com</span>

<span class="cmt"># ì¶”ì²œ ì‹œì‘ ëª¨ë¸</span>
llama3.2           <span class="cmt"># ë²”ìš© ëŒ€í™” (3B ë˜ëŠ” 7B)</span>
codellama:13b      <span class="cmt"># ì½”ë”© ì „ë¬¸</span>
mistral            <span class="cmt"># ë¹ ë¥´ê³  ì •í™•</span>
nomic-embed-text   <span class="cmt"># RAGìš© ì„ë² ë”©</span>

<span class="cmt"># ë˜ëŠ” í„°ë¯¸ë„ì—ì„œ ë¯¸ë¦¬ ë‹¤ìš´ë¡œë“œ</span>
ollama pull llama3.2
ollama pull codellama:13b
ollama pull nomic-embed-text</code></pre>

  <h4>3. ê¸°ë³¸ ì„¤ì • ìµœì í™”</h4>

  <pre><code><span class="cmt"># Settings â†’ General</span>
<span class="str">âœ“ Default Model: llama3.2</span>
<span class="str">âœ“ Request Mode: Chat (ëŒ€í™”í˜•)</span>
<span class="str">âœ“ System Prompt: (ë¹„ì›Œë‘ê±°ë‚˜ ì»¤ìŠ¤í…€)</span>

<span class="cmt"># Settings â†’ Interface</span>
<span class="str">âœ“ Theme: Dark (or Auto)</span>
<span class="str">âœ“ Language: í•œêµ­ì–´</span>
<span class="str">âœ“ Chat Direction: LTR</span>

<span class="cmt"># Settings â†’ Audio</span>
<span class="str">âœ“ STT Engine: Web API (ë˜ëŠ” Whisper)</span>
<span class="str">âœ“ TTS Engine: Web API</span></code></pre>

  <h3 id="webui-features">í•µì‹¬ ê¸°ëŠ¥ ìƒì„¸</h3>

  <h4>1. ëŒ€í™” ê´€ë¦¬</h4>

  <pre><code><span class="cmt"># ìƒˆ ëŒ€í™” ì‹œì‘</span>
<span class="str">1. ìƒë‹¨ "+ New Chat" ë²„íŠ¼ í´ë¦­</span>
<span class="str">2. ë˜ëŠ” Ctrl + Shift + O (ë‹¨ì¶•í‚¤)</span>

<span class="cmt"># ëŒ€í™” ê²€ìƒ‰</span>
<span class="str">ì™¼ìª½ ì‚¬ì´ë“œë°” ìƒë‹¨ ê²€ìƒ‰ì°½ì— í‚¤ì›Œë“œ ì…ë ¥</span>
<span class="str">ì˜ˆ: "python ì½”ë“œ" ì…ë ¥ â†’ ê´€ë ¨ ëŒ€í™” í•„í„°ë§</span>

<span class="cmt"># ëŒ€í™” í´ë” êµ¬ì¡°í™”</span>
<span class="str">1. ëŒ€í™” ìš°í´ë¦­ â†’ "Move to Folder"</span>
<span class="str">2. í´ë” ìƒì„±: "í”„ë¡œì íŠ¸", "í•™ìŠµ", "ë””ë²„ê¹…" ë“±</span>
<span class="str">3. ë“œë˜ê·¸ ì•¤ ë“œë¡­ìœ¼ë¡œ ì´ë™</span>

<span class="cmt"># ëŒ€í™” ë‚´ë³´ë‚´ê¸°</span>
<span class="str">1. ëŒ€í™” ì„¤ì • (â‹®) í´ë¦­</span>
<span class="str">2. Export â†’ Markdown or JSON ì„ íƒ</span>
<span class="str">3. íŒŒì¼ ì €ì¥ â†’ Notion, Obsidian ë“±ì— í™œìš©</span>

<span class="cmt"># ëŒ€í™” ê³µìœ  (íŒ€ í˜‘ì—…)</span>
<span class="str">1. ëŒ€í™” ì„¤ì • â†’ Share</span>
<span class="str">2. ë§í¬ ìƒì„± â†’ íŒ€ì›ì—ê²Œ ì „ë‹¬</span>
<span class="str">3. ê¶Œí•œ ì„¤ì •: View Only / Can Edit</span></code></pre>

  <h4>2. ë¬¸ì„œ ì—…ë¡œë“œ & RAG (ê²€ìƒ‰ ì¦ê°• ìƒì„±)</h4>

  <p><strong>RAG</strong>ëŠ” ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ê³  LLMì´ ê·¸ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ê²Œ í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.</p>

  <pre><code><span class="cmt"># ë‹¨ì¼ ë¬¸ì„œ ì—…ë¡œë“œ (ë¹ ë¥¸ ì§ˆë¬¸)</span>
<span class="str">1. ì±„íŒ… ì…ë ¥ì°½ ì™¼ìª½ ğŸ“ ì•„ì´ì½˜ í´ë¦­</span>
<span class="str">2. íŒŒì¼ ì„ íƒ (PDF, DOCX, TXT, MD, ì½”ë“œ ë“±)</span>
<span class="str">3. ìë™ìœ¼ë¡œ ì„ë² ë”© ìƒì„±</span>
<span class="str">4. ì§ˆë¬¸: "ì´ ë¬¸ì„œì˜ ì£¼ìš” ê²°ë¡ ì€?"</span>

<span class="cmt"># ì§€ì› íŒŒì¼ í˜•ì‹</span>
- ë¬¸ì„œ: PDF, DOCX, PPTX, TXT, MD
- ì½”ë“œ: .py, .js, .java, .cpp, .go, .rs
- ë°ì´í„°: CSV, JSON, XML
- ê¸°íƒ€: HTML, RTF

<span class="cmt"># ì˜êµ¬ ë¬¸ì„œ ì»¬ë ‰ì…˜ (ì§€ì‹ ë² ì´ìŠ¤)</span>
<span class="str">1. Settings â†’ Documents â†’ Upload</span>
<span class="str">2. ì—¬ëŸ¬ íŒŒì¼ ë™ì‹œ ì—…ë¡œë“œ (ë“œë˜ê·¸ ì•¤ ë“œë¡­)</span>
<span class="str">3. íƒœê·¸ ì¶”ê°€: #manual, #api, #guide</span>
<span class="str">4. ëª¨ë“  ì±„íŒ…ì—ì„œ ì´ ë¬¸ì„œ ê²€ìƒ‰ ê°€ëŠ¥</span>

<span class="cmt"># ì±„íŒ…ì—ì„œ ë¬¸ì„œ í™œì„±í™”</span>
<span class="str">1. ì±„íŒ… ì‹œì‘</span>
<span class="str">2. ë©”ì‹œì§€ ì…ë ¥ì°½ ìœ„ "# Add Document" í´ë¦­</span>
<span class="str">3. í™œì„±í™”í•  ë¬¸ì„œ ì„ íƒ (ì—¬ëŸ¬ ê°œ ê°€ëŠ¥)</span>
<span class="str">4. ì„ íƒëœ ë¬¸ì„œì—ì„œë§Œ ì •ë³´ ê²€ìƒ‰</span></code></pre>

  <h4>RAG ê³ ê¸‰ ì„¤ì •</h4>

  <pre><code><span class="cmt"># Settings â†’ Documents â†’ RAG</span>
<span class="kw">Embedding Model</span>: nomic-embed-text     <span class="cmt"># ì„ë² ë”© ëª¨ë¸</span>
<span class="kw">Chunk Size</span>: 1000                      <span class="cmt"># í…ìŠ¤íŠ¸ ë¶„í•  í¬ê¸°</span>
<span class="kw">Chunk Overlap</span>: 200                    <span class="cmt"># ì²­í¬ ì˜¤ë²„ë©</span>
<span class="kw">Top K</span>: 5                               <span class="cmt"># ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜</span>
<span class="kw">Relevance Threshold</span>: 0.5              <span class="cmt"># ê´€ë ¨ì„± ì„ê³„ê°’</span>

<span class="cmt"># ì„ë² ë”© ëª¨ë¸ ì¶”ì²œ</span>
<span class="str">ì˜ì–´ ë¬¸ì„œ</span>: nomic-embed-text (768ì°¨ì›, ë¹ ë¦„)
<span class="str">ë‹¤êµ­ì–´ (í•œêµ­ì–´ í¬í•¨)</span>: mxbai-embed-large
<span class="str">ì½”ë“œ</span>: nomic-embed-text</code></pre>

  <h4>ì‹¤ì „ RAG í™œìš© ì˜ˆì œ</h4>

  <pre><code><span class="cmt"># ì‹œë‚˜ë¦¬ì˜¤ 1: íšŒì‚¬ ë¬¸ì„œ Q&A ë´‡</span>
<span class="str">1. ëª¨ë“  ì‚¬ë‚´ ë§¤ë‰´ì–¼ PDF ì—…ë¡œë“œ (100ê°œ+)</span>
<span class="str">2. íƒœê·¸: #hr, #it, #finance</span>
<span class="str">3. ì§ˆë¬¸: "íœ´ê°€ ì‹ ì²­ ì ˆì°¨ëŠ”?"</span>
<span class="str">â†’ HR ë§¤ë‰´ì–¼ì—ì„œ ê´€ë ¨ ì •ë³´ ì¶”ì¶œ í›„ ë‹µë³€</span>

<span class="cmt"># ì‹œë‚˜ë¦¬ì˜¤ 2: ì½”ë“œë² ì´ìŠ¤ ë¶„ì„</span>
<span class="str">1. í”„ë¡œì íŠ¸ì˜ ëª¨ë“  .py íŒŒì¼ ì—…ë¡œë“œ</span>
<span class="str">2. ì§ˆë¬¸: "ì¸ì¦ ë¡œì§ì´ ì–´ë–¤ íŒŒì¼ì— ìˆì–´?"</span>
<span class="str">â†’ auth.py, middleware.py ë“±ì—ì„œ ê´€ë ¨ ì½”ë“œ ì°¾ì•„ ì„¤ëª…</span>

<span class="cmt"># ì‹œë‚˜ë¦¬ì˜¤ 3: ì—°êµ¬ ë…¼ë¬¸ ìš”ì•½</span>
<span class="str">1. ë…¼ë¬¸ PDF 10ê°œ ì—…ë¡œë“œ</span>
<span class="str">2. ì§ˆë¬¸: "Transformer ì•„í‚¤í…ì²˜ì˜ í•œê³„ì ì€?"</span>
<span class="str">â†’ 10ê°œ ë…¼ë¬¸ì—ì„œ ê´€ë ¨ ë‚´ìš© í†µí•© ìš”ì•½</span></code></pre>

  <h4>3. ë©€í‹°ëª¨ë‹¬ & ë¹„ì „</h4>

  <pre><code><span class="cmt"># ì´ë¯¸ì§€ ë¶„ì„ (llava, bakllava ëª¨ë¸ í•„ìš”)</span>
ollama pull llava:13b

<span class="cmt"># ì‚¬ìš©</span>
<span class="str">1. ì±„íŒ…ì—ì„œ ëª¨ë¸ì„ llava:13bë¡œ ë³€ê²½</span>
<span class="str">2. ì´ë¯¸ì§€ ì—…ë¡œë“œ (PNG, JPG, WEBP)</span>
<span class="str">3. ì§ˆë¬¸: "ì´ ì´ë¯¸ì§€ì—ì„œ ë¬´ì—‡ì´ ë³´ì´ë‚˜ìš”?"</span>
<span class="str">4. ì§ˆë¬¸: "ì´ ë‹¤ì´ì–´ê·¸ë¨ì„ ì„¤ëª…í•´ì¤˜"</span>
<span class="str">5. ì§ˆë¬¸: "ì´ ì½”ë“œ ìŠ¤í¬ë¦°ìƒ·ì„ ë¶„ì„í•´ì¤˜"</span>

<span class="cmt"># ì‹¤ì „ í™œìš©</span>
- UI ëª©ì—… â†’ "ì´ ë””ìì¸ì„ HTML/CSSë¡œ êµ¬í˜„í•´ì¤˜"
- ì—ëŸ¬ ìŠ¤í¬ë¦°ìƒ· â†’ "ì´ ì—ëŸ¬ ì›ì¸ê³¼ í•´ê²° ë°©ë²•ì€?"
- ë‹¤ì´ì–´ê·¸ë¨ â†’ "ì´ ì•„í‚¤í…ì²˜ì˜ ë¬¸ì œì ì€?"
- ì†ê¸€ì”¨ ìˆ˜í•™ â†’ "ì´ ìˆ˜ì‹ì„ í’€ì–´ì¤˜"</code></pre>

  <h4>4. ìŒì„± ì…ë ¥ & ì¶œë ¥</h4>

  <pre><code><span class="cmt"># STT (Speech-to-Text) ì„¤ì •</span>
<span class="str">Settings â†’ Audio â†’ STT</span>
<span class="kw">Engine</span>: Web API (ë¸Œë¼ìš°ì € ë‚´ì¥) ë˜ëŠ” Whisper API

<span class="cmt"># Whisper (Ollama) ì‚¬ìš©</span>
<span class="str">1. Whisper ëª¨ë¸ ë‹¤ìš´ë¡œë“œ</span>
ollama pull whisper

<span class="str">2. Open WebUI ì„¤ì •</span>
<span class="str">Settings â†’ Audio â†’ STT Engine: Whisper (Ollama)</span>
<span class="str">Settings â†’ Audio â†’ STT Model: whisper</span>

<span class="str">3. ì‚¬ìš©: ì±„íŒ… ì…ë ¥ì°½ ğŸ¤ ì•„ì´ì½˜ í´ë¦­ â†’ ë§í•˜ê¸°</span>

<span class="cmt"># TTS (Text-to-Speech)</span>
<span class="str">Settings â†’ Audio â†’ TTS</span>
<span class="kw">Engine</span>: Web API (ë¬´ë£Œ) ë˜ëŠ” ElevenLabs (ê³ í’ˆì§ˆ)

<span class="cmt"># ë©”ì‹œì§€ ìŒì„± ì¬ìƒ</span>
<span class="str">1. AI ì‘ë‹µ ìœ„ì— ë§ˆìš°ìŠ¤ ì˜¤ë²„</span>
<span class="str">2. ğŸ”Š ì•„ì´ì½˜ í´ë¦­</span>
<span class="str">3. ìë™ìœ¼ë¡œ ìŒì„± ì¬ìƒ</span></code></pre>

  <h4>5. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ & í”„ë¦¬ì…‹</h4>

  <pre><code><span class="cmt"># í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±</span>
<span class="str">Settings â†’ Prompts â†’ + New Prompt</span>

<span class="cmt"># í…œí”Œë¦¿ 1: ì½”ë“œ ë¦¬ë·°</span>
<span class="kw">Name</span>: Code Review
<span class="kw">Command</span>: /review
<span class="kw">Prompt</span>:
<span class="str">"""
ë‹¹ì‹ ì€ 10ë…„ ê²½ë ¥ì˜ ì‹œë‹ˆì–´ ê°œë°œìì…ë‹ˆë‹¤.
ë‹¤ìŒ ì½”ë“œë¥¼ ìƒì„¸íˆ ë¦¬ë·°í•˜ê³  ê°œì„ ì ì„ ì œì•ˆí•´ì£¼ì„¸ìš”:

**ì²´í¬ë¦¬ìŠ¤íŠ¸:**
1. ë²„ê·¸ ë° ì˜¤ë¥˜ ê°€ëŠ¥ì„±
2. ì„±ëŠ¥ ìµœì í™” ê¸°íšŒ
3. ì½”ë“œ ê°€ë…ì„± ë° ìœ ì§€ë³´ìˆ˜ì„±
4. ë³´ì•ˆ ì·¨ì•½ì  (SQL injection, XSS ë“±)
5. ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤ ì¤€ìˆ˜ ì—¬ë¶€

**ì½”ë“œ:**
{{CODE}}

**ë¦¬ë·° í˜•ì‹:**
- ğŸ”´ Critical: ì¦‰ì‹œ ìˆ˜ì • í•„ìš”
- ğŸŸ¡ Important: ê°œì„  ê¶Œì¥
- ğŸŸ¢ Nice to have: ì„ íƒì  ê°œì„ 
"""</span>

<span class="cmt"># ì‚¬ìš©</span>
<span class="str">ì±„íŒ…ì—ì„œ "/review" ì…ë ¥ í›„ ì½”ë“œ ë¶™ì—¬ë„£ê¸°</span>

<span class="cmt"># í…œí”Œë¦¿ 2: ë¬¸ì„œ ìš”ì•½</span>
<span class="kw">Name</span>: Summarize Document
<span class="kw">Command</span>: /summary
<span class="kw">Prompt</span>:
<span class="str">"""
ë‹¤ìŒ ë¬¸ì„œë¥¼ 3ë‹¨ê³„ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”:

1. **í•œ ì¤„ ìš”ì•½** (í•µì‹¬ ë©”ì‹œì§€)
2. **ì£¼ìš” í¬ì¸íŠ¸** (3-5ê°œ ë¶ˆë¦¿ í¬ì¸íŠ¸)
3. **ìƒì„¸ ìš”ì•½** (1-2 ë¬¸ë‹¨)

ë¬¸ì„œ:
{{DOCUMENT}}
"""</span>

<span class="cmt"># í…œí”Œë¦¿ 3: ë²ˆì—­</span>
<span class="kw">Name</span>: Translate
<span class="kw">Command</span>: /translate
<span class="kw">Prompt</span>:
<span class="str">"""
ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ {{TARGET_LANG}}ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ë²ˆì—­í•´ì£¼ì„¸ìš”.
ì „ë¬¸ ìš©ì–´ëŠ” ì •í™•í•˜ê²Œ ë²ˆì—­í•˜ê³ , ë§¥ë½ì„ ìœ ì§€í•˜ì„¸ìš”.

ì›ë¬¸:
{{TEXT}}
"""</span>

<span class="cmt"># í…œí”Œë¦¿ 4: í•™ìŠµ ë„ìš°ë¯¸</span>
<span class="kw">Name</span>: Explain Like I'm 5
<span class="kw">Command</span>: /eli5
<span class="kw">Prompt</span>:
<span class="str">"""
ë‹¤ìŒ ê°œë…ì„ 5ì‚´ ì•„ì´ë„ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ì‰½ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”:
- ë¹„ìœ ì™€ ì˜ˆì‹œ ì‚¬ìš©
- ì „ë¬¸ ìš©ì–´ ìµœì†Œí™”
- ë‹¨ê³„ë³„ ì„¤ëª…

ê°œë…: {{CONCEPT}}
"""</span></code></pre>

  <h3 id="webui-admin">ê´€ë¦¬ì ê¸°ëŠ¥</h3>

  <h4>ì‚¬ìš©ì ê´€ë¦¬</h4>

  <pre><code><span class="cmt"># Admin Panel ì ‘ê·¼</span>
<span class="str">Settings â†’ Admin Panel (ê´€ë¦¬ìë§Œ ë³´ì„)</span>

<span class="cmt"># ì‚¬ìš©ì ì¶”ê°€</span>
<span class="str">Admin Panel â†’ Users â†’ + Add User</span>
<span class="kw">Name</span>: í™ê¸¸ë™
<span class="kw">Email</span>: hong@company.com
<span class="kw">Password</span>: (ì„ì‹œ ë¹„ë°€ë²ˆí˜¸)
<span class="kw">Role</span>: User (ë˜ëŠ” Admin)

<span class="cmt"># ì‚¬ìš©ì ê¶Œí•œ ê´€ë¦¬</span>
<span class="str">User Actions:</span>
- Edit: ì •ë³´ ìˆ˜ì •
- Delete: ì‚¬ìš©ì ì‚­ì œ
- Ban: ì¼ì‹œ ì •ì§€
- Set as Admin: ê´€ë¦¬ì ìŠ¹ê²©

<span class="cmt"># ëŒ€ëŸ‰ ì‚¬ìš©ì ë“±ë¡ (CSV)</span>
<span class="str">1. CSV íŒŒì¼ ì¤€ë¹„</span>
name,email,role
ê¹€ì² ìˆ˜,kim@company.com,user
ì´ì˜í¬,lee@company.com,admin

<span class="str">2. Admin Panel â†’ Import Users</span>
<span class="str">3. CSV ì—…ë¡œë“œ â†’ ìë™ ìƒì„±</span></code></pre>

  <h4>ëª¨ë¸ ê´€ë¦¬</h4>

  <pre><code><span class="cmt"># ëª¨ë¸ ë‹¤ìš´ë¡œë“œ</span>
<span class="str">Settings â†’ Models â†’ Pull a model</span>
llama3.2:7b
codellama:13b
mistral:latest
qwen2.5-coder:14b

<span class="cmt"># ëª¨ë¸ ì‚­ì œ</span>
<span class="str">ëª¨ë¸ ìš°í´ë¦­ â†’ Delete</span>

<span class="cmt"># ëª¨ë¸ ë³„ì¹­ (ì‚¬ìš©ì ì¹œí™”ì  ì´ë¦„)</span>
<span class="str">Settings â†’ Models â†’ Model Aliases</span>
llama3.2:7b â†’ "ì¼ë°˜ ëŒ€í™”"
codellama:13b â†’ "ì½”ë”© ì „ë¬¸ê°€"

<span class="cmt"># ëª¨ë¸ ì ‘ê·¼ ì œì–´</span>
<span class="str">Admin Panel â†’ Models â†’ Permissions</span>
<span class="str">íŠ¹ì • ì‚¬ìš©ì/ê·¸ë£¹ë§Œ íŠ¹ì • ëª¨ë¸ ì‚¬ìš© í—ˆìš©</span>
<span class="str">ì˜ˆ: GPT-4 ëª¨ë¸ì€ Senior íŒ€ë§Œ ì ‘ê·¼</span></code></pre>

  <h4>ì‹œìŠ¤í…œ ì„¤ì •</h4>

  <pre><code><span class="cmt"># íšŒì›ê°€ì… ì œì–´</span>
<span class="str">Admin Panel â†’ Settings â†’ General</span>
<span class="kw">Enable Signup</span>: false     <span class="cmt"># ê³µê°œ ê°€ì… ì°¨ë‹¨</span>
<span class="kw">Default Role</span>: user        <span class="cmt"># ê¸°ë³¸ ê¶Œí•œ</span>

<span class="cmt"># API í‚¤ ê´€ë¦¬</span>
<span class="str">Admin Panel â†’ Settings â†’ API Keys</span>
<span class="str">ì™¸ë¶€ ì„œë¹„ìŠ¤ ì—°ë™ìš© API í‚¤ ìƒì„±/ê´€ë¦¬</span>

<span class="cmt"># ê°ì‚¬ ë¡œê·¸</span>
<span class="str">Admin Panel â†’ Audit Logs</span>
<span class="str">ëª¨ë“  ì‚¬ìš©ì í™œë™ ë¡œê·¸ í™•ì¸</span>
- ë¡œê·¸ì¸/ë¡œê·¸ì•„ì›ƒ
- ëª¨ë¸ ì‚¬ìš©
- ë¬¸ì„œ ì—…ë¡œë“œ/ì‚­ì œ
- ì„¤ì • ë³€ê²½</code></pre>

  <h3 id="webui-plugins">í”ŒëŸ¬ê·¸ì¸ & í™•ì¥</h3>

  <h4>1. Web Search (ì‹¤ì‹œê°„ ê²€ìƒ‰)</h4>

  <pre><code><span class="cmt"># SearxNG ì—°ë™</span>
<span class="str">Settings â†’ Web Search</span>
<span class="kw">Enable</span>: true
<span class="kw">Engine</span>: SearxNG
<span class="kw">URL</span>: https://searx.example.com

<span class="cmt"># ì‚¬ìš©</span>
<span class="str">ì±„íŒ…ì—ì„œ "#" ì…ë ¥ í›„ ê²€ìƒ‰</span>
<span class="str">ì˜ˆ: "#ì˜¤ëŠ˜ ë‚ ì”¨ëŠ”?" â†’ ì‹¤ì‹œê°„ ê²€ìƒ‰ í›„ ë‹µë³€</span></code></pre>

  <h4>2. Python Code Interpreter</h4>

  <pre><code><span class="cmt"># ì„¤ì •</span>
<span class="str">Settings â†’ Functions â†’ Python Interpreter</span>
<span class="kw">Enable</span>: true

<span class="cmt"># ì‚¬ìš©</span>
<span class="str">ì±„íŒ…:</span> "í”¼ë³´ë‚˜ì¹˜ ìˆ˜ì—´ 20ê°œ ê³„ì‚°í•´ì¤˜"
<span class="str">â†’ Python ì½”ë“œ ìë™ ìƒì„± ë° ì‹¤í–‰ â†’ ê²°ê³¼ í‘œì‹œ</span>

<span class="cmt"># ì§€ì› ë¼ì´ë¸ŒëŸ¬ë¦¬</span>
numpy, pandas, matplotlib, requests, beautifulsoup4</code></pre>

  <h4>3. Wolfram Alpha (ìˆ˜í•™/ê³¼í•™)</h4>

  <pre><code><span class="cmt"># API í‚¤ ë°œê¸‰</span>
<span class="str">1. https://products.wolframalpha.com/api/ ê°€ì…</span>
<span class="str">2. App ID ë°œê¸‰</span>

<span class="cmt"># ì„¤ì •</span>
<span class="str">Settings â†’ Functions â†’ Wolfram Alpha</span>
<span class="kw">App ID</span>: YOUR_APP_ID

<span class="cmt"># ì‚¬ìš©</span>
<span class="str">"âˆ« xÂ² dx ê³„ì‚°í•´ì¤˜"</span>
<span class="str">"íƒœì–‘ê³¼ ì§€êµ¬ì˜ ê±°ë¦¬ëŠ”?"</span></code></pre>

  <h3 id="webui-api">API í†µí•©</h3>

  <pre><code><span class="cmt"># Open WebUI API í‚¤ ìƒì„±</span>
<span class="str">Settings â†’ Account â†’ API Keys â†’ + New Key</span>
<span class="kw">Name</span>: MyApp
<span class="kw">Expires</span>: Never (ë˜ëŠ” 30 days)

<span class="cmt"># Pythonì—ì„œ API ì‚¬ìš©</span>
<span class="kw">import</span> requests

url = <span class="str">"http://localhost:3000/api/chat"</span>
headers = {
    <span class="str">"Authorization"</span>: <span class="str">"Bearer YOUR_API_KEY"</span>,
    <span class="str">"Content-Type"</span>: <span class="str">"application/json"</span>
}
payload = {
    <span class="str">"model"</span>: <span class="str">"llama3.2"</span>,
    <span class="str">"messages"</span>: [
        {<span class="str">"role"</span>: <span class="str">"user"</span>, <span class="str">"content"</span>: <span class="str">"Hello!"</span>}
    ]
}

response = requests.post(url, json=payload, headers=headers)
<span class="kw">print</span>(response.json())

<span class="cmt"># JavaScriptì—ì„œ ì‚¬ìš©</span>
<span class="kw">const</span> response = <span class="kw">await</span> fetch(<span class="str">'http://localhost:3000/api/chat'</span>, {
  method: <span class="str">'POST'</span>,
  headers: {
    <span class="str">'Authorization'</span>: <span class="str">'Bearer YOUR_API_KEY'</span>,
    <span class="str">'Content-Type'</span>: <span class="str">'application/json'</span>
  },
  body: JSON.stringify({
    model: <span class="str">'llama3.2'</span>,
    messages: [{role: <span class="str">'user'</span>, content: <span class="str">'Hello!'</span>}]
  })
});

<span class="kw">const</span> data = <span class="kw">await</span> response.json();
console.log(data);</code></pre>

  <h3 id="webui-deployment">í”„ë¡œë•ì…˜ ë°°í¬</h3>

  <h4>HTTPS ì„¤ì • (Nginx + Let's Encrypt)</h4>

  <pre><code><span class="cmt"># Nginx ì„¤ì •</span>
<span class="kw">server</span> {
    <span class="kw">listen</span> <span class="num">80</span>;
    <span class="kw">server_name</span> ai.company.com;
    <span class="kw">return</span> <span class="num">301</span> https://<span class="macro">$server_name$request_uri</span>;
}

<span class="kw">server</span> {
    <span class="kw">listen</span> <span class="num">443</span> ssl http2;
    <span class="kw">server_name</span> ai.company.com;

    <span class="kw">ssl_certificate</span> /etc/letsencrypt/live/ai.company.com/fullchain.pem;
    <span class="kw">ssl_certificate_key</span> /etc/letsencrypt/live/ai.company.com/privkey.pem;

    <span class="kw">location</span> / {
        <span class="kw">proxy_pass</span> http://localhost:<span class="num">3000</span>;
        <span class="kw">proxy_http_version</span> <span class="num">1.1</span>;
        <span class="kw">proxy_set_header</span> Upgrade <span class="macro">$http_upgrade</span>;
        <span class="kw">proxy_set_header</span> Connection <span class="str">'upgrade'</span>;
        <span class="kw">proxy_set_header</span> Host <span class="macro">$host</span>;
        <span class="kw">proxy_cache_bypass</span> <span class="macro">$http_upgrade</span>;
    }
}</code></pre>

  <h4>ë°±ì—… ë° ë³µì›</h4>

  <pre><code><span class="cmt"># ë°ì´í„° ë°±ì—…</span>
docker run --rm \
  -v open-webui:/source \
  -v $(pwd):/backup \
  alpine tar czf /backup/open-webui-backup-$(date +%Y%m%d).tar.gz -C /source .

<span class="cmt"># ë³µì›</span>
docker run --rm \
  -v open-webui:/target \
  -v $(pwd):/backup \
  alpine sh -c <span class="str">"cd /target && tar xzf /backup/open-webui-backup-20250212.tar.gz"</span>

<span class="cmt"># ìë™ ë°±ì—… (cron)</span>
<span class="num">0</span> <span class="num">2</span> * * * /path/to/backup-script.sh</code></pre>

  <h4>í™˜ê²½ ë³€ìˆ˜ ì „ì²´ ëª©ë¡</h4>

  <pre><code><span class="cmt"># docker-compose.yml ë˜ëŠ” .env íŒŒì¼</span>
<span class="kw">OLLAMA_BASE_URL</span>=http://ollama:11434
<span class="kw">WEBUI_SECRET_KEY</span>=random-secret-key-min-32-chars
<span class="kw">WEBUI_AUTH</span>=true
<span class="kw">ENABLE_SIGNUP</span>=false
<span class="kw">DEFAULT_MODELS</span>=llama3.2,codellama:13b
<span class="kw">DEFAULT_USER_ROLE</span>=user
<span class="kw">ENABLE_RAG_WEB_SEARCH</span>=true
<span class="kw">RAG_EMBEDDING_MODEL</span>=nomic-embed-text
<span class="kw">CHUNK_SIZE</span>=1000
<span class="kw">CHUNK_OVERLAP</span>=200
<span class="kw">RAG_TOP_K</span>=5
<span class="kw">ENABLE_IMAGE_GENERATION</span>=false
<span class="kw">IMAGE_GENERATION_API_URL</span>=http://stable-diffusion:7860
<span class="kw">WHISPER_MODEL</span>=whisper
<span class="kw">TTS_ENGINE</span>=web_api</code></pre>

  <div class="info-box tip">
    <div class="info-box-title">ğŸš€ í”„ë¡œë•ì…˜ ì²´í¬ë¦¬ìŠ¤íŠ¸</div>
    <ul>
      <li>âœ“ HTTPS ì„¤ì • (Let's Encrypt)</li>
      <li>âœ“ íšŒì›ê°€ì… ë¹„í™œì„±í™” (ENABLE_SIGNUP=false)</li>
      <li>âœ“ ê°•ë ¥í•œ SECRET_KEY ì„¤ì •</li>
      <li>âœ“ ì •ê¸° ë°±ì—… (ë§¤ì¼ 2AM)</li>
      <li>âœ“ ë¦¬ë²„ìŠ¤ í”„ë¡ì‹œ (Nginx/Caddy)</li>
      <li>âœ“ ë°©í™”ë²½ ì„¤ì • (3000 í¬íŠ¸ ë‚´ë¶€ë§ë§Œ)</li>
      <li>âœ“ ë¡œê·¸ ëª¨ë‹ˆí„°ë§</li>
      <li>âœ“ ë¦¬ì†ŒìŠ¤ ì œí•œ (Docker memory/CPU)</li>
    </ul>
  </div>

  <h3 id="webui-troubleshooting">ë¬¸ì œ í•´ê²°</h3>

  <pre><code><span class="cmt"># ë¬¸ì œ 1: Ollama ì—°ê²° ì•ˆë¨</span>
<span class="str">ì¦ìƒ:</span> "Cannot connect to Ollama"
<span class="str">í•´ê²°:</span>
1. Ollamaê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸: curl http://localhost:11434
2. Docker ë„¤íŠ¸ì›Œí¬: host.docker.internal â†’ localhost ë³€ê²½
3. OLLAMA_BASE_URL í™˜ê²½ ë³€ìˆ˜ í™•ì¸

<span class="cmt"># ë¬¸ì œ 2: ëª¨ë¸ì´ ë³´ì´ì§€ ì•ŠìŒ</span>
<span class="str">í•´ê²°:</span>
ollama list                    <span class="cmt"># ëª¨ë¸ í™•ì¸</span>
ollama pull llama3.2           <span class="cmt"># ëª¨ë¸ ë‹¤ìš´ë¡œë“œ</span>
docker restart open-webui      <span class="cmt"># ì¬ì‹œì‘</span>

<span class="cmt"># ë¬¸ì œ 3: RAG ì„ë² ë”© ì‹¤íŒ¨</span>
<span class="str">í•´ê²°:</span>
ollama pull nomic-embed-text   <span class="cmt"># ì„ë² ë”© ëª¨ë¸ ì„¤ì¹˜</span>
<span class="str">Settings â†’ Documents â†’ RAG â†’ Embedding Model ì„¤ì •</span>

<span class="cmt"># ë¬¸ì œ 4: ë©”ëª¨ë¦¬ ë¶€ì¡±</span>
<span class="str">í•´ê²°:</span>
<span class="cmt"># docker-compose.ymlì— ë¦¬ì†ŒìŠ¤ ì œí•œ ì¶”ê°€</span>
<span class="kw">deploy</span>:
  <span class="kw">resources</span>:
    <span class="kw">limits</span>:
      <span class="kw">memory</span>: 4G

<span class="cmt"># ë¬¸ì œ 5: ëŠë¦° ì‘ë‹µ</span>
<span class="str">í•´ê²°:</span>
1. ë” ì‘ì€ ëª¨ë¸ ì‚¬ìš© (7B â†’ 3B)
2. GPU í™œì„±í™” (--gpus all)
3. Ollama num_ctx ì¤„ì´ê¸° (4096 â†’ 2048)</code></pre>
</section>

<section class="content-section">
  <h2 id="langchain">LangChain + Ollama (Python)</h2>

  <p>
    <strong>LangChain</strong>ì€ LLM ê¸°ë°˜ ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.
    Ollamaì™€ í†µí•©í•˜ì—¬ ì²´ì¸, ì—ì´ì „íŠ¸, RAG íŒŒì´í”„ë¼ì¸ì„ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
  </p>

  <h3 id="langchain-install">ì„¤ì¹˜</h3>

  <pre><code><span class="cmt"># LangChain ë° Ollama í†µí•© ì„¤ì¹˜</span>
pip install langchain langchain-community

<span class="cmt"># ë˜ëŠ” ì „ì²´ íŒ¨í‚¤ì§€</span>
pip install langchain langchain-community langchain-ollama</code></pre>

  <h3 id="langchain-basic">ê¸°ë³¸ ì‚¬ìš©</h3>

  <h4>1. ê°„ë‹¨í•œ LLM í˜¸ì¶œ</h4>

  <pre><code><span class="kw">from</span> langchain_community.llms <span class="kw">import</span> Ollama

<span class="cmt"># Ollama LLM ì´ˆê¸°í™”</span>
llm = Ollama(model=<span class="str">"llama3.2"</span>)

<span class="cmt"># í…ìŠ¤íŠ¸ ìƒì„±</span>
response = llm.invoke(<span class="str">"íŒŒì´ì¬ìœ¼ë¡œ í€µì†ŒíŠ¸ ì•Œê³ ë¦¬ì¦˜ì„ ì‘ì„±í•´ì¤˜"</span>)
<span class="kw">print</span>(response)</code></pre>

  <h4>2. ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ</h4>

  <pre><code><span class="kw">from</span> langchain_community.llms <span class="kw">import</span> Ollama

llm = Ollama(model=<span class="str">"llama3.2"</span>)

<span class="cmt"># ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì‘ë‹µ ë°›ê¸°</span>
<span class="kw">for</span> chunk <span class="kw">in</span> llm.stream(<span class="str">"ê¸´ ì´ì•¼ê¸°ë¥¼ ë“¤ë ¤ì¤˜"</span>):
    <span class="kw">print</span>(chunk, end=<span class="str">""</span>, flush=<span class="kw">True</span>)</code></pre>

  <h4>3. ì±„íŒ… ëª¨ë¸</h4>

  <pre><code><span class="kw">from</span> langchain_community.chat_models <span class="kw">import</span> ChatOllama
<span class="kw">from</span> langchain.schema <span class="kw">import</span> HumanMessage, SystemMessage

chat = ChatOllama(model=<span class="str">"llama3.2"</span>)

messages = [
    SystemMessage(content=<span class="str">"ë‹¹ì‹ ì€ íŒŒì´ì¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."</span>),
    HumanMessage(content=<span class="str">"FastAPIë¡œ REST APIë¥¼ ë§Œë“œëŠ” ë°©ë²•ì„ ì•Œë ¤ì¤˜"</span>)
]

response = chat.invoke(messages)
<span class="kw">print</span>(response.content)</code></pre>

  <h3 id="langchain-chains">ì²´ì¸ (Chains)</h3>

  <h4>í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿</h4>

  <pre><code><span class="kw">from</span> langchain_community.llms <span class="kw">import</span> Ollama
<span class="kw">from</span> langchain.prompts <span class="kw">import</span> PromptTemplate
<span class="kw">from</span> langchain.chains <span class="kw">import</span> LLMChain

llm = Ollama(model=<span class="str">"codellama:13b"</span>)

<span class="cmt"># í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜</span>
template = <span class="str">"""ë‹¹ì‹ ì€ {language} ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ë‹¤ìŒ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”:

ì‘ì—…: {task}

ì½”ë“œ:"""</span>

prompt = PromptTemplate(
    input_variables=[<span class="str">"language"</span>, <span class="str">"task"</span>],
    template=template
)

<span class="cmt"># ì²´ì¸ ìƒì„±</span>
chain = LLMChain(llm=llm, prompt=prompt)

<span class="cmt"># ì‹¤í–‰</span>
result = chain.run(
    language=<span class="str">"Python"</span>,
    task=<span class="str">"CSV íŒŒì¼ì„ ì½ê³  Pandas DataFrameìœ¼ë¡œ ë³€í™˜"</span>
)
<span class="kw">print</span>(result)</code></pre>

  <h4>ìˆœì°¨ ì²´ì¸</h4>

  <pre><code><span class="kw">from</span> langchain.chains <span class="kw">import</span> SimpleSequentialChain, LLMChain
<span class="kw">from</span> langchain.prompts <span class="kw">import</span> PromptTemplate
<span class="kw">from</span> langchain_community.llms <span class="kw">import</span> Ollama

llm = Ollama(model=<span class="str">"llama3.2"</span>)

<span class="cmt"># ì²´ì¸ 1: ì•„ì´ë””ì–´ ìƒì„±</span>
template1 = <span class="str">"ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ ì•„ì´ë””ì–´ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ ì œì•ˆí•´ì¤˜: {topic}"</span>
prompt1 = PromptTemplate(input_variables=[<span class="str">"topic"</span>], template=template1)
chain1 = LLMChain(llm=llm, prompt=prompt1)

<span class="cmt"># ì²´ì¸ 2: ê¸°ìˆ  ìŠ¤íƒ ì œì•ˆ</span>
template2 = <span class="str">"ì´ ì•„ì´ë””ì–´ì— ì í•©í•œ ê¸°ìˆ  ìŠ¤íƒì„ ì¶”ì²œí•´ì¤˜: {idea}"</span>
prompt2 = PromptTemplate(input_variables=[<span class="str">"idea"</span>], template=template2)
chain2 = LLMChain(llm=llm, prompt=prompt2)

<span class="cmt"># ìˆœì°¨ ì‹¤í–‰</span>
overall_chain = SimpleSequentialChain(chains=[chain1, chain2])
result = overall_chain.run(<span class="str">"AI ê¸°ë°˜ í•™ìŠµ í”Œë«í¼"</span>)
<span class="kw">print</span>(result)</code></pre>

  <h3 id="langchain-rag">RAG (Retrieval-Augmented Generation)</h3>

  <pre><code><span class="kw">from</span> langchain_community.llms <span class="kw">import</span> Ollama
<span class="kw">from</span> langchain_community.embeddings <span class="kw">import</span> OllamaEmbeddings
<span class="kw">from</span> langchain_community.vectorstores <span class="kw">import</span> Chroma
<span class="kw">from</span> langchain.text_splitter <span class="kw">import</span> RecursiveCharacterTextSplitter
<span class="kw">from</span> langchain.chains <span class="kw">import</span> RetrievalQA
<span class="kw">from</span> langchain_community.document_loaders <span class="kw">import</span> TextLoader

<span class="cmt"># 1. ë¬¸ì„œ ë¡œë“œ</span>
loader = TextLoader(<span class="str">"company_docs.txt"</span>)
documents = loader.load()

<span class="cmt"># 2. í…ìŠ¤íŠ¸ ë¶„í• </span>
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=<span class="num">1000</span>,
    chunk_overlap=<span class="num">200</span>
)
texts = text_splitter.split_documents(documents)

<span class="cmt"># 3. ì„ë² ë”© ìƒì„± (Ollama)</span>
embeddings = OllamaEmbeddings(model=<span class="str">"nomic-embed-text"</span>)

<span class="cmt"># 4. ë²¡í„° DB ìƒì„±</span>
vectorstore = Chroma.from_documents(
    documents=texts,
    embedding=embeddings,
    persist_directory=<span class="str">"./chroma_db"</span>
)

<span class="cmt"># 5. RAG ì²´ì¸ ìƒì„±</span>
llm = Ollama(model=<span class="str">"llama3.2"</span>)
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type=<span class="str">"stuff"</span>,
    retriever=vectorstore.as_retriever(search_kwargs={<span class="str">"k"</span>: <span class="num">3</span>})
)

<span class="cmt"># 6. ì§ˆë¬¸</span>
query = <span class="str">"íšŒì‚¬ì˜ íœ´ê°€ ì •ì±…ì€ ë¬´ì—‡ì¸ê°€ìš”?"</span>
result = qa_chain.invoke({<span class="str">"query"</span>: query})
<span class="kw">print</span>(result[<span class="str">"result"</span>])</code></pre>

  <h3 id="langchain-agents">ì—ì´ì „íŠ¸</h3>

  <pre><code><span class="kw">from</span> langchain.agents <span class="kw">import</span> initialize_agent, Tool
<span class="kw">from</span> langchain.agents <span class="kw">import</span> AgentType
<span class="kw">from</span> langchain_community.llms <span class="kw">import</span> Ollama
<span class="kw">import</span> requests

<span class="cmt"># ë„êµ¬ ì •ì˜</span>
<span class="kw">def</span> <span class="fn">search_wikipedia</span>(query: <span class="type">str</span>) -&gt; <span class="type">str</span>:
    url = <span class="str">f"https://en.wikipedia.org/api/rest_v1/page/summary/{query}"</span>
    response = requests.get(url)
    <span class="kw">if</span> response.status_code == <span class="num">200</span>:
        <span class="kw">return</span> response.json().get(<span class="str">"extract"</span>, <span class="str">"No results"</span>)
    <span class="kw">return</span> <span class="str">"Error"</span>

<span class="kw">def</span> <span class="fn">calculate</span>(expression: <span class="type">str</span>) -&gt; <span class="type">str</span>:
    <span class="kw">try</span>:
        <span class="kw">return</span> str(eval(expression))
    <span class="kw">except</span>:
        <span class="kw">return</span> <span class="str">"Invalid expression"</span>

tools = [
    Tool(
        name=<span class="str">"Wikipedia"</span>,
        func=search_wikipedia,
        description=<span class="str">"ìœ„í‚¤í”¼ë””ì•„ì—ì„œ ì •ë³´ ê²€ìƒ‰"</span>
    ),
    Tool(
        name=<span class="str">"Calculator"</span>,
        func=calculate,
        description=<span class="str">"ìˆ˜í•™ ê³„ì‚° ìˆ˜í–‰"</span>
    )
]

<span class="cmt"># ì—ì´ì „íŠ¸ ì´ˆê¸°í™”</span>
llm = Ollama(model=<span class="str">"llama3.2"</span>, temperature=<span class="num">0</span>)
agent = initialize_agent(
    tools,
    llm,
    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
    verbose=<span class="kw">True</span>
)

<span class="cmt"># ì‹¤í–‰</span>
result = agent.run(<span class="str">"íŒŒì´ì¬ì´ ì–¸ì œ ë§Œë“¤ì–´ì¡ŒëŠ”ì§€ ì°¾ê³ , í˜„ì¬ ì—°ë„ì—ì„œ ë¹¼ì„œ ë‚˜ì´ë¥¼ ê³„ì‚°í•´ì¤˜"</span>)
<span class="kw">print</span>(result)</code></pre>
</section>

<section class="content-section">
  <h2 id="llamaindex">LlamaIndex + Ollama (RAG)</h2>

  <p>
    <strong>LlamaIndex</strong>ëŠ” RAG(ê²€ìƒ‰ ì¦ê°• ìƒì„±) ì „ë¬¸ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.
    ëŒ€ìš©ëŸ‰ ë¬¸ì„œ, ë°ì´í„°ë² ì´ìŠ¤, APIì™€ LLMì„ ì—°ê²°í•˜ì—¬ ì§€ëŠ¥í˜• ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•©ë‹ˆë‹¤.
  </p>

  <h3 id="llamaindex-install">ì„¤ì¹˜</h3>

  <pre><code><span class="cmt"># LlamaIndex ë° Ollama í†µí•©</span>
pip install llama-index llama-index-llms-ollama llama-index-embeddings-ollama</code></pre>

  <h3 id="llamaindex-basic">ê¸°ë³¸ RAG íŒŒì´í”„ë¼ì¸</h3>

  <pre><code><span class="kw">from</span> llama_index.core <span class="kw">import</span> VectorStoreIndex, SimpleDirectoryReader, Settings
<span class="kw">from</span> llama_index.llms.ollama <span class="kw">import</span> Ollama
<span class="kw">from</span> llama_index.embeddings.ollama <span class="kw">import</span> OllamaEmbedding

<span class="cmt"># 1. LLM ë° ì„ë² ë”© ì„¤ì •</span>
Settings.llm = Ollama(model=<span class="str">"llama3.2"</span>, request_timeout=<span class="num">120.0</span>)
Settings.embed_model = OllamaEmbedding(model_name=<span class="str">"nomic-embed-text"</span>)

<span class="cmt"># 2. ë¬¸ì„œ ë¡œë“œ (ë””ë ‰í† ë¦¬ ë‚´ ëª¨ë“  íŒŒì¼)</span>
documents = SimpleDirectoryReader(<span class="str">"./docs"</span>).load_data()

<span class="cmt"># 3. ì¸ë±ìŠ¤ ìƒì„±</span>
index = VectorStoreIndex.from_documents(documents)

<span class="cmt"># 4. ì¿¼ë¦¬ ì—”ì§„ ìƒì„±</span>
query_engine = index.as_query_engine()

<span class="cmt"># 5. ì§ˆë¬¸</span>
response = query_engine.query(<span class="str">"í”„ë¡œì íŠ¸ì˜ ì£¼ìš” ê¸°ëŠ¥ì€ ë¬´ì—‡ì¸ê°€ìš”?"</span>)
<span class="kw">print</span>(response)</code></pre>

  <h3 id="llamaindex-advanced">ê³ ê¸‰ ê¸°ëŠ¥</h3>

  <h4>1. ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸</h4>

  <pre><code><span class="kw">from</span> llama_index.core <span class="kw">import</span> PromptTemplate

<span class="cmt"># ì»¤ìŠ¤í…€ QA í”„ë¡¬í”„íŠ¸</span>
qa_template = PromptTemplate(
    <span class="str">"""ë‹¹ì‹ ì€ ê¸°ìˆ  ë¬¸ì„œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

Context information:
---------------------
{context_str}
---------------------

Question: {query_str}

ìœ„ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ìƒì„¸í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.
ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ì—†ë‹¤ë©´ "ì •ë³´ ì—†ìŒ"ì´ë¼ê³  ë‹µí•˜ì„¸ìš”."""</span>
)

query_engine = index.as_query_engine(
    text_qa_template=qa_template
)

response = query_engine.query(<span class="str">"API ì¸ì¦ ë°©ë²•ì€?"</span>)
<span class="kw">print</span>(response)</code></pre>

  <h4>2. ì±„íŒ… ì—”ì§„</h4>

  <pre><code><span class="cmt"># ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ìœ ì§€í•˜ëŠ” ì±„íŒ… ì—”ì§„</span>
chat_engine = index.as_chat_engine()

<span class="cmt"># ëŒ€í™”</span>
response1 = chat_engine.chat(<span class="str">"FastAPIê°€ ë­ì•¼?"</span>)
<span class="kw">print</span>(response1)

<span class="cmt"># ì´ì „ ëŒ€í™”ë¥¼ ê¸°ì–µ</span>
response2 = chat_engine.chat(<span class="str">"ê·¸ëŸ¼ Flaskì™€ ë¹„êµí•˜ë©´?"</span>)
<span class="kw">print</span>(response2)</code></pre>

  <h4>3. ë‹¤ì¤‘ ë¬¸ì„œ ì†ŒìŠ¤</h4>

  <pre><code><span class="kw">from</span> llama_index.core <span class="kw">import</span> SimpleDirectoryReader
<span class="kw">from</span> llama_index.readers.web <span class="kw">import</span> SimpleWebPageReader

<span class="cmt"># ë¡œì»¬ íŒŒì¼</span>
local_docs = SimpleDirectoryReader(<span class="str">"./docs"</span>).load_data()

<span class="cmt"># ì›¹ í˜ì´ì§€</span>
web_docs = SimpleWebPageReader().load_data([
    <span class="str">"https://docs.python.org/3/tutorial/"</span>
])

<span class="cmt"># í•©ì³ì„œ ì¸ë±ìŠ¤ ìƒì„±</span>
all_docs = local_docs + web_docs
index = VectorStoreIndex.from_documents(all_docs)</code></pre>

  <h4>4. ì§€ì†ì  ì €ì¥</h4>

  <pre><code><span class="kw">from</span> llama_index.core <span class="kw">import</span> StorageContext, load_index_from_storage

<span class="cmt"># ì¸ë±ìŠ¤ ì €ì¥</span>
index.storage_context.persist(persist_dir=<span class="str">"./storage"</span>)

<span class="cmt"># ë‚˜ì¤‘ì— ë¡œë“œ</span>
storage_context = StorageContext.from_defaults(persist_dir=<span class="str">"./storage"</span>)
loaded_index = load_index_from_storage(storage_context)</code></pre>
</section>

<section class="content-section">
  <h2 id="api-integration">API í†µí•© (Python, JavaScript)</h2>

  <h3 id="python-api">Python API í´ë¼ì´ì–¸íŠ¸</h3>

  <h4>requests ë¼ì´ë¸ŒëŸ¬ë¦¬</h4>

  <pre><code><span class="kw">import</span> requests
<span class="kw">import</span> json

<span class="cmt"># ê¸°ë³¸ ìƒì„± ìš”ì²­</span>
<span class="kw">def</span> <span class="fn">generate</span>(prompt: <span class="type">str</span>, model: <span class="type">str</span> = <span class="str">"llama3.2"</span>) -&gt; <span class="type">str</span>:
    url = <span class="str">"http://localhost:11434/api/generate"</span>
    payload = {
        <span class="str">"model"</span>: model,
        <span class="str">"prompt"</span>: prompt,
        <span class="str">"stream"</span>: <span class="kw">False</span>
    }

    response = requests.post(url, json=payload)
    <span class="kw">return</span> response.json()[<span class="str">"response"</span>]

<span class="cmt"># ì‚¬ìš©</span>
result = generate(<span class="str">"Pythonìœ¼ë¡œ Hello World ì‘ì„±"</span>)
<span class="kw">print</span>(result)</code></pre>

  <h4>ìŠ¤íŠ¸ë¦¬ë°</h4>

  <pre><code><span class="kw">import</span> requests

<span class="kw">def</span> <span class="fn">generate_stream</span>(prompt: <span class="type">str</span>):
    url = <span class="str">"http://localhost:11434/api/generate"</span>
    payload = {
        <span class="str">"model"</span>: <span class="str">"llama3.2"</span>,
        <span class="str">"prompt"</span>: prompt,
        <span class="str">"stream"</span>: <span class="kw">True</span>
    }

    <span class="kw">with</span> requests.post(url, json=payload, stream=<span class="kw">True</span>) <span class="kw">as</span> response:
        <span class="kw">for</span> line <span class="kw">in</span> response.iter_lines():
            <span class="kw">if</span> line:
                data = json.loads(line)
                <span class="kw">if</span> <span class="str">"response"</span> <span class="kw">in</span> data:
                    <span class="kw">print</span>(data[<span class="str">"response"</span>], end=<span class="str">""</span>, flush=<span class="kw">True</span>)

generate_stream(<span class="str">"ê¸´ ì´ì•¼ê¸°ë¥¼ ë“¤ë ¤ì¤˜"</span>)</code></pre>

  <h4>ì±„íŒ… API</h4>

  <pre><code><span class="kw">import</span> requests

<span class="kw">def</span> <span class="fn">chat</span>(messages: <span class="type">list</span>) -&gt; <span class="type">str</span>:
    url = <span class="str">"http://localhost:11434/api/chat"</span>
    payload = {
        <span class="str">"model"</span>: <span class="str">"llama3.2"</span>,
        <span class="str">"messages"</span>: messages,
        <span class="str">"stream"</span>: <span class="kw">False</span>
    }

    response = requests.post(url, json=payload)
    <span class="kw">return</span> response.json()[<span class="str">"message"</span>][<span class="str">"content"</span>]

<span class="cmt"># ëŒ€í™”</span>
messages = [
    {<span class="str">"role"</span>: <span class="str">"system"</span>, <span class="str">"content"</span>: <span class="str">"ë‹¹ì‹ ì€ íŒŒì´ì¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."</span>},
    {<span class="str">"role"</span>: <span class="str">"user"</span>, <span class="str">"content"</span>: <span class="str">"FastAPIë¡œ REST API ë§Œë“œëŠ” ë°©ë²•ì€?"</span>}
]

response = chat(messages)
<span class="kw">print</span>(response)</code></pre>

  <h3 id="javascript-api">JavaScript/Node.js API í´ë¼ì´ì–¸íŠ¸</h3>

  <h4>Fetch API (ë¸Œë¼ìš°ì €)</h4>

  <pre><code><span class="cmt">// ê¸°ë³¸ ìƒì„±</span>
<span class="kw">async</span> <span class="kw">function</span> <span class="fn">generate</span>(prompt) {
  <span class="kw">const</span> response = <span class="kw">await</span> fetch(<span class="str">'http://localhost:11434/api/generate'</span>, {
    method: <span class="str">'POST'</span>,
    headers: { <span class="str">'Content-Type'</span>: <span class="str">'application/json'</span> },
    body: JSON.stringify({
      model: <span class="str">'llama3.2'</span>,
      prompt: prompt,
      stream: <span class="kw">false</span>
    })
  });

  <span class="kw">const</span> data = <span class="kw">await</span> response.json();
  <span class="kw">return</span> data.response;
}

<span class="cmt">// ì‚¬ìš©</span>
generate(<span class="str">'JavaScriptë¡œ Hello World'</span>).then(<span class="fn">console.log</span>);</code></pre>

  <h4>ìŠ¤íŠ¸ë¦¬ë° (Node.js)</h4>

  <pre><code><span class="kw">const</span> https = require(<span class="str">'https'</span>);

<span class="kw">function</span> <span class="fn">generateStream</span>(prompt) {
  <span class="kw">const</span> data = JSON.stringify({
    model: <span class="str">'llama3.2'</span>,
    prompt: prompt,
    stream: <span class="kw">true</span>
  });

  <span class="kw">const</span> options = {
    hostname: <span class="str">'localhost'</span>,
    port: <span class="num">11434</span>,
    path: <span class="str">'/api/generate'</span>,
    method: <span class="str">'POST'</span>,
    headers: {
      <span class="str">'Content-Type'</span>: <span class="str">'application/json'</span>,
      <span class="str">'Content-Length'</span>: data.length
    }
  };

  <span class="kw">const</span> req = http.request(options, (res) =&gt; {
    res.on(<span class="str">'data'</span>, (chunk) =&gt; {
      <span class="kw">const</span> line = chunk.toString();
      <span class="kw">const</span> parsed = JSON.parse(line);
      process.stdout.write(parsed.response);
    });
  });

  req.write(data);
  req.end();
}

generateStream(<span class="str">'ê¸´ ì´ì•¼ê¸°ë¥¼ ë“¤ë ¤ì¤˜'</span>);</code></pre>

  <h4>ollama-js ë¼ì´ë¸ŒëŸ¬ë¦¬</h4>

  <pre><code><span class="cmt">// ì„¤ì¹˜</span>
<span class="cmt">// npm install ollama</span>

<span class="kw">import</span> { Ollama } <span class="kw">from</span> <span class="str">'ollama'</span>;

<span class="kw">const</span> ollama = <span class="kw">new</span> Ollama({ host: <span class="str">'http://localhost:11434'</span> });

<span class="cmt">// ìƒì„±</span>
<span class="kw">const</span> response = <span class="kw">await</span> ollama.generate({
  model: <span class="str">'llama3.2'</span>,
  prompt: <span class="str">'TypeScriptë¡œ íƒ€ì… ì •ì˜í•˜ëŠ” ë°©ë²•'</span>
});

console.log(response.response);

<span class="cmt">// ì±„íŒ…</span>
<span class="kw">const</span> chatResponse = <span class="kw">await</span> ollama.chat({
  model: <span class="str">'llama3.2'</span>,
  messages: [
    { role: <span class="str">'user'</span>, content: <span class="str">'React Hook ì„¤ëª…í•´ì¤˜'</span> }
  ]
});

console.log(chatResponse.message.content);</code></pre>
</section>

<section class="content-section">
  <h2 id="other-tools">ê¸°íƒ€ ìœ ìš©í•œ ë„êµ¬</h2>

  <h3 id="twinny">Twinny (VS Code í™•ì¥)</h3>

  <pre><code><span class="cmt"># Continueì˜ ëŒ€ì•ˆ, ë” ê°€ë²¼ì›€</span>
<span class="str">1. VS Code Extensionsì—ì„œ "Twinny" ì„¤ì¹˜</span>
<span class="str">2. Settings â†’ Ollama API URL: http://localhost:11434</span>
<span class="str">3. ëª¨ë¸ ì„ íƒ: codellama:7b</span>

<span class="cmt"># íŠ¹ì§•</span>
- ë¹ ë¥¸ ìë™ì™„ì„±
- ë‚®ì€ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©
- ê°„ë‹¨í•œ UI</code></pre>

  <h3 id="shell-gpt">ShellGPT (í„°ë¯¸ë„ AI)</h3>

  <pre><code><span class="cmt"># ì„¤ì¹˜</span>
pip install shell-gpt

<span class="cmt"># Ollama ì—°ë™</span>
<span class="kw">export</span> OPENAI_API_BASE=<span class="str">"http://localhost:11434/v1"</span>
<span class="kw">export</span> OPENAI_API_KEY=<span class="str">"ollama"</span>
<span class="kw">export</span> DEFAULT_MODEL=<span class="str">"llama3.2"</span>

<span class="cmt"># ì‚¬ìš©</span>
sgpt <span class="str">"í˜„ì¬ ë””ë ‰í† ë¦¬ì˜ í° íŒŒì¼ ì°¾ëŠ” ëª…ë ¹ì–´"</span>
<span class="cmt"># find . -type f -exec du -h {} + | sort -rh | head -n 10</span>

<span class="cmt"># ì½”ë“œ ìƒì„±</span>
sgpt --code <span class="str">"Pythonìœ¼ë¡œ JSON íŒŒì¼ ì½ê¸°"</span></code></pre>

  <h3 id="fabric">Fabric (íŒ¨í„´ ê¸°ë°˜ í”„ë¡¬í”„íŠ¸)</h3>

  <pre><code><span class="cmt"># ì„¤ì¹˜</span>
pip install fabric-ai

<span class="cmt"># Ollama ì„¤ì •</span>
fabric --setup
<span class="str">API Base: http://localhost:11434</span>
<span class="str">Model: llama3.2</span>

<span class="cmt"># ì‚¬ìš© (ë‹¤ì–‘í•œ íŒ¨í„´)</span>
cat article.txt | fabric --pattern summarize
cat code.py | fabric --pattern explain_code
cat email.txt | fabric --pattern extract_insights</code></pre>

  <h3 id="anythingllm">AnythingLLM (ë¬¸ì„œ ê´€ë¦¬)</h3>

  <pre><code><span class="cmt"># Dockerë¡œ ì„¤ì¹˜</span>
docker run -d -p 3001:3001 \
  -v anythingllm:/app/server/storage \
  -e OLLAMA_BASE_PATH=<span class="str">"http://host.docker.internal:11434"</span> \
  mintplexlabs/anythingllm

<span class="cmt"># ì ‘ì†</span>
<span class="str">http://localhost:3001</span>

<span class="cmt"># ê¸°ëŠ¥</span>
- ë¬¸ì„œ ì—…ë¡œë“œ (PDF, DOCX, TXT)
- ì›Œí¬ìŠ¤í˜ì´ìŠ¤ë³„ RAG
- ì±„íŒ… íˆìŠ¤í† ë¦¬
- ì—ì´ì „íŠ¸ ëª¨ë“œ</code></pre>
</section>

<section class="content-section">
  <h2 id="comparison">ë„êµ¬ ë¹„êµ</h2>

  <table>
    <thead>
      <tr>
        <th>ë„êµ¬</th>
        <th>ìš©ë„</th>
        <th>ë‚œì´ë„</th>
        <th>ì¶”ì²œ ëŒ€ìƒ</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Continue</strong></td>
        <td>VS Code ì½”ë”©</td>
        <td>â­ ì‰¬ì›€</td>
        <td>ëª¨ë“  ê°œë°œì</td>
      </tr>
      <tr>
        <td><strong>Aider</strong></td>
        <td>CLI ì½”ë”©</td>
        <td>â­â­ ë³´í†µ</td>
        <td>í„°ë¯¸ë„ ì„ í˜¸ì</td>
      </tr>
      <tr>
        <td><strong>Open WebUI</strong></td>
        <td>ì±„íŒ… ì¸í„°í˜ì´ìŠ¤</td>
        <td>â­ ì‰¬ì›€</td>
        <td>ë¹„ê°œë°œì í¬í•¨</td>
      </tr>
      <tr>
        <td><strong>LangChain</strong></td>
        <td>Python ì•± ê°œë°œ</td>
        <td>â­â­â­ ì–´ë ¤ì›€</td>
        <td>Python ê°œë°œì</td>
      </tr>
      <tr>
        <td><strong>LlamaIndex</strong></td>
        <td>RAG ì „ë¬¸</td>
        <td>â­â­â­ ì–´ë ¤ì›€</td>
        <td>ë°ì´í„° ê³¼í•™ì</td>
      </tr>
      <tr>
        <td><strong>API</strong></td>
        <td>ì»¤ìŠ¤í…€ í†µí•©</td>
        <td>â­â­ ë³´í†µ</td>
        <td>ê³ ê¸‰ ê°œë°œì</td>
      </tr>
    </tbody>
  </table>

  <div class="info-box tip">
    <div class="info-box-title">ğŸ¯ ì¶”ì²œ ì¡°í•©</div>
    <ul>
      <li><strong>ì¼ë°˜ ê°œë°œì</strong>: Continue + Aider</li>
      <li><strong>ë°ì´í„° ê³¼í•™ì</strong>: LlamaIndex + Jupyter</li>
      <li><strong>íŒ€ í˜‘ì—…</strong>: Open WebUI + API í†µí•©</li>
      <li><strong>í”„ë¡œë•ì…˜ ì•±</strong>: LangChain + FastAPI</li>
    </ul>
  </div>
</section>

<section class="content-section">
  <h2 id="next-steps">ë‹¤ìŒ ë‹¨ê³„</h2>

  <p>ë„êµ¬ ì—°ë™ì„ ì™„ë£Œí–ˆë‹¤ë©´, ì´ì œ ê³ ê¸‰ í™œìš©ë²•ì„ ë°°ì›Œë´…ì‹œë‹¤!</p>

  <div class="info-box info">
    <div class="info-box-title">ğŸ“š ê³„ì† í•™ìŠµí•˜ê¸°</div>
    <ol>
      <li><a href="ollama-advanced.html">ê³ ê¸‰ í™œìš©</a> - Modelfile, ì»¤ìŠ¤í…€ ëª¨ë¸, í”„ë¡œë•ì…˜ ë°°í¬</li>
      <li><a href="ollama-troubleshooting.html">íŠ¸ëŸ¬ë¸”ìŠˆíŒ…</a> - ì¼ë°˜ì ì¸ ë¬¸ì œ í•´ê²°</li>
      <li><a href="ollama-models.html">ëª¨ë¸ ì„ íƒ</a> - ìš©ë„ë³„ ìµœì  ëª¨ë¸ ì°¾ê¸°</li>
    </ol>
  </div>

  <div class="info-box tip">
    <div class="info-box-title">âš¡ ì‹¤ì „ í”„ë¡œì íŠ¸ ì•„ì´ë””ì–´</div>
    <ul>
      <li><strong>ê°œì¸ ì½”ë”© ì–´ì‹œìŠ¤í„´íŠ¸</strong>: Continue + CodeLlamaë¡œ ë¬´ë£Œ Copilot</li>
      <li><strong>ë¬¸ì„œ Q&A ë´‡</strong>: LlamaIndex + íšŒì‚¬ ë¬¸ì„œ (í”„ë¼ì´ë²„ì‹œ ë³´ì¥)</li>
      <li><strong>ìë™ ì½”ë“œ ë¦¬ë·°</strong>: Aider + Git Hooksë¡œ ì»¤ë°‹ ì „ ê²€í† </li>
      <li><strong>ì‚¬ë‚´ ChatGPT</strong>: Open WebUI + íŒ€ ë°°í¬</li>
    </ul>
  </div>
</section>

<nav class="page-nav"></nav>
</main>

<aside class="inline-toc"></aside>
<footer class="site-footer"></footer>

</div>
<script src="../js/main.js"></script>
</body>
</html>
