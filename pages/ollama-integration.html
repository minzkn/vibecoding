<!DOCTYPE html>
<html lang="ko" data-theme="dark-kernel">
<head>
<script>
(function(){var m=document.cookie.match(/claude_theme=([^;]+)/);if(m)document.documentElement.setAttribute('data-theme',m[1]);})();
</script>
<meta charset="UTF-8">
<meta property="og:type" content="article">
<meta property="og:site_name" content="AI Vibe Coding ê°€ì´ë“œ /with MINZKN">
<meta property="og:title" content="Ollama ë„êµ¬ ì—°ë™ - Continue, Aider, LangChain, Open WebUI">
<meta property="og:description" content="Ollamaë¥¼ VS Code, LangChain, LlamaIndex, APIì™€ í†µí•©í•˜ì—¬ ê°•ë ¥í•œ AI ê°œë°œ í™˜ê²½ êµ¬ì¶•">
<meta property="og:url" content="https://minzkn.com/claude/pages/ollama-integration.html">
<meta property="og:image" content="https://minzkn.com/claude/images/og-image.png">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="Ollama í†µí•© ê°€ì´ë“œ. Continue.dev, Aider, LangChain, LlamaIndex, Open WebUI ì—°ë™ ë°©ë²•.">
<meta name="keywords" content="ollama ì—°ë™, continue dev, aider, langchain, llamaindex, open webui, api integration">
<meta name="author" content="MINZKN">
<title>Ollama ë„êµ¬ ì—°ë™ - AI Vibe Coding ê°€ì´ë“œ /with MINZKN</title>
<link rel="icon" type="image/svg+xml" href="../images/favicon.svg">
<link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/joungkyun/font-d2coding/d2coding.css">
<link rel="stylesheet" href="../css/themes.css">
<link rel="stylesheet" href="../css/style.css">
<link rel="stylesheet" href="../css/responsive.css">
</head>
<body>
<div class="page-wrapper">

<header class="site-header"></header>
<nav class="side-nav" aria-label="ì‚¬ì´íŠ¸ ë‚´ë¹„ê²Œì´ì…˜"></nav>

<main class="main-content">
<nav class="breadcrumb"></nav>

<h1 id="top">Ollama ë„êµ¬ ì—°ë™</h1>
<p class="lead">ë¡œì»¬ LLMì„ ê°œë°œ ë„êµ¬ì™€ í†µí•©í•˜ì—¬ ì™„ì „í•œ AI ì½”ë”© í™˜ê²½ êµ¬ì¶•í•˜ê¸°</p>

<div class="info-box tip">
  <div class="info-box-title">ğŸš€ ë¹ ë¥¸ ì‹œì‘</div>
  <p><strong>ê°€ì¥ ì¸ê¸° ìˆëŠ” í†µí•© 3ê°€ì§€:</strong></p>
  <ol>
    <li><strong>Continue.dev</strong> - VS Codeì—ì„œ GitHub Copilotì²˜ëŸ¼ ì‚¬ìš© (ë¬´ë£Œ)</li>
    <li><strong>Aider</strong> - CLI ê¸°ë°˜ AI í˜ì–´ í”„ë¡œê·¸ë˜ë°</li>
    <li><strong>Open WebUI</strong> - ChatGPT ìŠ¤íƒ€ì¼ ì›¹ ì¸í„°í˜ì´ìŠ¤</li>
  </ol>
  <p>ì´ 3ê°€ì§€ë§Œ ì„¤ì •í•´ë„ 90% ì‘ì—…ì„ ë¡œì»¬ì—ì„œ ë¬´ë£Œë¡œ ì²˜ë¦¬!</p>
</div>

<section class="content-section">
  <h2 id="continue-dev">Continue.dev + Ollama (VS Code)</h2>

  <p>
    <strong>Continue</strong>ëŠ” VS Code/JetBrainsì—ì„œ GitHub Copilotì²˜ëŸ¼ ë™ì‘í•˜ëŠ” ì˜¤í”ˆì†ŒìŠ¤ AI ì½”ë”© ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
    Ollamaì™€ ì™„ë²½í•˜ê²Œ í†µí•©ë˜ì–´ ë¬´ë£Œë¡œ ë¬´ì œí•œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
  </p>

  <h3 id="continue-install">ì„¤ì¹˜</h3>

  <pre><code><span class="cmt"># 1. VS Code í™•ì¥ ë§ˆì¼“í”Œë ˆì´ìŠ¤ì—ì„œ ì„¤ì¹˜</span>
<span class="str">1. VS Code ì—´ê¸°</span>
<span class="str">2. Extensions (Ctrl+Shift+X)</span>
<span class="str">3. "Continue" ê²€ìƒ‰</span>
<span class="str">4. Install í´ë¦­</span>

<span class="cmt"># ë˜ëŠ” CLIë¡œ ì„¤ì¹˜</span>
code --install-extension continue.continue</code></pre>

  <h3 id="continue-config">ì„¤ì •</h3>

  <p>Continue ì‚¬ì´ë“œë°”ì—ì„œ ì„¤ì • íŒŒì¼ì„ ì—´ê±°ë‚˜ <code>~/.continue/config.json</code> ì§ì ‘ í¸ì§‘:</p>

  <pre><code><span class="cmt">// ~/.continue/config.json</span>
{
  <span class="str">"models"</span>: [
    {
      <span class="str">"title"</span>: <span class="str">"CodeLlama 13B"</span>,
      <span class="str">"provider"</span>: <span class="str">"ollama"</span>,
      <span class="str">"model"</span>: <span class="str">"codellama:13b"</span>,
      <span class="str">"apiBase"</span>: <span class="str">"http://localhost:11434"</span>
    },
    {
      <span class="str">"title"</span>: <span class="str">"Llama 3.2"</span>,
      <span class="str">"provider"</span>: <span class="str">"ollama"</span>,
      <span class="str">"model"</span>: <span class="str">"llama3.2"</span>
    }
  ],
  <span class="str">"tabAutocompleteModel"</span>: {
    <span class="str">"title"</span>: <span class="str">"Starcoder2 3B"</span>,
    <span class="str">"provider"</span>: <span class="str">"ollama"</span>,
    <span class="str">"model"</span>: <span class="str">"starcoder2:3b"</span>
  },
  <span class="str">"embeddingsProvider"</span>: {
    <span class="str">"provider"</span>: <span class="str">"ollama"</span>,
    <span class="str">"model"</span>: <span class="str">"nomic-embed-text"</span>
  }
}</code></pre>

  <h3 id="continue-usage">ì‚¬ìš©ë²•</h3>

  <h4>1. ì¸ë¼ì¸ ì½”ë“œ ìƒì„± (Ctrl+I)</h4>

  <pre><code><span class="cmt"># ì½”ë“œ í¸ì§‘ê¸°ì—ì„œ Ctrl+I ëˆ„ë¥´ê³  ìš”ì²­</span>
<span class="str">"í”¼ë³´ë‚˜ì¹˜ ìˆ˜ì—´ ìƒì„± í•¨ìˆ˜ë¥¼ ì¬ê·€ë¡œ ì‘ì„±í•´ì¤˜"</span>

<span class="cmt"># Continueê°€ ìë™ìœ¼ë¡œ ì½”ë“œ ìƒì„± ë° ì‚½ì…</span>
<span class="kw">def</span> <span class="fn">fibonacci</span>(n):
    <span class="kw">if</span> n &lt;= <span class="num">1</span>:
        <span class="kw">return</span> n
    <span class="kw">return</span> fibonacci(n-<span class="num">1</span>) + fibonacci(n-<span class="num">2</span>)</code></pre>

  <h4>2. ì½”ë“œ ì„¤ëª… (Ctrl+L)</h4>

  <pre><code><span class="cmt"># ì½”ë“œ ë¸”ë¡ ì„ íƒ í›„ Ctrl+L</span>
<span class="str">"ì´ ì½”ë“œê°€ ë¬´ì—‡ì„ í•˜ëŠ”ì§€ ì„¤ëª…í•´ì¤˜"</span>

<span class="cmt"># ì‚¬ì´ë“œë°”ì— ì„¤ëª… í‘œì‹œ</span></code></pre>

  <h4>3. ë¦¬íŒ©í† ë§</h4>

  <pre><code><span class="cmt"># ì½”ë“œ ì„ íƒ í›„ Ctrl+I</span>
<span class="str">"ì´ í•¨ìˆ˜ë¥¼ íƒ€ì… íŒíŠ¸ë¥¼ ì¶”ê°€í•˜ê³  docstring ì‘ì„±í•´ì¤˜"</span>

<span class="kw">def</span> <span class="fn">fibonacci</span>(n: <span class="type">int</span>) -&gt; <span class="type">int</span>:
    <span class="str">"""
    Calculate the nth Fibonacci number recursively.

    Args:
        n: The position in the Fibonacci sequence (0-indexed)

    Returns:
        The nth Fibonacci number
    """</span>
    <span class="kw">if</span> n &lt;= <span class="num">1</span>:
        <span class="kw">return</span> n
    <span class="kw">return</span> fibonacci(n-<span class="num">1</span>) + fibonacci(n-<span class="num">2</span>)</code></pre>

  <h4>4. íƒ­ ìë™ì™„ì„± (ì‹¤í—˜ì )</h4>

  <pre><code><span class="cmt"># ì½”ë“œ ì‘ì„± ì¤‘ íƒ­ ëˆŒëŸ¬ ìë™ì™„ì„±</span>
<span class="kw">def</span> <span class="fn">sort_list</span>(items):
    <span class="cmt"># Tab ëˆ„ë¥´ë©´ ìë™ìœ¼ë¡œ ì œì•ˆ</span>
    <span class="kw">return</span> sorted(items)  <span class="cmt"># â† Continueê°€ ìƒì„±</span></code></pre>

  <h3 id="continue-tips">íŒ & íŠ¸ë¦­</h3>

  <div class="info-box tip">
    <div class="info-box-title">ğŸ’¡ Continue ìµœì í™”</div>
    <ul>
      <li><strong>ì½”ë“œ ìƒì„±</strong>: CodeLlama 13B ë˜ëŠ” Qwen2.5-Coder 14B</li>
      <li><strong>ëŒ€í™”/ì„¤ëª…</strong>: Llama 3.2 7B ë˜ëŠ” Mistral 7B</li>
      <li><strong>ìë™ì™„ì„±</strong>: Starcoder2 3B (ë¹ ë¦„, ê°€ë²¼ì›€)</li>
      <li><strong>ì„ë² ë”©</strong>: nomic-embed-text (ì½”ë“œë² ì´ìŠ¤ ê²€ìƒ‰ìš©)</li>
    </ul>
  </div>

  <pre><code><span class="cmt"># ëª¨ë¸ ë¯¸ë¦¬ ë‹¤ìš´ë¡œë“œ</span>
ollama pull codellama:13b
ollama pull llama3.2
ollama pull starcoder2:3b
ollama pull nomic-embed-text</code></pre>

  <h3 id="continue-keyboard">í‚¤ë³´ë“œ ë‹¨ì¶•í‚¤</h3>

  <table>
    <thead>
      <tr>
        <th>ë‹¨ì¶•í‚¤</th>
        <th>ê¸°ëŠ¥</th>
        <th>ì„¤ëª…</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><code>Ctrl+I</code></td>
        <td>ì¸ë¼ì¸ í¸ì§‘</td>
        <td>ì„ íƒ ì˜ì—­ì— ì½”ë“œ ìƒì„±/ìˆ˜ì •</td>
      </tr>
      <tr>
        <td><code>Ctrl+L</code></td>
        <td>ì±„íŒ… ì—´ê¸°</td>
        <td>ì‚¬ì´ë“œë°”ì—ì„œ ëŒ€í™” ì‹œì‘</td>
      </tr>
      <tr>
        <td><code>Tab</code></td>
        <td>ìë™ì™„ì„±</td>
        <td>ì œì•ˆ ìˆ˜ë½ (ì„¤ì • í•„ìš”)</td>
      </tr>
      <tr>
        <td><code>Ctrl+Shift+R</code></td>
        <td>ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€</td>
        <td>í˜„ì¬ íŒŒì¼ì„ ì»¨í…ìŠ¤íŠ¸ì— ì¶”ê°€</td>
      </tr>
    </tbody>
  </table>
</section>

<section class="content-section">
  <h2 id="aider">Aider + Ollama (CLI)</h2>

  <p>
    <strong>Aider</strong>ëŠ” í„°ë¯¸ë„ì—ì„œ ë™ì‘í•˜ëŠ” AI í˜ì–´ í”„ë¡œê·¸ë˜ë° ë„êµ¬ë¡œ, Gitê³¼ ì™„ë²½í•˜ê²Œ í†µí•©ë©ë‹ˆë‹¤.
    ì½”ë“œ ë³€ê²½ì‚¬í•­ì„ ìë™ìœ¼ë¡œ ì»¤ë°‹í•˜ê³ , ì—¬ëŸ¬ íŒŒì¼ì„ ë™ì‹œì— ìˆ˜ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
  </p>

  <h3 id="aider-install">ì„¤ì¹˜</h3>

  <pre><code><span class="cmt"># Python 3.8+ í•„ìš”</span>
pip install aider-chat

<span class="cmt"># ë˜ëŠ” pipxë¡œ ê²©ë¦¬ ì„¤ì¹˜</span>
pipx install aider-chat

<span class="cmt"># ì„¤ì¹˜ í™•ì¸</span>
aider --version
<span class="cmt"># aider version 0.x.x</span></code></pre>

  <h3 id="aider-config">Ollama ì—°ë™ ì„¤ì •</h3>

  <pre><code><span class="cmt"># ê¸°ë³¸ ì‚¬ìš© (ëª¨ë¸ ì§€ì •)</span>
aider --model ollama/codellama:13b

<span class="cmt"># í™˜ê²½ ë³€ìˆ˜ë¡œ ê¸°ë³¸ ëª¨ë¸ ì„¤ì •</span>
<span class="kw">export</span> AIDER_MODEL=ollama/llama3.2
<span class="kw">export</span> OLLAMA_API_BASE=http://localhost:11434

<span class="cmt"># ~/.aider.conf.yml ìƒì„± (ì˜êµ¬ ì„¤ì •)</span>
cat &gt; ~/.aider.conf.yml &lt;&lt;EOF
model: ollama/codellama:13b
edit-format: whole
auto-commits: true
dirty-commits: false
EOF</code></pre>

  <h3 id="aider-usage">ì‚¬ìš©ë²•</h3>

  <h4>1. í”„ë¡œì íŠ¸ì—ì„œ Aider ì‹œì‘</h4>

  <pre><code><span class="cmt"># Git ì €ì¥ì†Œ ë£¨íŠ¸ì—ì„œ ì‹¤í–‰</span>
cd ~/my-project
aider --model ollama/codellama:13b

<span class="cmt"># íŠ¹ì • íŒŒì¼ ì§€ì •</span>
aider src/main.py tests/test_main.py --model ollama/llama3.2

<span class="cmt"># ëª¨ë“  Python íŒŒì¼</span>
aider src/*.py</code></pre>

  <h4>2. ì½”ë“œ ìƒì„±</h4>

  <pre><code><span class="cmt"># Aider í”„ë¡¬í”„íŠ¸</span>
<span class="str">&gt; main.pyì— FastAPI ì•±ì„ ìƒì„±í•˜ê³ , /health ì—”ë“œí¬ì¸íŠ¸ë¥¼ ì¶”ê°€í•´ì¤˜</span>

<span class="cmt"># Aiderê°€ ìë™ìœ¼ë¡œ ì½”ë“œ ìƒì„± ë° íŒŒì¼ ì‘ì„±</span>
<span class="cmt"># main.py</span>
<span class="kw">from</span> fastapi <span class="kw">import</span> FastAPI

app = FastAPI()

<span class="pp">@app.get</span>(<span class="str">"/health"</span>)
<span class="kw">async</span> <span class="kw">def</span> <span class="fn">health_check</span>():
    <span class="kw">return</span> {<span class="str">"status"</span>: <span class="str">"healthy"</span>}

<span class="cmt"># Aiderê°€ ìë™ìœ¼ë¡œ Git ì»¤ë°‹</span>
Applied edit to main.py
Commit 3a8f7e1 Add FastAPI health endpoint</code></pre>

  <h4>3. ë²„ê·¸ ìˆ˜ì •</h4>

  <pre><code><span class="str">&gt; user.pyì˜ validate_email í•¨ìˆ˜ì— ë²„ê·¸ê°€ ìˆì–´. ì˜ëª»ëœ ì´ë©”ì¼ì„ í—ˆìš©í•˜ê³  ìˆì–´.</span>

<span class="cmt"># Aiderê°€ ì½”ë“œ ë¶„ì„ í›„ ìˆ˜ì •</span>
I found the issue. The regex pattern is missing the @ symbol check.

<span class="cmt"># ìˆ˜ì •ëœ ì½”ë“œ</span>
<span class="kw">import</span> re

<span class="kw">def</span> <span class="fn">validate_email</span>(email: <span class="type">str</span>) -&gt; <span class="type">bool</span>:
    pattern = <span class="str">r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'</span>
    <span class="kw">return</span> bool(re.match(pattern, email))

Applied edit to user.py
Commit b4c2d9f Fix email validation regex</code></pre>

  <h4>4. ë¦¬íŒ©í† ë§</h4>

  <pre><code><span class="str">&gt; database.pyì˜ ëª¨ë“  í•¨ìˆ˜ì— íƒ€ì… íŒíŠ¸ì™€ docstringì„ ì¶”ê°€í•˜ê³ , ì—ëŸ¬ í•¸ë“¤ë§ì„ ê°œì„ í•´ì¤˜</span>

<span class="cmt"># ì—¬ëŸ¬ í•¨ìˆ˜ ë™ì‹œ ë¦¬íŒ©í† ë§</span>
Applied edit to database.py
- Added type hints to 5 functions
- Added docstrings following Google style
- Added proper exception handling
Commit 7e1a5c3 Refactor database.py with type hints and docs</code></pre>

  <h3 id="aider-commands">Aider ëª…ë ¹ì–´</h3>

  <table>
    <thead>
      <tr>
        <th>ëª…ë ¹ì–´</th>
        <th>ì„¤ëª…</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><code>/add &lt;file&gt;</code></td>
        <td>íŒŒì¼ì„ ì±„íŒ…ì— ì¶”ê°€</td>
      </tr>
      <tr>
        <td><code>/drop &lt;file&gt;</code></td>
        <td>íŒŒì¼ì„ ì±„íŒ…ì—ì„œ ì œê±°</td>
      </tr>
      <tr>
        <td><code>/ls</code></td>
        <td>í˜„ì¬ ì±„íŒ…ì— í¬í•¨ëœ íŒŒì¼ ëª©ë¡</td>
      </tr>
      <tr>
        <td><code>/run &lt;cmd&gt;</code></td>
        <td>ì‰˜ ëª…ë ¹ì–´ ì‹¤í–‰</td>
      </tr>
      <tr>
        <td><code>/undo</code></td>
        <td>ë§ˆì§€ë§‰ ë³€ê²½ ì·¨ì†Œ</td>
      </tr>
      <tr>
        <td><code>/commit</code></td>
        <td>ìˆ˜ë™ ì»¤ë°‹</td>
      </tr>
      <tr>
        <td><code>/diff</code></td>
        <td>ë³€ê²½ì‚¬í•­ í™•ì¸</td>
      </tr>
      <tr>
        <td><code>/help</code></td>
        <td>ë„ì›€ë§</td>
      </tr>
      <tr>
        <td><code>/exit</code></td>
        <td>ì¢…ë£Œ</td>
      </tr>
    </tbody>
  </table>

  <h3 id="aider-advanced">ê³ ê¸‰ ì‚¬ìš©</h3>

  <h4>ì „ì²´ ì½”ë“œë² ì´ìŠ¤ ê²€ìƒ‰</h4>

  <pre><code><span class="cmt"># ì €ì¥ì†Œ ë§µ í™œì„±í™” (--map-tokens ì˜µì…˜)</span>
aider --model ollama/llama3.2 --map-tokens 2048

<span class="str">&gt; ì¸ì¦ ê´€ë ¨ ëª¨ë“  í•¨ìˆ˜ë¥¼ ì°¾ì•„ì„œ JWT í† í° ê²€ì¦ ë¡œì§ì„ ì¶”ê°€í•´ì¤˜</span>

<span class="cmt"># Aiderê°€ ì „ì²´ ì½”ë“œë² ì´ìŠ¤ ê²€ìƒ‰ í›„ ê´€ë ¨ íŒŒì¼ ìë™ ìˆ˜ì •</span></code></pre>

  <h4>í…ŒìŠ¤íŠ¸ ìë™ ìƒì„±</h4>

  <pre><code>aider src/calculator.py

<span class="str">&gt; calculator.pyì˜ ëª¨ë“  í•¨ìˆ˜ì— ëŒ€í•œ pytest í…ŒìŠ¤íŠ¸ë¥¼ tests/test_calculator.pyì— ìƒì„±í•´ì¤˜</span>

Applied edit to tests/test_calculator.py
Created 8 test functions
Commit a3d8e4f Add comprehensive tests for calculator</code></pre>

  <div class="info-box tip">
    <div class="info-box-title">ğŸ’¡ Aider ì‚¬ìš© íŒ</div>
    <ul>
      <li><strong>--watch-files</strong>: íŒŒì¼ ë³€ê²½ ê°ì§€ ìë™ ì ìš©</li>
      <li><strong>--auto-commits false</strong>: ìˆ˜ë™ ì»¤ë°‹ ì œì–´</li>
      <li><strong>--edit-format whole</strong>: ì „ì²´ íŒŒì¼ í¸ì§‘ ëª¨ë“œ (ì‘ì€ íŒŒì¼)</li>
      <li><strong>--edit-format diff</strong>: diff íŒ¨ì¹˜ ëª¨ë“œ (í° íŒŒì¼, ê¸°ë³¸ê°’)</li>
      <li><strong>--message "..."</strong>: ëŒ€í™” ì—†ì´ ë‹¨ì¼ ëª…ë ¹ ì‹¤í–‰</li>
    </ul>
  </div>
</section>

<section class="content-section">
  <h2 id="open-webui">Open WebUI (ChatGPT ìŠ¤íƒ€ì¼ ì¸í„°í˜ì´ìŠ¤)</h2>

  <p>
    <strong>Open WebUI</strong>ëŠ” Ollamaë¥¼ ìœ„í•œ ChatGPT ìŠ¤íƒ€ì¼ ì›¹ ì¸í„°í˜ì´ìŠ¤ì…ë‹ˆë‹¤.
    ë¸Œë¼ìš°ì €ì—ì„œ ëŒ€í™”, ë¬¸ì„œ ì—…ë¡œë“œ, ì´ë¯¸ì§€ ìƒì„± ë“± ëª¨ë“  ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
  </p>

  <h3 id="webui-install">ì„¤ì¹˜</h3>

  <h4>Docker ì‚¬ìš© (ê¶Œì¥)</h4>

  <pre><code><span class="cmt"># Ollamaì™€ í•¨ê»˜ ì‹¤í–‰ (CPU)</span>
docker run -d -p 3000:8080 \
  -v open-webui:/app/backend/data \
  -e OLLAMA_BASE_URL=http://host.docker.internal:11434 \
  --name open-webui \
  ghcr.io/open-webui/open-webui:main

<span class="cmt"># GPU ì§€ì› (NVIDIA)</span>
docker run -d -p 3000:8080 \
  --gpus all \
  -v open-webui:/app/backend/data \
  -v ollama:/root/.ollama \
  -e OLLAMA_BASE_URL=http://host.docker.internal:11434 \
  --name open-webui \
  ghcr.io/open-webui/open-webui:cuda

<span class="cmt"># ë¸Œë¼ìš°ì €ì—ì„œ ì ‘ì†</span>
<span class="str">http://localhost:3000</span></code></pre>

  <h4>Docker Compose (ì¶”ì²œ)</h4>

  <pre><code><span class="cmt"># docker-compose.yml</span>
<span class="kw">version</span>: <span class="str">'3.8'</span>

<span class="kw">services</span>:
  <span class="kw">ollama</span>:
    <span class="kw">image</span>: ollama/ollama:latest
    <span class="kw">container_name</span>: ollama
    <span class="kw">ports</span>:
      - <span class="str">"11434:11434"</span>
    <span class="kw">volumes</span>:
      - ollama:/root/.ollama
    <span class="kw">deploy</span>:
      <span class="kw">resources</span>:
        <span class="kw">reservations</span>:
          <span class="kw">devices</span>:
            - <span class="kw">driver</span>: nvidia
              <span class="kw">count</span>: 1
              <span class="kw">capabilities</span>: [gpu]

  <span class="kw">open-webui</span>:
    <span class="kw">image</span>: ghcr.io/open-webui/open-webui:main
    <span class="kw">container_name</span>: open-webui
    <span class="kw">ports</span>:
      - <span class="str">"3000:8080"</span>
    <span class="kw">volumes</span>:
      - open-webui:/app/backend/data
    <span class="kw">environment</span>:
      - OLLAMA_BASE_URL=http://ollama:11434
    <span class="kw">depends_on</span>:
      - ollama

<span class="kw">volumes</span>:
  <span class="kw">ollama</span>:
  <span class="kw">open-webui</span>:</code></pre>

  <pre><code><span class="cmt"># ì‹¤í–‰</span>
docker-compose up -d

<span class="cmt"># ë¡œê·¸ í™•ì¸</span>
docker-compose logs -f open-webui</code></pre>

  <h3 id="webui-features">ì£¼ìš” ê¸°ëŠ¥</h3>

  <h4>1. ëŒ€í™” ê´€ë¦¬</h4>
  <ul>
    <li>ì—¬ëŸ¬ ì±„íŒ… ì„¸ì…˜ ë™ì‹œ ê´€ë¦¬</li>
    <li>ëŒ€í™” íˆìŠ¤í† ë¦¬ ì €ì¥ ë° ê²€ìƒ‰</li>
    <li>ëŒ€í™” ë‚´ë³´ë‚´ê¸° (Markdown, JSON)</li>
    <li>í´ë”ë¡œ ì±„íŒ… êµ¬ì¡°í™”</li>
  </ul>

  <h4>2. ë¬¸ì„œ ì—…ë¡œë“œ & RAG</h4>

  <pre><code><span class="cmt"># ë¬¸ì„œ ì—…ë¡œë“œí•˜ì—¬ ì§ˆë¬¸</span>
<span class="str">1. ì±„íŒ…ì—ì„œ íŒŒì¼ ì•„ì´ì½˜ í´ë¦­</span>
<span class="str">2. PDF, TXT, DOCX, Markdown ì—…ë¡œë“œ</span>
<span class="str">3. "ì´ ë¬¸ì„œì˜ ì£¼ìš” ë‚´ìš©ì„ ìš”ì•½í•´ì¤˜" ì§ˆë¬¸</span>

<span class="cmt"># ì§€ì› í˜•ì‹</span>
- PDF (.pdf)
- Word (.docx)
- Text (.txt, .md)
- ì½”ë“œ (.py, .js, .java, etc.)</code></pre>

  <h4>3. ì´ë¯¸ì§€ ìƒì„±</h4>

  <pre><code><span class="cmt"># Stable Diffusion í†µí•© (ì„ íƒì‚¬í•­)</span>
<span class="str">1. Settings â†’ Images â†’ Enable Image Generation</span>
<span class="str">2. AUTOMATIC1111 ë˜ëŠ” ComfyUI API ì—°ê²°</span>
<span class="str">3. ì±„íŒ…ì—ì„œ "ê³ ì–‘ì´ ê·¸ë¦¼ ìƒì„±í•´ì¤˜" ìš”ì²­</span></code></pre>

  <h4>4. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿</h4>

  <pre><code><span class="cmt"># ìì£¼ ì‚¬ìš©í•˜ëŠ” í”„ë¡¬í”„íŠ¸ ì €ì¥</span>
<span class="str">Settings â†’ Prompts â†’ New Prompt</span>

<span class="cmt"># ì˜ˆì‹œ í…œí”Œë¦¿</span>
<span class="kw">Name</span>: Code Review
<span class="kw">Prompt</span>: <span class="str">"""
ë‹¹ì‹ ì€ ì‹œë‹ˆì–´ ê°œë°œìì…ë‹ˆë‹¤. ë‹¤ìŒ ì½”ë“œë¥¼ ë¦¬ë·°í•˜ê³  ê°œì„ ì ì„ ì œì•ˆí•´ì£¼ì„¸ìš”:
- ë²„ê·¸ ê°€ëŠ¥ì„±
- ì„±ëŠ¥ ìµœì í™”
- ì½”ë“œ ê°€ë…ì„±
- ë³´ì•ˆ ì´ìŠˆ

ì½”ë“œ:
{{CODE}}
"""</span></code></pre>

  <h3 id="webui-admin">ê´€ë¦¬ ê¸°ëŠ¥</h3>

  <h4>ì‚¬ìš©ì ê´€ë¦¬</h4>

  <pre><code><span class="cmt"># ì²« ë²ˆì§¸ ì‚¬ìš©ì = ê´€ë¦¬ì</span>
<span class="str">1. http://localhost:3000 ì ‘ì†</span>
<span class="str">2. ê³„ì • ìƒì„± (ì²« ì‚¬ìš©ìëŠ” ìë™ admin)</span>
<span class="str">3. Settings â†’ Admin Panel â†’ Users</span>

<span class="cmt"># ìƒˆ ì‚¬ìš©ì ì¶”ê°€</span>
- ì´ë©”ì¼, ë¹„ë°€ë²ˆí˜¸ ì„¤ì •
- ê¶Œí•œ ë¶€ì—¬ (Admin/User)
- ëª¨ë¸ ì ‘ê·¼ ì œí•œ</code></pre>

  <h4>ëª¨ë¸ ê´€ë¦¬</h4>

  <pre><code><span class="cmt"># ì›¹ UIì—ì„œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ</span>
<span class="str">Settings â†’ Models â†’ Pull a model from Ollama.com</span>
llama3.2
codellama:13b
mistral

<span class="cmt"># ëª¨ë¸ ì‚­ì œ</span>
<span class="str">Settings â†’ Models â†’ Delete icon</span></code></pre>

  <div class="info-box tip">
    <div class="info-box-title">ğŸ’¡ Open WebUI ì¶”ì²œ í”ŒëŸ¬ê·¸ì¸</div>
    <ul>
      <li><strong>Web Search</strong>: SearxNG í†µí•© (ì‹¤ì‹œê°„ ê²€ìƒ‰)</li>
      <li><strong>Wolfram Alpha</strong>: ìˆ˜í•™ ê³„ì‚°</li>
      <li><strong>Python Interpreter</strong>: ì½”ë“œ ì‹¤í–‰</li>
      <li><strong>Wikipedia</strong>: ë°±ê³¼ì‚¬ì „ ê²€ìƒ‰</li>
    </ul>
  </div>
</section>

<section class="content-section">
  <h2 id="langchain">LangChain + Ollama (Python)</h2>

  <p>
    <strong>LangChain</strong>ì€ LLM ê¸°ë°˜ ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.
    Ollamaì™€ í†µí•©í•˜ì—¬ ì²´ì¸, ì—ì´ì „íŠ¸, RAG íŒŒì´í”„ë¼ì¸ì„ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
  </p>

  <h3 id="langchain-install">ì„¤ì¹˜</h3>

  <pre><code><span class="cmt"># LangChain ë° Ollama í†µí•© ì„¤ì¹˜</span>
pip install langchain langchain-community

<span class="cmt"># ë˜ëŠ” ì „ì²´ íŒ¨í‚¤ì§€</span>
pip install langchain langchain-community langchain-ollama</code></pre>

  <h3 id="langchain-basic">ê¸°ë³¸ ì‚¬ìš©</h3>

  <h4>1. ê°„ë‹¨í•œ LLM í˜¸ì¶œ</h4>

  <pre><code><span class="kw">from</span> langchain_community.llms <span class="kw">import</span> Ollama

<span class="cmt"># Ollama LLM ì´ˆê¸°í™”</span>
llm = Ollama(model=<span class="str">"llama3.2"</span>)

<span class="cmt"># í…ìŠ¤íŠ¸ ìƒì„±</span>
response = llm.invoke(<span class="str">"íŒŒì´ì¬ìœ¼ë¡œ í€µì†ŒíŠ¸ ì•Œê³ ë¦¬ì¦˜ì„ ì‘ì„±í•´ì¤˜"</span>)
<span class="kw">print</span>(response)</code></pre>

  <h4>2. ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ</h4>

  <pre><code><span class="kw">from</span> langchain_community.llms <span class="kw">import</span> Ollama

llm = Ollama(model=<span class="str">"llama3.2"</span>)

<span class="cmt"># ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì‘ë‹µ ë°›ê¸°</span>
<span class="kw">for</span> chunk <span class="kw">in</span> llm.stream(<span class="str">"ê¸´ ì´ì•¼ê¸°ë¥¼ ë“¤ë ¤ì¤˜"</span>):
    <span class="kw">print</span>(chunk, end=<span class="str">""</span>, flush=<span class="kw">True</span>)</code></pre>

  <h4>3. ì±„íŒ… ëª¨ë¸</h4>

  <pre><code><span class="kw">from</span> langchain_community.chat_models <span class="kw">import</span> ChatOllama
<span class="kw">from</span> langchain.schema <span class="kw">import</span> HumanMessage, SystemMessage

chat = ChatOllama(model=<span class="str">"llama3.2"</span>)

messages = [
    SystemMessage(content=<span class="str">"ë‹¹ì‹ ì€ íŒŒì´ì¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."</span>),
    HumanMessage(content=<span class="str">"FastAPIë¡œ REST APIë¥¼ ë§Œë“œëŠ” ë°©ë²•ì„ ì•Œë ¤ì¤˜"</span>)
]

response = chat.invoke(messages)
<span class="kw">print</span>(response.content)</code></pre>

  <h3 id="langchain-chains">ì²´ì¸ (Chains)</h3>

  <h4>í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿</h4>

  <pre><code><span class="kw">from</span> langchain_community.llms <span class="kw">import</span> Ollama
<span class="kw">from</span> langchain.prompts <span class="kw">import</span> PromptTemplate
<span class="kw">from</span> langchain.chains <span class="kw">import</span> LLMChain

llm = Ollama(model=<span class="str">"codellama:13b"</span>)

<span class="cmt"># í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜</span>
template = <span class="str">"""ë‹¹ì‹ ì€ {language} ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ë‹¤ìŒ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”:

ì‘ì—…: {task}

ì½”ë“œ:"""</span>

prompt = PromptTemplate(
    input_variables=[<span class="str">"language"</span>, <span class="str">"task"</span>],
    template=template
)

<span class="cmt"># ì²´ì¸ ìƒì„±</span>
chain = LLMChain(llm=llm, prompt=prompt)

<span class="cmt"># ì‹¤í–‰</span>
result = chain.run(
    language=<span class="str">"Python"</span>,
    task=<span class="str">"CSV íŒŒì¼ì„ ì½ê³  Pandas DataFrameìœ¼ë¡œ ë³€í™˜"</span>
)
<span class="kw">print</span>(result)</code></pre>

  <h4>ìˆœì°¨ ì²´ì¸</h4>

  <pre><code><span class="kw">from</span> langchain.chains <span class="kw">import</span> SimpleSequentialChain, LLMChain
<span class="kw">from</span> langchain.prompts <span class="kw">import</span> PromptTemplate
<span class="kw">from</span> langchain_community.llms <span class="kw">import</span> Ollama

llm = Ollama(model=<span class="str">"llama3.2"</span>)

<span class="cmt"># ì²´ì¸ 1: ì•„ì´ë””ì–´ ìƒì„±</span>
template1 = <span class="str">"ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ ì•„ì´ë””ì–´ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ ì œì•ˆí•´ì¤˜: {topic}"</span>
prompt1 = PromptTemplate(input_variables=[<span class="str">"topic"</span>], template=template1)
chain1 = LLMChain(llm=llm, prompt=prompt1)

<span class="cmt"># ì²´ì¸ 2: ê¸°ìˆ  ìŠ¤íƒ ì œì•ˆ</span>
template2 = <span class="str">"ì´ ì•„ì´ë””ì–´ì— ì í•©í•œ ê¸°ìˆ  ìŠ¤íƒì„ ì¶”ì²œí•´ì¤˜: {idea}"</span>
prompt2 = PromptTemplate(input_variables=[<span class="str">"idea"</span>], template=template2)
chain2 = LLMChain(llm=llm, prompt=prompt2)

<span class="cmt"># ìˆœì°¨ ì‹¤í–‰</span>
overall_chain = SimpleSequentialChain(chains=[chain1, chain2])
result = overall_chain.run(<span class="str">"AI ê¸°ë°˜ í•™ìŠµ í”Œë«í¼"</span>)
<span class="kw">print</span>(result)</code></pre>

  <h3 id="langchain-rag">RAG (Retrieval-Augmented Generation)</h3>

  <pre><code><span class="kw">from</span> langchain_community.llms <span class="kw">import</span> Ollama
<span class="kw">from</span> langchain_community.embeddings <span class="kw">import</span> OllamaEmbeddings
<span class="kw">from</span> langchain_community.vectorstores <span class="kw">import</span> Chroma
<span class="kw">from</span> langchain.text_splitter <span class="kw">import</span> RecursiveCharacterTextSplitter
<span class="kw">from</span> langchain.chains <span class="kw">import</span> RetrievalQA
<span class="kw">from</span> langchain_community.document_loaders <span class="kw">import</span> TextLoader

<span class="cmt"># 1. ë¬¸ì„œ ë¡œë“œ</span>
loader = TextLoader(<span class="str">"company_docs.txt"</span>)
documents = loader.load()

<span class="cmt"># 2. í…ìŠ¤íŠ¸ ë¶„í• </span>
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=<span class="num">1000</span>,
    chunk_overlap=<span class="num">200</span>
)
texts = text_splitter.split_documents(documents)

<span class="cmt"># 3. ì„ë² ë”© ìƒì„± (Ollama)</span>
embeddings = OllamaEmbeddings(model=<span class="str">"nomic-embed-text"</span>)

<span class="cmt"># 4. ë²¡í„° DB ìƒì„±</span>
vectorstore = Chroma.from_documents(
    documents=texts,
    embedding=embeddings,
    persist_directory=<span class="str">"./chroma_db"</span>
)

<span class="cmt"># 5. RAG ì²´ì¸ ìƒì„±</span>
llm = Ollama(model=<span class="str">"llama3.2"</span>)
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type=<span class="str">"stuff"</span>,
    retriever=vectorstore.as_retriever(search_kwargs={<span class="str">"k"</span>: <span class="num">3</span>})
)

<span class="cmt"># 6. ì§ˆë¬¸</span>
query = <span class="str">"íšŒì‚¬ì˜ íœ´ê°€ ì •ì±…ì€ ë¬´ì—‡ì¸ê°€ìš”?"</span>
result = qa_chain.invoke({<span class="str">"query"</span>: query})
<span class="kw">print</span>(result[<span class="str">"result"</span>])</code></pre>

  <h3 id="langchain-agents">ì—ì´ì „íŠ¸</h3>

  <pre><code><span class="kw">from</span> langchain.agents <span class="kw">import</span> initialize_agent, Tool
<span class="kw">from</span> langchain.agents <span class="kw">import</span> AgentType
<span class="kw">from</span> langchain_community.llms <span class="kw">import</span> Ollama
<span class="kw">import</span> requests

<span class="cmt"># ë„êµ¬ ì •ì˜</span>
<span class="kw">def</span> <span class="fn">search_wikipedia</span>(query: <span class="type">str</span>) -&gt; <span class="type">str</span>:
    url = <span class="str">f"https://en.wikipedia.org/api/rest_v1/page/summary/{query}"</span>
    response = requests.get(url)
    <span class="kw">if</span> response.status_code == <span class="num">200</span>:
        <span class="kw">return</span> response.json().get(<span class="str">"extract"</span>, <span class="str">"No results"</span>)
    <span class="kw">return</span> <span class="str">"Error"</span>

<span class="kw">def</span> <span class="fn">calculate</span>(expression: <span class="type">str</span>) -&gt; <span class="type">str</span>:
    <span class="kw">try</span>:
        <span class="kw">return</span> str(eval(expression))
    <span class="kw">except</span>:
        <span class="kw">return</span> <span class="str">"Invalid expression"</span>

tools = [
    Tool(
        name=<span class="str">"Wikipedia"</span>,
        func=search_wikipedia,
        description=<span class="str">"ìœ„í‚¤í”¼ë””ì•„ì—ì„œ ì •ë³´ ê²€ìƒ‰"</span>
    ),
    Tool(
        name=<span class="str">"Calculator"</span>,
        func=calculate,
        description=<span class="str">"ìˆ˜í•™ ê³„ì‚° ìˆ˜í–‰"</span>
    )
]

<span class="cmt"># ì—ì´ì „íŠ¸ ì´ˆê¸°í™”</span>
llm = Ollama(model=<span class="str">"llama3.2"</span>, temperature=<span class="num">0</span>)
agent = initialize_agent(
    tools,
    llm,
    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
    verbose=<span class="kw">True</span>
)

<span class="cmt"># ì‹¤í–‰</span>
result = agent.run(<span class="str">"íŒŒì´ì¬ì´ ì–¸ì œ ë§Œë“¤ì–´ì¡ŒëŠ”ì§€ ì°¾ê³ , í˜„ì¬ ì—°ë„ì—ì„œ ë¹¼ì„œ ë‚˜ì´ë¥¼ ê³„ì‚°í•´ì¤˜"</span>)
<span class="kw">print</span>(result)</code></pre>
</section>

<section class="content-section">
  <h2 id="llamaindex">LlamaIndex + Ollama (RAG)</h2>

  <p>
    <strong>LlamaIndex</strong>ëŠ” RAG(ê²€ìƒ‰ ì¦ê°• ìƒì„±) ì „ë¬¸ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.
    ëŒ€ìš©ëŸ‰ ë¬¸ì„œ, ë°ì´í„°ë² ì´ìŠ¤, APIì™€ LLMì„ ì—°ê²°í•˜ì—¬ ì§€ëŠ¥í˜• ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•©ë‹ˆë‹¤.
  </p>

  <h3 id="llamaindex-install">ì„¤ì¹˜</h3>

  <pre><code><span class="cmt"># LlamaIndex ë° Ollama í†µí•©</span>
pip install llama-index llama-index-llms-ollama llama-index-embeddings-ollama</code></pre>

  <h3 id="llamaindex-basic">ê¸°ë³¸ RAG íŒŒì´í”„ë¼ì¸</h3>

  <pre><code><span class="kw">from</span> llama_index.core <span class="kw">import</span> VectorStoreIndex, SimpleDirectoryReader, Settings
<span class="kw">from</span> llama_index.llms.ollama <span class="kw">import</span> Ollama
<span class="kw">from</span> llama_index.embeddings.ollama <span class="kw">import</span> OllamaEmbedding

<span class="cmt"># 1. LLM ë° ì„ë² ë”© ì„¤ì •</span>
Settings.llm = Ollama(model=<span class="str">"llama3.2"</span>, request_timeout=<span class="num">120.0</span>)
Settings.embed_model = OllamaEmbedding(model_name=<span class="str">"nomic-embed-text"</span>)

<span class="cmt"># 2. ë¬¸ì„œ ë¡œë“œ (ë””ë ‰í† ë¦¬ ë‚´ ëª¨ë“  íŒŒì¼)</span>
documents = SimpleDirectoryReader(<span class="str">"./docs"</span>).load_data()

<span class="cmt"># 3. ì¸ë±ìŠ¤ ìƒì„±</span>
index = VectorStoreIndex.from_documents(documents)

<span class="cmt"># 4. ì¿¼ë¦¬ ì—”ì§„ ìƒì„±</span>
query_engine = index.as_query_engine()

<span class="cmt"># 5. ì§ˆë¬¸</span>
response = query_engine.query(<span class="str">"í”„ë¡œì íŠ¸ì˜ ì£¼ìš” ê¸°ëŠ¥ì€ ë¬´ì—‡ì¸ê°€ìš”?"</span>)
<span class="kw">print</span>(response)</code></pre>

  <h3 id="llamaindex-advanced">ê³ ê¸‰ ê¸°ëŠ¥</h3>

  <h4>1. ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸</h4>

  <pre><code><span class="kw">from</span> llama_index.core <span class="kw">import</span> PromptTemplate

<span class="cmt"># ì»¤ìŠ¤í…€ QA í”„ë¡¬í”„íŠ¸</span>
qa_template = PromptTemplate(
    <span class="str">"""ë‹¹ì‹ ì€ ê¸°ìˆ  ë¬¸ì„œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

Context information:
---------------------
{context_str}
---------------------

Question: {query_str}

ìœ„ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ìƒì„¸í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.
ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ì—†ë‹¤ë©´ "ì •ë³´ ì—†ìŒ"ì´ë¼ê³  ë‹µí•˜ì„¸ìš”."""</span>
)

query_engine = index.as_query_engine(
    text_qa_template=qa_template
)

response = query_engine.query(<span class="str">"API ì¸ì¦ ë°©ë²•ì€?"</span>)
<span class="kw">print</span>(response)</code></pre>

  <h4>2. ì±„íŒ… ì—”ì§„</h4>

  <pre><code><span class="cmt"># ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ìœ ì§€í•˜ëŠ” ì±„íŒ… ì—”ì§„</span>
chat_engine = index.as_chat_engine()

<span class="cmt"># ëŒ€í™”</span>
response1 = chat_engine.chat(<span class="str">"FastAPIê°€ ë­ì•¼?"</span>)
<span class="kw">print</span>(response1)

<span class="cmt"># ì´ì „ ëŒ€í™”ë¥¼ ê¸°ì–µ</span>
response2 = chat_engine.chat(<span class="str">"ê·¸ëŸ¼ Flaskì™€ ë¹„êµí•˜ë©´?"</span>)
<span class="kw">print</span>(response2)</code></pre>

  <h4>3. ë‹¤ì¤‘ ë¬¸ì„œ ì†ŒìŠ¤</h4>

  <pre><code><span class="kw">from</span> llama_index.core <span class="kw">import</span> SimpleDirectoryReader
<span class="kw">from</span> llama_index.readers.web <span class="kw">import</span> SimpleWebPageReader

<span class="cmt"># ë¡œì»¬ íŒŒì¼</span>
local_docs = SimpleDirectoryReader(<span class="str">"./docs"</span>).load_data()

<span class="cmt"># ì›¹ í˜ì´ì§€</span>
web_docs = SimpleWebPageReader().load_data([
    <span class="str">"https://docs.python.org/3/tutorial/"</span>
])

<span class="cmt"># í•©ì³ì„œ ì¸ë±ìŠ¤ ìƒì„±</span>
all_docs = local_docs + web_docs
index = VectorStoreIndex.from_documents(all_docs)</code></pre>

  <h4>4. ì§€ì†ì  ì €ì¥</h4>

  <pre><code><span class="kw">from</span> llama_index.core <span class="kw">import</span> StorageContext, load_index_from_storage

<span class="cmt"># ì¸ë±ìŠ¤ ì €ì¥</span>
index.storage_context.persist(persist_dir=<span class="str">"./storage"</span>)

<span class="cmt"># ë‚˜ì¤‘ì— ë¡œë“œ</span>
storage_context = StorageContext.from_defaults(persist_dir=<span class="str">"./storage"</span>)
loaded_index = load_index_from_storage(storage_context)</code></pre>
</section>

<section class="content-section">
  <h2 id="api-integration">API í†µí•© (Python, JavaScript)</h2>

  <h3 id="python-api">Python API í´ë¼ì´ì–¸íŠ¸</h3>

  <h4>requests ë¼ì´ë¸ŒëŸ¬ë¦¬</h4>

  <pre><code><span class="kw">import</span> requests
<span class="kw">import</span> json

<span class="cmt"># ê¸°ë³¸ ìƒì„± ìš”ì²­</span>
<span class="kw">def</span> <span class="fn">generate</span>(prompt: <span class="type">str</span>, model: <span class="type">str</span> = <span class="str">"llama3.2"</span>) -&gt; <span class="type">str</span>:
    url = <span class="str">"http://localhost:11434/api/generate"</span>
    payload = {
        <span class="str">"model"</span>: model,
        <span class="str">"prompt"</span>: prompt,
        <span class="str">"stream"</span>: <span class="kw">False</span>
    }

    response = requests.post(url, json=payload)
    <span class="kw">return</span> response.json()[<span class="str">"response"</span>]

<span class="cmt"># ì‚¬ìš©</span>
result = generate(<span class="str">"Pythonìœ¼ë¡œ Hello World ì‘ì„±"</span>)
<span class="kw">print</span>(result)</code></pre>

  <h4>ìŠ¤íŠ¸ë¦¬ë°</h4>

  <pre><code><span class="kw">import</span> requests

<span class="kw">def</span> <span class="fn">generate_stream</span>(prompt: <span class="type">str</span>):
    url = <span class="str">"http://localhost:11434/api/generate"</span>
    payload = {
        <span class="str">"model"</span>: <span class="str">"llama3.2"</span>,
        <span class="str">"prompt"</span>: prompt,
        <span class="str">"stream"</span>: <span class="kw">True</span>
    }

    <span class="kw">with</span> requests.post(url, json=payload, stream=<span class="kw">True</span>) <span class="kw">as</span> response:
        <span class="kw">for</span> line <span class="kw">in</span> response.iter_lines():
            <span class="kw">if</span> line:
                data = json.loads(line)
                <span class="kw">if</span> <span class="str">"response"</span> <span class="kw">in</span> data:
                    <span class="kw">print</span>(data[<span class="str">"response"</span>], end=<span class="str">""</span>, flush=<span class="kw">True</span>)

generate_stream(<span class="str">"ê¸´ ì´ì•¼ê¸°ë¥¼ ë“¤ë ¤ì¤˜"</span>)</code></pre>

  <h4>ì±„íŒ… API</h4>

  <pre><code><span class="kw">import</span> requests

<span class="kw">def</span> <span class="fn">chat</span>(messages: <span class="type">list</span>) -&gt; <span class="type">str</span>:
    url = <span class="str">"http://localhost:11434/api/chat"</span>
    payload = {
        <span class="str">"model"</span>: <span class="str">"llama3.2"</span>,
        <span class="str">"messages"</span>: messages,
        <span class="str">"stream"</span>: <span class="kw">False</span>
    }

    response = requests.post(url, json=payload)
    <span class="kw">return</span> response.json()[<span class="str">"message"</span>][<span class="str">"content"</span>]

<span class="cmt"># ëŒ€í™”</span>
messages = [
    {<span class="str">"role"</span>: <span class="str">"system"</span>, <span class="str">"content"</span>: <span class="str">"ë‹¹ì‹ ì€ íŒŒì´ì¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."</span>},
    {<span class="str">"role"</span>: <span class="str">"user"</span>, <span class="str">"content"</span>: <span class="str">"FastAPIë¡œ REST API ë§Œë“œëŠ” ë°©ë²•ì€?"</span>}
]

response = chat(messages)
<span class="kw">print</span>(response)</code></pre>

  <h3 id="javascript-api">JavaScript/Node.js API í´ë¼ì´ì–¸íŠ¸</h3>

  <h4>Fetch API (ë¸Œë¼ìš°ì €)</h4>

  <pre><code><span class="cmt">// ê¸°ë³¸ ìƒì„±</span>
<span class="kw">async</span> <span class="kw">function</span> <span class="fn">generate</span>(prompt) {
  <span class="kw">const</span> response = <span class="kw">await</span> fetch(<span class="str">'http://localhost:11434/api/generate'</span>, {
    method: <span class="str">'POST'</span>,
    headers: { <span class="str">'Content-Type'</span>: <span class="str">'application/json'</span> },
    body: JSON.stringify({
      model: <span class="str">'llama3.2'</span>,
      prompt: prompt,
      stream: <span class="kw">false</span>
    })
  });

  <span class="kw">const</span> data = <span class="kw">await</span> response.json();
  <span class="kw">return</span> data.response;
}

<span class="cmt">// ì‚¬ìš©</span>
generate(<span class="str">'JavaScriptë¡œ Hello World'</span>).then(<span class="fn">console.log</span>);</code></pre>

  <h4>ìŠ¤íŠ¸ë¦¬ë° (Node.js)</h4>

  <pre><code><span class="kw">const</span> https = require(<span class="str">'https'</span>);

<span class="kw">function</span> <span class="fn">generateStream</span>(prompt) {
  <span class="kw">const</span> data = JSON.stringify({
    model: <span class="str">'llama3.2'</span>,
    prompt: prompt,
    stream: <span class="kw">true</span>
  });

  <span class="kw">const</span> options = {
    hostname: <span class="str">'localhost'</span>,
    port: <span class="num">11434</span>,
    path: <span class="str">'/api/generate'</span>,
    method: <span class="str">'POST'</span>,
    headers: {
      <span class="str">'Content-Type'</span>: <span class="str">'application/json'</span>,
      <span class="str">'Content-Length'</span>: data.length
    }
  };

  <span class="kw">const</span> req = http.request(options, (res) =&gt; {
    res.on(<span class="str">'data'</span>, (chunk) =&gt; {
      <span class="kw">const</span> line = chunk.toString();
      <span class="kw">const</span> parsed = JSON.parse(line);
      process.stdout.write(parsed.response);
    });
  });

  req.write(data);
  req.end();
}

generateStream(<span class="str">'ê¸´ ì´ì•¼ê¸°ë¥¼ ë“¤ë ¤ì¤˜'</span>);</code></pre>

  <h4>ollama-js ë¼ì´ë¸ŒëŸ¬ë¦¬</h4>

  <pre><code><span class="cmt">// ì„¤ì¹˜</span>
<span class="cmt">// npm install ollama</span>

<span class="kw">import</span> { Ollama } <span class="kw">from</span> <span class="str">'ollama'</span>;

<span class="kw">const</span> ollama = <span class="kw">new</span> Ollama({ host: <span class="str">'http://localhost:11434'</span> });

<span class="cmt">// ìƒì„±</span>
<span class="kw">const</span> response = <span class="kw">await</span> ollama.generate({
  model: <span class="str">'llama3.2'</span>,
  prompt: <span class="str">'TypeScriptë¡œ íƒ€ì… ì •ì˜í•˜ëŠ” ë°©ë²•'</span>
});

console.log(response.response);

<span class="cmt">// ì±„íŒ…</span>
<span class="kw">const</span> chatResponse = <span class="kw">await</span> ollama.chat({
  model: <span class="str">'llama3.2'</span>,
  messages: [
    { role: <span class="str">'user'</span>, content: <span class="str">'React Hook ì„¤ëª…í•´ì¤˜'</span> }
  ]
});

console.log(chatResponse.message.content);</code></pre>
</section>

<section class="content-section">
  <h2 id="other-tools">ê¸°íƒ€ ìœ ìš©í•œ ë„êµ¬</h2>

  <h3 id="twinny">Twinny (VS Code í™•ì¥)</h3>

  <pre><code><span class="cmt"># Continueì˜ ëŒ€ì•ˆ, ë” ê°€ë²¼ì›€</span>
<span class="str">1. VS Code Extensionsì—ì„œ "Twinny" ì„¤ì¹˜</span>
<span class="str">2. Settings â†’ Ollama API URL: http://localhost:11434</span>
<span class="str">3. ëª¨ë¸ ì„ íƒ: codellama:7b</span>

<span class="cmt"># íŠ¹ì§•</span>
- ë¹ ë¥¸ ìë™ì™„ì„±
- ë‚®ì€ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©
- ê°„ë‹¨í•œ UI</code></pre>

  <h3 id="shell-gpt">ShellGPT (í„°ë¯¸ë„ AI)</h3>

  <pre><code><span class="cmt"># ì„¤ì¹˜</span>
pip install shell-gpt

<span class="cmt"># Ollama ì—°ë™</span>
<span class="kw">export</span> OPENAI_API_BASE=<span class="str">"http://localhost:11434/v1"</span>
<span class="kw">export</span> OPENAI_API_KEY=<span class="str">"ollama"</span>
<span class="kw">export</span> DEFAULT_MODEL=<span class="str">"llama3.2"</span>

<span class="cmt"># ì‚¬ìš©</span>
sgpt <span class="str">"í˜„ì¬ ë””ë ‰í† ë¦¬ì˜ í° íŒŒì¼ ì°¾ëŠ” ëª…ë ¹ì–´"</span>
<span class="cmt"># find . -type f -exec du -h {} + | sort -rh | head -n 10</span>

<span class="cmt"># ì½”ë“œ ìƒì„±</span>
sgpt --code <span class="str">"Pythonìœ¼ë¡œ JSON íŒŒì¼ ì½ê¸°"</span></code></pre>

  <h3 id="fabric">Fabric (íŒ¨í„´ ê¸°ë°˜ í”„ë¡¬í”„íŠ¸)</h3>

  <pre><code><span class="cmt"># ì„¤ì¹˜</span>
pip install fabric-ai

<span class="cmt"># Ollama ì„¤ì •</span>
fabric --setup
<span class="str">API Base: http://localhost:11434</span>
<span class="str">Model: llama3.2</span>

<span class="cmt"># ì‚¬ìš© (ë‹¤ì–‘í•œ íŒ¨í„´)</span>
cat article.txt | fabric --pattern summarize
cat code.py | fabric --pattern explain_code
cat email.txt | fabric --pattern extract_insights</code></pre>

  <h3 id="anythingllm">AnythingLLM (ë¬¸ì„œ ê´€ë¦¬)</h3>

  <pre><code><span class="cmt"># Dockerë¡œ ì„¤ì¹˜</span>
docker run -d -p 3001:3001 \
  -v anythingllm:/app/server/storage \
  -e OLLAMA_BASE_PATH=<span class="str">"http://host.docker.internal:11434"</span> \
  mintplexlabs/anythingllm

<span class="cmt"># ì ‘ì†</span>
<span class="str">http://localhost:3001</span>

<span class="cmt"># ê¸°ëŠ¥</span>
- ë¬¸ì„œ ì—…ë¡œë“œ (PDF, DOCX, TXT)
- ì›Œí¬ìŠ¤í˜ì´ìŠ¤ë³„ RAG
- ì±„íŒ… íˆìŠ¤í† ë¦¬
- ì—ì´ì „íŠ¸ ëª¨ë“œ</code></pre>
</section>

<section class="content-section">
  <h2 id="comparison">ë„êµ¬ ë¹„êµ</h2>

  <table>
    <thead>
      <tr>
        <th>ë„êµ¬</th>
        <th>ìš©ë„</th>
        <th>ë‚œì´ë„</th>
        <th>ì¶”ì²œ ëŒ€ìƒ</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Continue</strong></td>
        <td>VS Code ì½”ë”©</td>
        <td>â­ ì‰¬ì›€</td>
        <td>ëª¨ë“  ê°œë°œì</td>
      </tr>
      <tr>
        <td><strong>Aider</strong></td>
        <td>CLI ì½”ë”©</td>
        <td>â­â­ ë³´í†µ</td>
        <td>í„°ë¯¸ë„ ì„ í˜¸ì</td>
      </tr>
      <tr>
        <td><strong>Open WebUI</strong></td>
        <td>ì±„íŒ… ì¸í„°í˜ì´ìŠ¤</td>
        <td>â­ ì‰¬ì›€</td>
        <td>ë¹„ê°œë°œì í¬í•¨</td>
      </tr>
      <tr>
        <td><strong>LangChain</strong></td>
        <td>Python ì•± ê°œë°œ</td>
        <td>â­â­â­ ì–´ë ¤ì›€</td>
        <td>Python ê°œë°œì</td>
      </tr>
      <tr>
        <td><strong>LlamaIndex</strong></td>
        <td>RAG ì „ë¬¸</td>
        <td>â­â­â­ ì–´ë ¤ì›€</td>
        <td>ë°ì´í„° ê³¼í•™ì</td>
      </tr>
      <tr>
        <td><strong>API</strong></td>
        <td>ì»¤ìŠ¤í…€ í†µí•©</td>
        <td>â­â­ ë³´í†µ</td>
        <td>ê³ ê¸‰ ê°œë°œì</td>
      </tr>
    </tbody>
  </table>

  <div class="info-box tip">
    <div class="info-box-title">ğŸ¯ ì¶”ì²œ ì¡°í•©</div>
    <ul>
      <li><strong>ì¼ë°˜ ê°œë°œì</strong>: Continue + Aider</li>
      <li><strong>ë°ì´í„° ê³¼í•™ì</strong>: LlamaIndex + Jupyter</li>
      <li><strong>íŒ€ í˜‘ì—…</strong>: Open WebUI + API í†µí•©</li>
      <li><strong>í”„ë¡œë•ì…˜ ì•±</strong>: LangChain + FastAPI</li>
    </ul>
  </div>
</section>

<section class="content-section">
  <h2 id="next-steps">ë‹¤ìŒ ë‹¨ê³„</h2>

  <p>ë„êµ¬ ì—°ë™ì„ ì™„ë£Œí–ˆë‹¤ë©´, ì´ì œ ê³ ê¸‰ í™œìš©ë²•ì„ ë°°ì›Œë´…ì‹œë‹¤!</p>

  <div class="info-box info">
    <div class="info-box-title">ğŸ“š ê³„ì† í•™ìŠµí•˜ê¸°</div>
    <ol>
      <li><a href="ollama-advanced.html">ê³ ê¸‰ í™œìš©</a> - Modelfile, ì»¤ìŠ¤í…€ ëª¨ë¸, í”„ë¡œë•ì…˜ ë°°í¬</li>
      <li><a href="ollama-troubleshooting.html">íŠ¸ëŸ¬ë¸”ìŠˆíŒ…</a> - ì¼ë°˜ì ì¸ ë¬¸ì œ í•´ê²°</li>
      <li><a href="ollama-models.html">ëª¨ë¸ ì„ íƒ</a> - ìš©ë„ë³„ ìµœì  ëª¨ë¸ ì°¾ê¸°</li>
    </ol>
  </div>

  <div class="info-box tip">
    <div class="info-box-title">âš¡ ì‹¤ì „ í”„ë¡œì íŠ¸ ì•„ì´ë””ì–´</div>
    <ul>
      <li><strong>ê°œì¸ ì½”ë”© ì–´ì‹œìŠ¤í„´íŠ¸</strong>: Continue + CodeLlamaë¡œ ë¬´ë£Œ Copilot</li>
      <li><strong>ë¬¸ì„œ Q&A ë´‡</strong>: LlamaIndex + íšŒì‚¬ ë¬¸ì„œ (í”„ë¼ì´ë²„ì‹œ ë³´ì¥)</li>
      <li><strong>ìë™ ì½”ë“œ ë¦¬ë·°</strong>: Aider + Git Hooksë¡œ ì»¤ë°‹ ì „ ê²€í† </li>
      <li><strong>ì‚¬ë‚´ ChatGPT</strong>: Open WebUI + íŒ€ ë°°í¬</li>
    </ul>
  </div>
</section>

<nav class="page-nav"></nav>
</main>

<aside class="inline-toc"></aside>
<footer class="site-footer"></footer>

</div>
<script src="../js/main.js"></script>
</body>
</html>
