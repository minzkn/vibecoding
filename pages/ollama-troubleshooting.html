<!DOCTYPE html>
<html lang="ko" data-theme="dark-kernel">
<head>
<script>
(function(){var m=document.cookie.match(/claude_theme=([^;]+)/);if(m)document.documentElement.setAttribute('data-theme',m[1]);})();
</script>
<meta charset="UTF-8">
<meta property="og:type" content="article">
<meta property="og:site_name" content="AI Vibe Coding ê°€ì´ë“œ /with MINZKN">
<meta property="og:title" content="Ollama íŠ¸ëŸ¬ë¸”ìŠˆíŒ…">
<meta property="og:description" content="Ollama íŠ¸ëŸ¬ë¸”ìŠˆíŒ…: ì„¤ì¹˜ ë¬¸ì œ, ì‹¤í–‰ ì˜¤ë¥˜, GPU ì¸ì‹, ì„±ëŠ¥ ì €í•˜ ë“± ëª¨ë“  ë¬¸ì œ í•´ê²° ê°€ì´ë“œ">
<meta property="og:url" content="https://minzkn.com/claude/pages/ollama-troubleshooting.html">
<meta property="og:image" content="https://minzkn.com/claude/images/og-image.png">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="Ollama íŠ¸ëŸ¬ë¸”ìŠˆíŒ…: ì„¤ì¹˜ ë¬¸ì œ, ì‹¤í–‰ ì˜¤ë¥˜, GPU ì¸ì‹, ì„±ëŠ¥ ì €í•˜ ë“± ëª¨ë“  ë¬¸ì œ í•´ê²° ê°€ì´ë“œ">
<meta name="keywords" content="Claude, AI, LLM, Ollama íŠ¸ëŸ¬ë¸”ìŠˆíŒ…, ì„¤ì¹˜ ë¬¸ì œ, ì—°ê²° ë¬¸ì œ, GPU ë¬¸ì œ, ì„±ëŠ¥ ë¬¸ì œ">
<meta name="author" content="MINZKN">
<title>Ollama íŠ¸ëŸ¬ë¸”ìŠˆíŒ… - AI Vibe Coding ê°€ì´ë“œ /with MINZKN</title>
<link rel="icon" type="image/svg+xml" href="../images/favicon.svg">
<link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/joungkyun/font-d2coding/d2coding.css">
<link rel="stylesheet" href="../css/themes.css">
<link rel="stylesheet" href="../css/style.css">
<link rel="stylesheet" href="../css/responsive.css">
</head>
<body>
<div class="page-wrapper">

<header class="site-header"></header>
<nav class="side-nav" aria-label="ì‚¬ì´íŠ¸ ë‚´ë¹„ê²Œì´ì…˜"></nav>

<main class="main-content">
<nav class="breadcrumb"></nav>

<h1 id="top">Ollama íŠ¸ëŸ¬ë¸”ìŠˆíŒ…</h1>
<p class="lead">ì„¤ì¹˜ ë¬¸ì œ, ì‹¤í–‰ ì˜¤ë¥˜, GPU ì¸ì‹, ì„±ëŠ¥ ì €í•˜ ë“± ëª¨ë“  ë¬¸ì œ í•´ê²° ê°€ì´ë“œ</p>

<div class="info-box warning">
  <strong>ì—…ë°ì´íŠ¸ ì•ˆë‚´:</strong> ëª¨ë¸/ìš”ê¸ˆ/ë²„ì „/ì •ì±… ë“± ì‹œì ì— ë¯¼ê°í•œ ì •ë³´ëŠ” ë³€ë™ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
  ìµœì‹  ë‚´ìš©ì€ ê³µì‹ ë¬¸ì„œë¥¼ í™•ì¸í•˜ì„¸ìš”.
</div>

<div class="info-box tip">
  <div class="info-box-title">ğŸ” ë¹ ë¥¸ ì§„ë‹¨</div>
  <p><strong>ì¦ìƒë³„ ë°”ë¡œê°€ê¸°:</strong></p>
  <ul>
    <li><a href="#install-errors">ì„¤ì¹˜ ì‹¤íŒ¨</a> - "command not found", ê¶Œí•œ ì˜¤ë¥˜</li>
    <li><a href="#connection-errors">ì—°ê²° ì‹¤íŒ¨</a> - "connection refused", í¬íŠ¸ ë¬¸ì œ</li>
    <li><a href="#gpu-issues">GPU ë¯¸ê°ì§€</a> - CPUë§Œ ì‚¬ìš©, CUDA ì˜¤ë¥˜</li>
    <li><a href="#performance-issues">ì„±ëŠ¥ ì €í•˜</a> - ëŠë¦° ì‘ë‹µ, ë©”ëª¨ë¦¬ ë¶€ì¡±</li>
    <li><a href="#model-errors">ëª¨ë¸ ì˜¤ë¥˜</a> - ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨, ì‹¤í–‰ ë¶ˆê°€</li>
    <li><a href="#common-errors">ì¼ë°˜ ì—ëŸ¬</a> - 20ê°€ì§€ í”í•œ ì˜¤ë¥˜ ë©”ì‹œì§€</li>
  </ul>
</div>

<section class="content-section">
  <h2 id="install-errors">ì„¤ì¹˜ ë¬¸ì œ</h2>

  <h3 id="install-command-not-found">1. "ollama: command not found"</h3>

  <div class="info-box warning">
    <div class="info-box-title">ì¦ìƒ</div>
    <pre><code>$ ollama --version
bash: ollama: command not found</code></pre>
  </div>

  <h4>ì›ì¸</h4>
  <ul>
    <li>Ollamaê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ</li>
    <li>PATHì— Ollama ê²½ë¡œê°€ ì—†ìŒ</li>
    <li>ì„¤ì¹˜ê°€ ì¤‘ê°„ì— ì‹¤íŒ¨í•¨</li>
  </ul>

  <h4>í•´ê²° ë°©ë²•</h4>

  <pre><code><span class="cmt"># 1. Ollama ì„¤ì¹˜ í™•ì¸</span>
which ollama
<span class="cmt"># ì¶œë ¥ ì—†ìŒ â†’ ë¯¸ì„¤ì¹˜</span>

<span class="cmt"># 2. ì¬ì„¤ì¹˜ (Linux/macOS)</span>
curl -fsSL https://ollama.com/install.sh | sh

<span class="cmt"># 3. ìˆ˜ë™ PATH ì¶”ê°€ (í•„ìš”ì‹œ)</span>
<span class="kw">export</span> PATH=<span class="str">"/usr/local/bin:$PATH"</span>
<span class="kw">echo</span> <span class="str">'export PATH="/usr/local/bin:$PATH"'</span> &gt;&gt; ~/.bashrc
source ~/.bashrc

<span class="cmt"># 4. í™•ì¸</span>
ollama --version</code></pre>

  <h4>Windows</h4>

  <pre><code><span class="cmt"># PowerShellì—ì„œ</span>
Get-Command ollama
<span class="cmt"># CommandNotFoundException â†’ ì¬ì„¤ì¹˜ í•„ìš”</span>

<span class="cmt"># í™˜ê²½ ë³€ìˆ˜ PATH í™•ì¸</span>
$env:Path -split <span class="str">';'</span> | Select-String ollama

<span class="cmt"># PATH ì¶”ê°€ (ì˜êµ¬ ì„¤ì •)</span>
[System.Environment]::SetEnvironmentVariable(
    <span class="str">'Path'</span>,
    $env:Path + <span class="str">';C:\Users\[ì‚¬ìš©ì]\AppData\Local\Programs\Ollama\bin'</span>,
    [System.EnvironmentVariableTarget]::User
)</code></pre>

  <h3 id="install-permission">2. ê¶Œí•œ ì˜¤ë¥˜ (Permission Denied)</h3>

  <div class="info-box warning">
    <div class="info-box-title">ì¦ìƒ</div>
    <pre><code>Permission denied when trying to install Ollama</code></pre>
  </div>

  <h4>í•´ê²° ë°©ë²•</h4>

  <pre><code><span class="cmt"># Linux - sudo ê¶Œí•œìœ¼ë¡œ ì¬ì„¤ì¹˜</span>
curl -fsSL https://ollama.com/install.sh | sudo sh

<span class="cmt"># macOS - ê´€ë¦¬ì ê¶Œí•œ</span>
sudo curl -fsSL https://ollama.com/install.sh | sh

<span class="cmt"># íŒŒì¼ ê¶Œí•œ í™•ì¸</span>
ls -la /usr/local/bin/ollama
<span class="cmt"># -rwxr-xr-x ... ollama</span>

<span class="cmt"># ê¶Œí•œ ìˆ˜ì • (í•„ìš”ì‹œ)</span>
sudo chmod +x /usr/local/bin/ollama</code></pre>

  <h3 id="install-systemd">3. systemd ì„œë¹„ìŠ¤ ìƒì„± ì‹¤íŒ¨</h3>

  <div class="info-box warning">
    <div class="info-box-title">ì¦ìƒ</div>
    <pre><code>Failed to enable unit: Unit file ollama.service does not exist</code></pre>
  </div>

  <h4>í•´ê²° ë°©ë²•</h4>

  <pre><code><span class="cmt"># 1. ì„œë¹„ìŠ¤ íŒŒì¼ ìˆ˜ë™ ìƒì„±</span>
sudo tee /etc/systemd/system/ollama.service &lt;&lt;EOF
[Unit]
Description=Ollama Service
After=network-online.target

[Service]
ExecStart=/usr/local/bin/ollama serve
User=ollama
Group=ollama
Restart=always
RestartSec=3
Environment="OLLAMA_HOST=0.0.0.0:11434"

[Install]
WantedBy=default.target
EOF

<span class="cmt"># 2. ì‚¬ìš©ì ìƒì„± (ì—†ìœ¼ë©´)</span>
sudo useradd -r -s /bin/false -m -d /usr/share/ollama ollama

<span class="cmt"># 3. systemd ë¦¬ë¡œë“œ ë° ì‹œì‘</span>
sudo systemctl daemon-reload
sudo systemctl enable ollama
sudo systemctl start ollama

<span class="cmt"># 4. ìƒíƒœ í™•ì¸</span>
sudo systemctl status ollama</code></pre>
</section>

<section class="content-section">
  <h2 id="connection-errors">ì—°ê²° ë¬¸ì œ</h2>

  <h3 id="connection-refused">1. "Connection Refused" (í¬íŠ¸ 11434)</h3>

  <div class="info-box warning">
    <div class="info-box-title">ì¦ìƒ</div>
    <pre><code>curl: (7) Failed to connect to localhost port 11434: Connection refused</code></pre>
  </div>

  <h4>ì›ì¸</h4>
  <ul>
    <li>Ollama ì„œë¹„ìŠ¤ê°€ ì‹¤í–‰ë˜ì§€ ì•ŠìŒ</li>
    <li>í¬íŠ¸ 11434ê°€ ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤ì— ì˜í•´ ì‚¬ìš© ì¤‘</li>
    <li>ë°©í™”ë²½ì´ í¬íŠ¸ë¥¼ ì°¨ë‹¨</li>
  </ul>

  <h4>í•´ê²° ë°©ë²•</h4>

  <pre><code><span class="cmt"># 1. Ollama ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸</span>
<span class="cmt"># Linux</span>
sudo systemctl status ollama
<span class="cmt"># Active: active (running) ì´ì–´ì•¼ í•¨</span>

<span class="cmt"># macOS</span>
ps aux | grep ollama
<span class="cmt"># ollama serve í”„ë¡œì„¸ìŠ¤ê°€ ìˆì–´ì•¼ í•¨</span>

<span class="cmt"># Windows</span>
Get-Service Ollama

<span class="cmt"># 2. ì„œë¹„ìŠ¤ ì‹œì‘</span>
<span class="cmt"># Linux</span>
sudo systemctl start ollama

<span class="cmt"># macOS</span>
ollama serve &amp;

<span class="cmt"># 3. í¬íŠ¸ ì‚¬ìš© í™•ì¸</span>
sudo lsof -i :11434
<span class="cmt"># ollama ... LISTEN ì´ì–´ì•¼ í•¨</span>

<span class="cmt"># ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤ê°€ ì‚¬ìš© ì¤‘ì´ë©´ ì¢…ë£Œ</span>
sudo kill -9 [PID]

<span class="cmt"># 4. ë°©í™”ë²½ í™•ì¸ (Linux)</span>
sudo ufw status
sudo ufw allow 11434/tcp

<span class="cmt"># 5. ì¬ì‹œì‘</span>
sudo systemctl restart ollama</code></pre>

  <h3 id="connection-timeout">2. "Connection Timeout" (ì›ê²© ì ‘ì†)</h3>

  <div class="info-box warning">
    <div class="info-box-title">ì¦ìƒ</div>
    <pre><code>curl: (28) Failed to connect to 192.168.1.100 port 11434: Connection timed out</code></pre>
  </div>

  <h4>í•´ê²° ë°©ë²•</h4>

  <pre><code><span class="cmt"># 1. OLLAMA_HOST í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (0.0.0.0ìœ¼ë¡œ)</span>
<span class="cmt"># Linux - systemd ì„œë¹„ìŠ¤ í¸ì§‘</span>
sudo systemctl edit ollama
<span class="cmt"># ë‹¤ìŒ ì¶”ê°€:</span>
[Service]
Environment=<span class="str">"OLLAMA_HOST=0.0.0.0:11434"</span>

sudo systemctl restart ollama

<span class="cmt"># 2. ë°©í™”ë²½ í¬íŠ¸ ì—´ê¸°</span>
<span class="cmt"># Ubuntu/Debian</span>
sudo ufw allow 11434/tcp

<span class="cmt"># CentOS/RHEL</span>
sudo firewall-cmd --permanent --add-port=11434/tcp
sudo firewall-cmd --reload

<span class="cmt"># 3. í´ë¼ìš°ë“œ ë³´ì•ˆ ê·¸ë£¹ ì„¤ì • (AWS, GCP ë“±)</span>
<span class="cmt"># Inbound ê·œì¹™: TCP 11434 í—ˆìš©</span>

<span class="cmt"># 4. í…ŒìŠ¤íŠ¸</span>
curl http://[ì„œë²„IP]:11434/api/tags</code></pre>

  <h3 id="connection-ssl">3. SSL/TLS ì˜¤ë¥˜</h3>

  <div class="info-box warning">
    <div class="info-box-title">ì¦ìƒ</div>
    <pre><code>SSL certificate problem: unable to get local issuer certificate</code></pre>
  </div>

  <h4>í•´ê²° ë°©ë²•</h4>

  <pre><code><span class="cmt"># 1. ì¸ì¦ì„œ ë¬´ì‹œ (í…ŒìŠ¤íŠ¸ ì „ìš©)</span>
curl -k https://localhost:11434/api/tags

<span class="cmt"># 2. CA ì¸ì¦ì„œ ì„¤ì¹˜ (í”„ë¡œë•ì…˜)</span>
<span class="cmt"># Ubuntu/Debian</span>
sudo apt install ca-certificates
sudo update-ca-certificates

<span class="cmt"># 3. Python requestsì—ì„œ</span>
<span class="kw">import</span> requests
response = requests.post(
    <span class="str">'https://localhost:11434/api/generate'</span>,
    json={...},
    verify=<span class="kw">False</span>  <span class="cmt"># ë˜ëŠ” ì¸ì¦ì„œ ê²½ë¡œ</span>
)</code></pre>
</section>

<section class="content-section">
  <h2 id="gpu-issues">GPU ë¬¸ì œ</h2>

  <h3 id="gpu-not-detected">1. GPU ë¯¸ê°ì§€ (NVIDIA)</h3>

  <div class="info-box warning">
    <div class="info-box-title">ì¦ìƒ</div>
    <pre><code>OllamaëŠ” ì‹¤í–‰ë˜ì§€ë§Œ CPUë§Œ ì‚¬ìš©, GPU ì‚¬ìš©ë¥  0%</code></pre>
  </div>

  <h4>ì§„ë‹¨</h4>

  <pre><code><span class="cmt"># 1. nvidia-smi ì‹¤í–‰</span>
nvidia-smi
<span class="cmt"># GPU ì •ë³´ê°€ í‘œì‹œë˜ì–´ì•¼ í•¨</span>

<span class="cmt"># ì˜¤ë¥˜ ë°œìƒ ì‹œ â†’ ë“œë¼ì´ë²„ ë¬¸ì œ</span>
<span class="cmt"># "NVIDIA-SMI has failed..." â†’ ë“œë¼ì´ë²„ ì¬ì„¤ì¹˜ í•„ìš”</span>

<span class="cmt"># 2. CUDA ë²„ì „ í™•ì¸</span>
nvcc --version
<span class="cmt"># CUDA 11.8+ í•„ìš”</span>

<span class="cmt"># 3. Ollama ë¡œê·¸ í™•ì¸</span>
sudo journalctl -u ollama -n 100
<span class="cmt"># "GPU" ë˜ëŠ” "CUDA" ê´€ë ¨ ë©”ì‹œì§€ ì°¾ê¸°</span></code></pre>

  <h4>í•´ê²° ë°©ë²•</h4>

  <pre><code><span class="cmt"># 1. NVIDIA ë“œë¼ì´ë²„ ì¬ì„¤ì¹˜ (Ubuntu)</span>
<span class="cmt"># ê¸°ì¡´ ë“œë¼ì´ë²„ ì œê±°</span>
sudo apt purge nvidia-*
sudo apt autoremove

<span class="cmt"># ìµœì‹  ë“œë¼ì´ë²„ ì„¤ì¹˜</span>
sudo apt update
sudo apt install nvidia-driver-535

<span class="cmt"># ì¬ë¶€íŒ… í•„ìˆ˜</span>
sudo reboot

<span class="cmt"># 2. CUDA Toolkit ì„¤ì¹˜ (ì„ íƒì‚¬í•­)</span>
wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.1-1_all.deb
sudo dpkg -i cuda-keyring_1.1-1_all.deb
sudo apt update
sudo apt install cuda-toolkit-12-3

<span class="cmt"># 3. Ollama ì¬ì‹œì‘</span>
sudo systemctl restart ollama

<span class="cmt"># 4. GPU ì‚¬ìš© í™•ì¸</span>
ollama run llama3.2 <span class="str">"Hello"</span>
<span class="cmt"># ë™ì‹œì— ë‹¤ë¥¸ í„°ë¯¸ë„ì—ì„œ</span>
watch -n 1 nvidia-smi
<span class="cmt"># GPU ì‚¬ìš©ë¥ ì´ ì˜¬ë¼ê°€ì•¼ í•¨</span></code></pre>

  <h3 id="gpu-oom">2. GPU ë©”ëª¨ë¦¬ ë¶€ì¡± (OOM)</h3>

  <div class="info-box warning">
    <div class="info-box-title">ì¦ìƒ</div>
    <pre><code>CUDA out of memory. Tried to allocate X MiB</code></pre>
  </div>

  <h4>í•´ê²° ë°©ë²•</h4>

  <pre><code><span class="cmt"># 1. ë” ì‘ì€ ëª¨ë¸ ì‚¬ìš©</span>
ollama pull llama3.2:3b      <span class="cmt"># 3B ëª¨ë¸ (2GB VRAM)</span>
ollama pull phi3:mini        <span class="cmt"># Phi-3 Mini (2GB VRAM)</span>

<span class="cmt"># 2. ì–‘ìí™” ë²„ì „ ì‚¬ìš©</span>
ollama pull llama3.2:7b-q4_0  <span class="cmt"># 4-bit ì–‘ìí™” (4GB VRAM)</span>

<span class="cmt"># 3. GPU ë ˆì´ì–´ ìˆ˜ ì œí•œ</span>
<span class="kw">export</span> OLLAMA_NUM_GPU=<span class="num">20</span>  <span class="cmt"># ì¼ë¶€ë§Œ GPU, ë‚˜ë¨¸ì§€ëŠ” CPU</span>
sudo systemctl restart ollama

<span class="cmt"># 4. ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° ì¤„ì´ê¸°</span>
<span class="cmt"># Modelfile</span>
<span class="kw">PARAMETER</span> num_ctx 2048  <span class="cmt"># ê¸°ë³¸ 4096 â†’ 2048</span>

<span class="cmt"># 5. ë‹¤ë¥¸ GPU í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ</span>
nvidia-smi
<span class="cmt"># GPU ë©”ëª¨ë¦¬ ì‚¬ìš© ì¤‘ì¸ í”„ë¡œì„¸ìŠ¤ í™•ì¸</span>
kill -9 [PID]</code></pre>

  <h3 id="gpu-multi">3. ë‹¤ì¤‘ GPU ì„¤ì •</h3>

  <pre><code><span class="cmt"># íŠ¹ì • GPUë§Œ ì‚¬ìš©</span>
<span class="kw">export</span> CUDA_VISIBLE_DEVICES=0  <span class="cmt"># GPU 0ë§Œ ì‚¬ìš©</span>
<span class="kw">export</span> CUDA_VISIBLE_DEVICES=0,1  <span class="cmt"># GPU 0, 1 ì‚¬ìš©</span>

<span class="cmt"># systemd ì„œë¹„ìŠ¤ì— ì ìš©</span>
sudo systemctl edit ollama
[Service]
Environment=<span class="str">"CUDA_VISIBLE_DEVICES=0,1"</span>

sudo systemctl restart ollama

<span class="cmt"># í™•ì¸</span>
nvidia-smi
<span class="cmt"># ì§€ì •í•œ GPUë§Œ ì‚¬ìš© ì¤‘ì´ì–´ì•¼ í•¨</span></code></pre>

  <h3 id="gpu-amd">4. AMD GPU (ROCm) ë¬¸ì œ</h3>

  <div class="info-box warning">
    <div class="info-box-title">âš ï¸ ì‹¤í—˜ì  ì§€ì›</div>
    <p>AMD GPUëŠ” ROCmì„ í†µí•´ ì§€ì›ë˜ì§€ë§Œ, NVIDIA CUDAë³´ë‹¤ ì•ˆì •ì„±ì´ ë‚®ìŠµë‹ˆë‹¤.</p>
  </div>

  <pre><code><span class="cmt"># ROCm ì„¤ì¹˜ (Ubuntu)</span>
wget https://repo.radeon.com/amdgpu-install/latest/ubuntu/focal/amdgpu-install_*_all.deb
sudo dpkg -i amdgpu-install_*_all.deb
sudo amdgpu-install --usecase=rocm

<span class="cmt"># ì¬ë¶€íŒ…</span>
sudo reboot

<span class="cmt"># ROCm í™•ì¸</span>
rocm-smi

<span class="cmt"># Ollama ì¬ì‹œì‘</span>
sudo systemctl restart ollama</code></pre>

  <h3 id="gpu-apple-silicon">5. Apple Silicon (M1/M2/M3) ìµœì í™”</h3>

  <pre><code><span class="cmt"># Metal ê°€ì† ìë™ í™œì„±í™” (ì„¤ì • ë¶ˆí•„ìš”)</span>
ollama run llama3.2

<span class="cmt"># ë©”ëª¨ë¦¬ ì••ë ¥ ëª¨ë‹ˆí„°ë§</span>
sudo memory_pressure

<span class="cmt"># Activity Monitorì—ì„œ í™•ì¸</span>
<span class="cmt"># GPU íƒ­ì—ì„œ "ollama" í”„ë¡œì„¸ìŠ¤ì˜ GPU ì‚¬ìš©ë¥  í™•ì¸</span>

<span class="cmt"># ë©”ëª¨ë¦¬ ìŠ¤ì™‘ ìµœì†Œí™”</span>
<span class="cmt"># ë” ì‘ì€ ëª¨ë¸ ì‚¬ìš©</span>
ollama pull llama3.2:3b  <span class="cmt"># M1 8GBì— ì í•©</span></code></pre>
</section>

<section class="content-section">
  <h2 id="performance-issues">ì„±ëŠ¥ ë¬¸ì œ</h2>

  <h3 id="perf-slow">1. ì‘ë‹µ ì†ë„ê°€ ë§¤ìš° ëŠë¦¼</h3>

  <div class="info-box warning">
    <div class="info-box-title">ì¦ìƒ</div>
    <p>1-2 tokens/sec ì´í•˜, ì‘ë‹µì— ìˆ˜ ë¶„ ì†Œìš”</p>
  </div>

  <h4>ì›ì¸ ë° í•´ê²°</h4>

  <pre><code><span class="cmt"># 1. CPUë§Œ ì‚¬ìš© (GPU ë¯¸í™œìš©)</span>
<span class="cmt"># â†’ GPU ì„¤ì • í™•ì¸ (ìœ„ GPU ì„¹ì…˜ ì°¸ì¡°)</span>
nvidia-smi  <span class="cmt"># GPU ì‚¬ìš©ë¥  0% â†’ ë¬¸ì œ</span>

<span class="cmt"># 2. ëª¨ë¸ì´ ë„ˆë¬´ í¼</span>
<span class="cmt"># â†’ ë” ì‘ì€ ëª¨ë¸ ì‚¬ìš©</span>
ollama pull llama3.2:7b  <span class="cmt"># 70B â†’ 7B</span>

<span class="cmt"># 3. RAM ë¶€ì¡± (ìŠ¤ì™‘ ì‚¬ìš©)</span>
free -h
<span class="cmt"># Swap ì‚¬ìš©ëŸ‰ì´ ë†’ìœ¼ë©´ ë¬¸ì œ</span>
<span class="cmt"># â†’ ëª¨ë¸ í¬ê¸° ì¤„ì´ê¸° ë˜ëŠ” RAM ì¦ì„¤</span>

<span class="cmt"># 4. ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš°ê°€ ë„ˆë¬´ í¼</span>
<span class="cmt"># Modelfile</span>
<span class="kw">PARAMETER</span> num_ctx 2048  <span class="cmt"># 8192 â†’ 2048</span>

<span class="cmt"># 5. ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤ê°€ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©</span>
top
<span class="cmt"># CPU/ë©”ëª¨ë¦¬ ë§ì´ ì“°ëŠ” í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ</span></code></pre>

  <h3 id="perf-memory">2. ë©”ëª¨ë¦¬ ë¶€ì¡± (OOM Killed)</h3>

  <div class="info-box warning">
    <div class="info-box-title">ì¦ìƒ</div>
    <pre><code>Killed
dmesg | grep -i "out of memory"
# Out of memory: Killed process [PID] (ollama)</code></pre>
  </div>

  <h4>í•´ê²° ë°©ë²•</h4>

  <pre><code><span class="cmt"># 1. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸</span>
free -h
<span class="cmt"># ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ëª¨ë¦¬ í™•ì¸</span>

<span class="cmt"># 2. ë” ì‘ì€ ëª¨ë¸ ì‚¬ìš©</span>
<span class="cmt"># ëª¨ë¸ë³„ RAM ìš”êµ¬ì‚¬í•­:</span>
<span class="cmt"># - 3B: 8GB</span>
<span class="cmt"># - 7B: 16GB</span>
<span class="cmt"># - 13B: 24GB</span>
<span class="cmt"># - 70B: 64GB+</span>

ollama pull phi3:mini  <span class="cmt"># 3B, 8GB RAM</span>

<span class="cmt"># 3. ì–‘ìí™” ë²„ì „ ì‚¬ìš©</span>
ollama pull llama3.2:7b-q4_0  <span class="cmt"># ì ˆë°˜ í¬ê¸°</span>

<span class="cmt"># 4. ìŠ¤ì™‘ ë©”ëª¨ë¦¬ ì¦ê°€ (ì„ì‹œ ë°©í¸)</span>
sudo fallocate -l 8G /swapfile
sudo chmod 600 /swapfile
sudo mkswap /swapfile
sudo swapon /swapfile

<span class="cmt"># 5. ë™ì‹œ ì‹¤í–‰ ëª¨ë¸ ìˆ˜ ì œí•œ</span>
<span class="kw">export</span> OLLAMA_MAX_LOADED_MODELS=1
sudo systemctl restart ollama</code></pre>

  <h3 id="perf-disk">3. ë””ìŠ¤í¬ ê³µê°„ ë¶€ì¡±</h3>

  <div class="info-box warning">
    <div class="info-box-title">ì¦ìƒ</div>
    <pre><code>Error: no space left on device</code></pre>
  </div>

  <h4>í•´ê²° ë°©ë²•</h4>

  <pre><code><span class="cmt"># 1. ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰ í™•ì¸</span>
df -h
<span class="cmt"># ~/.ollama ê²½ë¡œì˜ ì‚¬ìš©ëŸ‰ í™•ì¸</span>

du -sh ~/.ollama/models
<span class="cmt"># ëª¨ë¸ ì´ í¬ê¸° í™•ì¸</span>

<span class="cmt"># 2. ë¯¸ì‚¬ìš© ëª¨ë¸ ì‚­ì œ</span>
ollama list
ollama rm [ëª¨ë¸ëª…]

<span class="cmt"># 3. ëª¨ë¸ ì €ì¥ ê²½ë¡œ ë³€ê²½ (ë” í° ë””ìŠ¤í¬ë¡œ)</span>
<span class="cmt"># Linux</span>
sudo systemctl edit ollama
[Service]
Environment=<span class="str">"OLLAMA_MODELS=/mnt/storage/ollama"</span>

sudo mkdir -p /mnt/storage/ollama
sudo chown ollama:ollama /mnt/storage/ollama
sudo systemctl restart ollama

<span class="cmt"># 4. Docker ì´ë¯¸ì§€ ì •ë¦¬ (Docker ì‚¬ìš© ì‹œ)</span>
docker system prune -a</code></pre>

  <h3 id="perf-concurrent">4. ë™ì‹œ ìš”ì²­ ì‹œ ëŠë ¤ì§</h3>

  <pre><code><span class="cmt"># ë³‘ë ¬ ìš”ì²­ ìˆ˜ ì¦ê°€</span>
<span class="kw">export</span> OLLAMA_NUM_PARALLEL=4  <span class="cmt"># ê¸°ë³¸ 1 â†’ 4</span>

<span class="cmt"># systemd ì„œë¹„ìŠ¤</span>
sudo systemctl edit ollama
[Service]
Environment=<span class="str">"OLLAMA_NUM_PARALLEL=4"</span>

sudo systemctl restart ollama

<span class="cmt"># ì£¼ì˜: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¦ê°€</span>
<span class="cmt"># ë³‘ë ¬ ì²˜ë¦¬ ìˆ˜ = ë™ì‹œ ë¡œë“œ ëª¨ë¸ ìˆ˜</span></code></pre>
</section>

<section class="content-section">
  <h2 id="model-errors">ëª¨ë¸ ë¬¸ì œ</h2>

  <h3 id="model-download-fail">1. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨</h3>

  <div class="info-box warning">
    <div class="info-box-title">ì¦ìƒ</div>
    <pre><code>Error: failed to download model
Error: unexpected EOF</code></pre>
  </div>

  <h4>í•´ê²° ë°©ë²•</h4>

  <pre><code><span class="cmt"># 1. ë„¤íŠ¸ì›Œí¬ ì—°ê²° í™•ì¸</span>
ping ollama.com
curl https://ollama.com

<span class="cmt"># 2. ì¬ì‹œë„ (ë‹¨ìˆœ ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜)</span>
ollama pull llama3.2

<span class="cmt"># 3. ê¸°ì¡´ ë¶€ë¶„ ë‹¤ìš´ë¡œë“œ ì‚­ì œ í›„ ì¬ì‹œë„</span>
rm -rf ~/.ollama/models/blobs/sha256-*
ollama pull llama3.2

<span class="cmt"># 4. í”„ë¡ì‹œ ì„¤ì • (í•„ìš”ì‹œ)</span>
<span class="kw">export</span> HTTP_PROXY=<span class="str">"http://proxy.example.com:8080"</span>
<span class="kw">export</span> HTTPS_PROXY=<span class="str">"http://proxy.example.com:8080"</span>

<span class="cmt"># 5. DNS ë¬¸ì œ (ë“œë¬¸ ê²½ìš°)</span>
<span class="kw">echo</span> <span class="str">"1.1.1.1"</span> | sudo tee /etc/resolv.conf</code></pre>

  <h3 id="model-corrupted">2. ì†ìƒëœ ëª¨ë¸ íŒŒì¼</h3>

  <div class="info-box warning">
    <div class="info-box-title">ì¦ìƒ</div>
    <pre><code>Error: invalid model file
Error: checksum mismatch</code></pre>
  </div>

  <h4>í•´ê²° ë°©ë²•</h4>

  <pre><code><span class="cmt"># 1. ëª¨ë¸ ì‚­ì œ</span>
ollama rm llama3.2

<span class="cmt"># 2. ìºì‹œ ì •ë¦¬</span>
rm -rf ~/.ollama/models/manifests/registry.ollama.ai/library/llama3.2
rm -rf ~/.ollama/models/blobs/sha256-*

<span class="cmt"># 3. ì¬ë‹¤ìš´ë¡œë“œ</span>
ollama pull llama3.2

<span class="cmt"># 4. ì—¬ì „íˆ ì‹¤íŒ¨ ì‹œ - Ollama ì¬ì„¤ì¹˜</span>
sudo systemctl stop ollama
sudo rm -rf ~/.ollama
curl -fsSL https://ollama.com/install.sh | sh
ollama pull llama3.2</code></pre>

  <h3 id="model-not-found">3. "Model Not Found"</h3>

  <div class="info-box warning">
    <div class="info-box-title">ì¦ìƒ</div>
    <pre><code>Error: model 'llama3.2' not found</code></pre>
  </div>

  <h4>í•´ê²° ë°©ë²•</h4>

  <pre><code><span class="cmt"># 1. ì„¤ì¹˜ëœ ëª¨ë¸ í™•ì¸</span>
ollama list
<span class="cmt"># ëª©ë¡ì— ì—†ìœ¼ë©´ ë‹¤ìš´ë¡œë“œ í•„ìš”</span>

<span class="cmt"># 2. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ</span>
ollama pull llama3.2

<span class="cmt"># 3. ëª¨ë¸ ì´ë¦„ ì˜¤íƒ€ í™•ì¸</span>
<span class="cmt"># ì •í™•í•œ ì´ë¦„: llama3.2 (llama-3.2 âŒ)</span>

<span class="cmt"># 4. ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ê²€ìƒ‰</span>
curl https://ollama.com/api/tags | jq <span class="str">'.models[] | .name'</span></code></pre>

  <h3 id="model-version">4. ëª¨ë¸ ë²„ì „ ì¶©ëŒ</h3>

  <pre><code><span class="cmt"># íŠ¹ì • ë²„ì „ ë‹¤ìš´ë¡œë“œ</span>
ollama pull llama3.2:latest
ollama pull llama3.2:7b-q4_0

<span class="cmt"># ëª¨ë“  ë²„ì „ í™•ì¸</span>
ollama list
<span class="cmt"># llama3.2:latest</span>
<span class="cmt"># llama3.2:7b-q4_0</span>

<span class="cmt"># íŠ¹ì • ë²„ì „ ì‹¤í–‰</span>
ollama run llama3.2:7b-q4_0

<span class="cmt"># ê¸°ë³¸ ë²„ì „ (latest) ë³€ê²½</span>
ollama rm llama3.2:latest
ollama pull llama3.2:7b-q4_0
ollama tag llama3.2:7b-q4_0 llama3.2:latest</code></pre>
</section>

<section class="content-section">
  <h2 id="common-errors">ì¼ë°˜ì ì¸ ì—ëŸ¬ ë©”ì‹œì§€ (A-Z)</h2>

  <h3 id="err-1">1. "API version mismatch"</h3>

  <pre><code><span class="cmt"># Ollama ë²„ì „ ë¶ˆì¼ì¹˜</span>
<span class="cmt"># í•´ê²°: Ollama ì—…ë°ì´íŠ¸</span>
curl -fsSL https://ollama.com/install.sh | sh
sudo systemctl restart ollama</code></pre>

  <h3 id="err-2">2. "context window exceeded"</h3>

  <pre><code><span class="cmt"># ì…ë ¥ í† í°ì´ ëª¨ë¸ì˜ ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° ì´ˆê³¼</span>
<span class="cmt"># í•´ê²°: ì…ë ¥ ì¤„ì´ê¸° ë˜ëŠ” num_ctx ì¦ê°€</span>
<span class="kw">PARAMETER</span> num_ctx 8192  <span class="cmt"># Modelfile</span></code></pre>

  <h3 id="err-3">3. "embedding model not found"</h3>

  <pre><code><span class="cmt"># ì„ë² ë”© ëª¨ë¸ ë¯¸ì„¤ì¹˜</span>
ollama pull nomic-embed-text</code></pre>

  <h3 id="err-4">4. "failed to load model"</h3>

  <pre><code><span class="cmt"># ë©”ëª¨ë¦¬ ë¶€ì¡± ë˜ëŠ” ì†ìƒëœ ëª¨ë¸</span>
<span class="cmt"># 1. ë©”ëª¨ë¦¬ í™•ì¸</span>
free -h

<span class="cmt"># 2. ëª¨ë¸ ì¬ë‹¤ìš´ë¡œë“œ</span>
ollama rm llama3.2
ollama pull llama3.2</code></pre>

  <h3 id="err-5">5. "invalid modelfile syntax"</h3>

  <pre><code><span class="cmt"># Modelfile ë¬¸ë²• ì˜¤ë¥˜</span>
<span class="cmt"># í™•ì¸: ëŒ€ì†Œë¬¸ì, ë”°ì˜´í‘œ, ì¤„ë°”ê¿ˆ</span>
<span class="kw">FROM</span> llama3.2  <span class="cmt"># ì˜¬ë°”ë¦„</span>
<span class="kw">from</span> llama3.2  <span class="cmt"># ì˜ëª»ë¨ (ì†Œë¬¸ì)</span>

<span class="kw">SYSTEM</span> <span class="str">"..."</span>  <span class="cmt"># ë”°ì˜´í‘œ í•„ìˆ˜</span></code></pre>

  <h3 id="err-6">6. "model is too large"</h3>

  <pre><code><span class="cmt"># ëª¨ë¸ì´ ë©”ëª¨ë¦¬ë³´ë‹¤ í¼</span>
<span class="cmt"># í•´ê²°: ì–‘ìí™” ë²„ì „ ì‚¬ìš©</span>
ollama pull llama3.2:7b-q4_0  <span class="cmt"># 7b ëŒ€ì‹ </span></code></pre>

  <h3 id="err-7">7. "NUMA node not available"</h3>

  <pre><code><span class="cmt"># CPU NUMA ì„¤ì • ë¬¸ì œ (ë¬´ì‹œ ê°€ëŠ¥)</span>
<span class="cmt"># ì„±ëŠ¥ì— ì˜í–¥ ì—†ìŒ, ê²½ê³  ë©”ì‹œì§€ì¼ ë¿</span></code></pre>

  <h3 id="err-8">8. "port already in use"</h3>

  <pre><code><span class="cmt"># í¬íŠ¸ 11434ê°€ ì´ë¯¸ ì‚¬ìš© ì¤‘</span>
sudo lsof -i :11434
sudo kill -9 [PID]

<span class="cmt"># ë˜ëŠ” ë‹¤ë¥¸ í¬íŠ¸ ì‚¬ìš©</span>
<span class="kw">export</span> OLLAMA_HOST=0.0.0.0:11435
sudo systemctl restart ollama</code></pre>

  <h3 id="err-9">9. "request timeout"</h3>

  <pre><code><span class="cmt"># ëª¨ë¸ ì‘ë‹µì´ ë„ˆë¬´ ëŠë¦¼</span>
<span class="cmt"># API í˜¸ì¶œ ì‹œ íƒ€ì„ì•„ì›ƒ ì¦ê°€</span>
requests.post(..., timeout=<span class="num">300</span>)  <span class="cmt"># 5ë¶„</span>

<span class="cmt"># ë˜ëŠ” ë” ë¹ ë¥¸ ëª¨ë¸ ì‚¬ìš©</span>
ollama pull llama3.2:3b</code></pre>

  <h3 id="err-10">10. "stream error"</h3>

  <pre><code><span class="cmt"># ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì¤‘ ì—°ê²° ëŠê¹€</span>
<span class="cmt"># í•´ê²°: ì¬ì‹œë„ ë˜ëŠ” stream=false ì‚¬ìš©</span>
{
  <span class="str">"model"</span>: <span class="str">"llama3.2"</span>,
  <span class="str">"prompt"</span>: <span class="str">"..."</span>,
  <span class="str">"stream"</span>: <span class="kw">false</span>  <span class="cmt"># ìŠ¤íŠ¸ë¦¬ë° ë¹„í™œì„±í™”</span>
}</code></pre>

  <h3 id="err-11">11. "temperature out of range"</h3>

  <pre><code><span class="cmt"># temperature ê°’ì´ ë²”ìœ„ ë°– (0.0 ~ 2.0)</span>
<span class="kw">PARAMETER</span> temperature 0.8  <span class="cmt"># 0.0 ~ 2.0 ì‚¬ì´</span></code></pre>

  <h3 id="err-12">12. "unable to locate model"</h3>

  <pre><code><span class="cmt"># ì»¤ìŠ¤í…€ ëª¨ë¸ íŒŒì¼ ê²½ë¡œ ì˜¤ë¥˜</span>
<span class="cmt"># Modelfileì˜ FROM ê²½ë¡œ í™•ì¸</span>
<span class="kw">FROM</span> ./model.gguf  <span class="cmt"># ìƒëŒ€ ê²½ë¡œ</span>
<span class="kw">FROM</span> /absolute/path/model.gguf  <span class="cmt"># ì ˆëŒ€ ê²½ë¡œ</span></code></pre>

  <h3 id="err-13">13. "unsupported model format"</h3>

  <pre><code><span class="cmt"># GGUF í˜•ì‹ì´ ì•„ë‹Œ íŒŒì¼</span>
<span class="cmt"># í•´ê²°: GGUF ë³€í™˜ (llama.cpp ì‚¬ìš©)</span>
python convert.py --outtype f16 --outfile model.gguf model/</code></pre>

  <h3 id="err-14">14. "WSL GPU not available"</h3>

  <pre><code><span class="cmt"># WSL2ì—ì„œ GPU ë¯¸ì§€ì›</span>
<span class="cmt"># 1. Windows NVIDIA ë“œë¼ì´ë²„ ìµœì‹  ë²„ì „ ì„¤ì¹˜</span>
<span class="cmt"># 2. WSL2ì—ì„œ nvidia-smi ì‹¤í–‰ í™•ì¸</span>
nvidia-smi

<span class="cmt"># 3. CUDA WSL ì„¤ì •</span>
<span class="cmt"># https://docs.nvidia.com/cuda/wsl-user-guide/</span></code></pre>

  <h3 id="err-15">15. "model loading timeout"</h3>

  <pre><code><span class="cmt"># ëª¨ë¸ ë¡œë“œì— ì‹œê°„ ì˜¤ë˜ ê±¸ë¦¼</span>
<span class="cmt"># OLLAMA_KEEP_ALIVE ì¦ê°€</span>
<span class="kw">export</span> OLLAMA_KEEP_ALIVE=30m  <span class="cmt"># ê¸°ë³¸ 5m</span>
sudo systemctl restart ollama</code></pre>

  <h3 id="err-16">16. "CUDA driver version insufficient"</h3>

  <pre><code><span class="cmt"># CUDA ë“œë¼ì´ë²„ ë²„ì „ ë‚®ìŒ</span>
nvidia-smi
<span class="cmt"># Driver Version í™•ì¸</span>

<span class="cmt"># 535 ì´ìƒ í•„ìš”</span>
sudo apt install nvidia-driver-535
sudo reboot</code></pre>

  <h3 id="err-17">17. "Metal not available"</h3>

  <pre><code><span class="cmt"># macOSì—ì„œ Metal ë¯¸ì§€ì› (êµ¬í˜• Mac)</span>
<span class="cmt"># CPU ëª¨ë“œë¡œ ë™ì‘ (ì •ìƒ)</span>
<span class="cmt"># Intel Macì€ Metal ë¯¸ì§€ì›</span></code></pre>

  <h3 id="err-18">18. "quantization not supported"</h3>

  <pre><code><span class="cmt"># ì§€ì›ë˜ì§€ ì•ŠëŠ” ì–‘ìí™” í˜•ì‹</span>
<span class="cmt"># ì‚¬ìš© ê°€ëŠ¥: Q4_0, Q5_K_M, Q8_0 ë“±</span>
ollama pull llama3.2:7b-q4_0  <span class="cmt"># ì˜¬ë°”ë¦„</span></code></pre>

  <h3 id="err-19">19. "disk write error"</h3>

  <pre><code><span class="cmt"># ë””ìŠ¤í¬ ì“°ê¸° ê¶Œí•œ ë˜ëŠ” ê³µê°„ ë¬¸ì œ</span>
<span class="cmt"># 1. ê¶Œí•œ í™•ì¸</span>
ls -ld ~/.ollama
<span class="cmt"># drwxr-xr-x ì‚¬ìš©ì ì‚¬ìš©ì</span>

<span class="cmt"># 2. ê³µê°„ í™•ì¸</span>
df -h

<span class="cmt"># 3. ê¶Œí•œ ìˆ˜ì •</span>
sudo chown -R $USER:$USER ~/.ollama</code></pre>

  <h3 id="err-20">20. "unexpected token in JSON"</h3>

  <pre><code><span class="cmt"># API ìš”ì²­ JSON í˜•ì‹ ì˜¤ë¥˜</span>
<span class="cmt"># ì˜¬ë°”ë¥¸ í˜•ì‹:</span>
{
  <span class="str">"model"</span>: <span class="str">"llama3.2"</span>,
  <span class="str">"prompt"</span>: <span class="str">"Hello"</span>,
  <span class="str">"stream"</span>: <span class="kw">false</span>
}

<span class="cmt"># ì˜ëª»ëœ ì˜ˆ:</span>
<span class="cmt"># - ë”°ì˜´í‘œ ëˆ„ë½</span>
<span class="cmt"># - ë§ˆì§€ë§‰ ì‰¼í‘œ</span>
<span class="cmt"># - ì˜ëª»ëœ ë¶ˆë¦° ê°’ (False â†’ false)</span></code></pre>
</section>

<section class="content-section">
  <h2 id="debug-tools">ë””ë²„ê¹… ë„êµ¬</h2>

  <h3 id="debug-logs">ë¡œê·¸ í™•ì¸</h3>

  <pre><code><span class="cmt"># Linux (systemd)</span>
sudo journalctl -u ollama -f
sudo journalctl -u ollama -n 100  <span class="cmt"># ìµœê·¼ 100ì¤„</span>
sudo journalctl -u ollama --since <span class="str">"1 hour ago"</span>

<span class="cmt"># macOS</span>
tail -f ~/.ollama/logs/server.log

<span class="cmt"># Docker</span>
docker logs -f ollama
docker logs --tail 100 ollama

<span class="cmt"># ë””ë²„ê·¸ ëª¨ë“œ í™œì„±í™”</span>
<span class="kw">export</span> OLLAMA_DEBUG=1
sudo systemctl restart ollama</code></pre>

  <h3 id="debug-api">API í…ŒìŠ¤íŠ¸</h3>

  <pre><code><span class="cmt"># 1. ì„œë²„ ìƒíƒœ í™•ì¸</span>
curl http://localhost:11434/api/tags

<span class="cmt"># 2. ëª¨ë¸ ëª©ë¡</span>
curl http://localhost:11434/api/tags | jq

<span class="cmt"># 3. ê°„ë‹¨í•œ ìƒì„± í…ŒìŠ¤íŠ¸</span>
curl http://localhost:11434/api/generate -d <span class="str">'{
  "model": "llama3.2",
  "prompt": "Hello",
  "stream": false
}'</span> | jq

<span class="cmt"># 4. ìŠ¤íŠ¸ë¦¬ë° í…ŒìŠ¤íŠ¸</span>
curl http://localhost:11434/api/generate -d <span class="str">'{
  "model": "llama3.2",
  "prompt": "Tell me a story",
  "stream": true
}'</span></code></pre>

  <h3 id="debug-monitoring">ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§</h3>

  <pre><code><span class="cmt"># CPU, ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ </span>
top
htop

<span class="cmt"># GPU ëª¨ë‹ˆí„°ë§ (NVIDIA)</span>
watch -n 1 nvidia-smi

<span class="cmt"># ë„¤íŠ¸ì›Œí¬ ì—°ê²°</span>
netstat -tuln | grep 11434
ss -tuln | grep 11434

<span class="cmt"># ë””ìŠ¤í¬ I/O</span>
iostat -x 1

<span class="cmt"># í”„ë¡œì„¸ìŠ¤ ìƒì„¸ ì •ë³´</span>
ps aux | grep ollama
pstree -p | grep ollama</code></pre>

  <h3 id="debug-benchmark">ë²¤ì¹˜ë§ˆí¬</h3>

  <pre><code><span class="cmt"># ê°„ë‹¨í•œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸</span>
time ollama run llama3.2 <span class="str">"Count from 1 to 100"</span>

<span class="cmt"># Python ë²¤ì¹˜ë§ˆí¬ ìŠ¤í¬ë¦½íŠ¸</span>
<span class="kw">import</span> time
<span class="kw">import</span> requests

<span class="kw">def</span> <span class="fn">benchmark</span>(model, prompt, iterations=<span class="num">10</span>):
    times = []
    <span class="kw">for</span> _ <span class="kw">in</span> range(iterations):
        start = time.time()
        response = requests.post(
            <span class="str">'http://localhost:11434/api/generate'</span>,
            json={<span class="str">'model'</span>: model, <span class="str">'prompt'</span>: prompt, <span class="str">'stream'</span>: <span class="kw">False</span>}
        )
        duration = time.time() - start
        times.append(duration)

    <span class="kw">print</span>(<span class="str">f"í‰ê· : {sum(times)/len(times):.2f}ì´ˆ"</span>)
    <span class="kw">print</span>(<span class="str">f"ìµœì†Œ: {min(times):.2f}ì´ˆ"</span>)
    <span class="kw">print</span>(<span class="str">f"ìµœëŒ€: {max(times):.2f}ì´ˆ"</span>)

benchmark(<span class="str">"llama3.2"</span>, <span class="str">"Hello, how are you?"</span>)</code></pre>
</section>

<section class="content-section">
  <h2 id="faq">ìì£¼ ë¬»ëŠ” ì§ˆë¬¸ (FAQ)</h2>

  <h3 id="faq-1">Q1. Ollamaë¥¼ ì™„ì „íˆ ì‚­ì œí•˜ë ¤ë©´?</h3>

  <pre><code><span class="cmt"># Linux</span>
sudo systemctl stop ollama
sudo systemctl disable ollama
sudo rm /etc/systemd/system/ollama.service
sudo rm /usr/local/bin/ollama
rm -rf ~/.ollama

<span class="cmt"># macOS</span>
rm -rf /Applications/Ollama.app
rm -rf ~/.ollama
rm /usr/local/bin/ollama

<span class="cmt"># Windows</span>
<span class="cmt"># ì œì–´íŒ â†’ í”„ë¡œê·¸ë¨ ì œê±°</span>
<span class="cmt"># %USERPROFILE%\.ollama í´ë” ì‚­ì œ</span></code></pre>

  <h3 id="faq-2">Q2. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ í´ë”ë¥¼ ì˜®ê¸°ë ¤ë©´?</h3>

  <pre><code><span class="cmt"># 1. ìƒˆ í´ë”ë¡œ ëª¨ë¸ ë³µì‚¬</span>
sudo mkdir -p /mnt/storage/ollama
sudo cp -r ~/.ollama/* /mnt/storage/ollama/

<span class="cmt"># 2. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •</span>
sudo systemctl edit ollama
[Service]
Environment=<span class="str">"OLLAMA_MODELS=/mnt/storage/ollama"</span>

<span class="cmt"># 3. ì¬ì‹œì‘</span>
sudo systemctl restart ollama

<span class="cmt"># 4. í™•ì¸</span>
ollama list</code></pre>

  <h3 id="faq-3">Q3. API í‚¤ ì¸ì¦ì„ ì¶”ê°€í•˜ë ¤ë©´?</h3>

  <pre><code><span class="cmt"># NGINX ë¦¬ë²„ìŠ¤ í”„ë¡ì‹œì— Basic Auth ì¶”ê°€</span>
<span class="cmt"># nginx.conf</span>
location / {
    auth_basic <span class="str">"Restricted"</span>;
    auth_basic_user_file /etc/nginx/.htpasswd;
    proxy_pass http://localhost:11434;
}

<span class="cmt"># htpasswd ìƒì„±</span>
sudo htpasswd -c /etc/nginx/.htpasswd admin

<span class="cmt"># ì‚¬ìš©</span>
curl -u admin:password http://your-server/api/generate ...</code></pre>

  <h3 id="faq-4">Q4. ì—¬ëŸ¬ ëª¨ë¸ì„ ë™ì‹œì— ì‚¬ìš©í•˜ë ¤ë©´?</h3>

  <pre><code><span class="cmt"># OLLAMA_MAX_LOADED_MODELS ì¦ê°€</span>
<span class="kw">export</span> OLLAMA_MAX_LOADED_MODELS=3  <span class="cmt"># ìµœëŒ€ 3ê°œ</span>
sudo systemctl restart ollama

<span class="cmt"># ì£¼ì˜: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¦ê°€</span>
<span class="cmt"># ëª¨ë¸ë‹¹ 3-7GB â†’ ì´ 9-21GB í•„ìš”</span></code></pre>

  <h3 id="faq-5">Q5. Ollamaë¥¼ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰í•˜ì§€ ì•Šìœ¼ë ¤ë©´?</h3>

  <pre><code><span class="cmt"># Linux - systemd ë¹„í™œì„±í™”</span>
sudo systemctl stop ollama
sudo systemctl disable ollama

<span class="cmt"># ìˆ˜ë™ ì‹¤í–‰ (í¬ê·¸ë¼ìš´ë“œ)</span>
ollama serve

<span class="cmt"># ë˜ëŠ” ë°±ê·¸ë¼ìš´ë“œ</span>
ollama serve &amp;

<span class="cmt"># ì¢…ë£Œ</span>
pkill ollama</code></pre>

  <h3 id="faq-6">Q6. íŠ¹ì • IPë§Œ ì ‘ì† í—ˆìš©í•˜ë ¤ë©´?</h3>

  <pre><code><span class="cmt"># ë°©í™”ë²½ ê·œì¹™ (UFW)</span>
sudo ufw deny 11434/tcp
sudo ufw allow from 192.168.1.0/24 to any port 11434

<span class="cmt"># NGINXì—ì„œ IP ì œí•œ</span>
location / {
    allow 192.168.1.0/24;
    deny all;
    proxy_pass http://localhost:11434;
}</code></pre>

  <h3 id="faq-7">Q7. CPU ì‚¬ìš©ëŸ‰ì´ ë„ˆë¬´ ë†’ìŠµë‹ˆë‹¤</h3>

  <pre><code><span class="cmt"># 1. GPU ì‚¬ìš© í™•ì¸ (GPU ìˆìœ¼ë©´)</span>
nvidia-smi

<span class="cmt"># 2. ë” ì‘ì€ ëª¨ë¸ ì‚¬ìš©</span>
ollama pull phi3:mini

<span class="cmt"># 3. ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° ì¤„ì´ê¸°</span>
<span class="kw">PARAMETER</span> num_ctx 1024

<span class="cmt"># 4. ë™ì‹œ ìš”ì²­ ìˆ˜ ì œí•œ</span>
<span class="kw">export</span> OLLAMA_NUM_PARALLEL=1</code></pre>

  <h3 id="faq-8">Q8. ì˜¤í”„ë¼ì¸ í™˜ê²½ì—ì„œ ì‚¬ìš©í•˜ë ¤ë©´?</h3>

  <pre><code><span class="cmt"># 1. ì¸í„°ë„· ì—°ê²°ëœ PCì—ì„œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ</span>
ollama pull llama3.2

<span class="cmt"># 2. ëª¨ë¸ í´ë” ì••ì¶•</span>
tar -czf ollama-models.tar.gz ~/.ollama

<span class="cmt"># 3. ì˜¤í”„ë¼ì¸ PCë¡œ ì „ì†¡</span>
scp ollama-models.tar.gz user@offline-pc:~

<span class="cmt"># 4. ì˜¤í”„ë¼ì¸ PCì—ì„œ ì••ì¶• í•´ì œ</span>
tar -xzf ollama-models.tar.gz -C ~/

<span class="cmt"># 5. Ollama ì„¤ì¹˜ (ì¸í„°ë„· í•„ìš”)</span>
<span class="cmt"># ì„¤ì¹˜ íŒŒì¼ì„ ë¯¸ë¦¬ ë‹¤ìš´ë¡œë“œí•˜ì—¬ ì „ì†¡</span></code></pre>
</section>

<section class="content-section">
  <h2 id="resources">ì¶”ê°€ ë¦¬ì†ŒìŠ¤</h2>

  <h3 id="res-official">ê³µì‹ ë¬¸ì„œ</h3>
  <ul>
    <li><a href="https://github.com/ollama/ollama" target="_blank">GitHub - ollama/ollama</a></li>
    <li><a href="https://ollama.com/library" target="_blank">Model Library</a></li>
    <li><a href="https://github.com/ollama/ollama/blob/main/docs/api.md" target="_blank">API Documentation</a></li>
  </ul>

  <h3 id="res-community">ì»¤ë®¤ë‹ˆí‹°</h3>
  <ul>
    <li><a href="https://discord.gg/ollama" target="_blank">Discord Server</a></li>
    <li><a href="https://www.reddit.com/r/ollama/" target="_blank">Reddit - r/ollama</a></li>
    <li><a href="https://www.reddit.com/r/LocalLLaMA/" target="_blank">Reddit - r/LocalLLaMA</a></li>
  </ul>

  <h3 id="res-tools">ê´€ë ¨ ë„êµ¬</h3>
  <ul>
    <li><a href="https://github.com/continuedev/continue" target="_blank">Continue.dev</a> - VS Code í™•ì¥ (<a href="ollama-integration.html#continue-dev">ê°€ì´ë“œ</a>)</li>
    <li><a href="https://github.com/paul-gauthier/aider" target="_blank">Aider</a> - CLI ì½”ë”© ë„êµ¬ (<a href="ollama-integration.html#aider">ê°€ì´ë“œ</a>)</li>
    <li><a href="https://github.com/open-webui/open-webui" target="_blank">Open WebUI</a> - ChatGPT ìŠ¤íƒ€ì¼ ì›¹ UI (<a href="ollama-integration.html#open-webui"><strong>ì™„ì „ ê°€ì´ë“œ</strong></a>)</li>
    <li><a href="https://github.com/langchain-ai/langchain" target="_blank">LangChain</a> - Python í”„ë ˆì„ì›Œí¬ (<a href="ollama-integration.html#langchain">ê°€ì´ë“œ</a>)</li>
    <li><a href="https://github.com/run-llama/llama_index" target="_blank">LlamaIndex</a> - RAG ì „ë¬¸ (<a href="ollama-integration.html#llamaindex">ê°€ì´ë“œ</a>)</li>
  </ul>
</section>

<section class="content-section">
  <h2 id="next-steps">ë‹¤ìŒ ë‹¨ê³„</h2>

  <p>ë¬¸ì œë¥¼ í•´ê²°í–ˆë‹¤ë©´, ì´ì œ Ollamaë¥¼ ì‹¤ì „ì—ì„œ í™œìš©í•´ë´…ì‹œë‹¤!</p>

  <div class="info-box info">
    <div class="info-box-title">ğŸ“š ê³„ì† í•™ìŠµí•˜ê¸°</div>
    <ol>
      <li><a href="ollama-intro.html">Ollama ì†Œê°œ</a> - ê¸°ë³¸ ê°œë… ë³µìŠµ</li>
      <li><a href="ollama-integration.html">ë„êµ¬ ì—°ë™</a> - Continue, Aider ì„¤ì •</li>
      <li><a href="ollama-advanced.html">ê³ ê¸‰ í™œìš©</a> - Modelfile, RAG, ì—ì´ì „íŠ¸</li>
    </ol>
  </div>

  <div class="info-box tip">
    <div class="info-box-title">ğŸ’¡ ë¬¸ì œ í•´ê²° ì²´í¬ë¦¬ìŠ¤íŠ¸</div>
    <ol>
      <li><strong>ë¡œê·¸ í™•ì¸</strong>: <code>sudo journalctl -u ollama -n 100</code></li>
      <li><strong>ì„œë¹„ìŠ¤ ì¬ì‹œì‘</strong>: <code>sudo systemctl restart ollama</code></li>
      <li><strong>ëª¨ë¸ ì¬ë‹¤ìš´ë¡œë“œ</strong>: <code>ollama rm â†’ ollama pull</code></li>
      <li><strong>GPU í™•ì¸</strong>: <code>nvidia-smi</code></li>
      <li><strong>ë©”ëª¨ë¦¬ í™•ì¸</strong>: <code>free -h</code></li>
      <li><strong>ë””ìŠ¤í¬ í™•ì¸</strong>: <code>df -h</code></li>
      <li><strong>API í…ŒìŠ¤íŠ¸</strong>: <code>curl localhost:11434/api/tags</code></li>
      <li><strong>ìµœì¢… ìˆ˜ë‹¨</strong>: Ollama ì¬ì„¤ì¹˜</li>
    </ol>
  </div>
</section>

<section class="content-section">
  <h2 id="summary">í•µì‹¬ ì •ë¦¬</h2>
  <ul>
    <li>Ollama íŠ¸ëŸ¬ë¸”ìŠˆíŒ…ì˜ í•µì‹¬ ê°œë…ê³¼ íë¦„ì„ ì •ë¦¬í•©ë‹ˆë‹¤.</li>
    <li>ì„¤ì¹˜ ë¬¸ì œë¥¼ ë‹¨ê³„ë³„ë¡œ ì´í•´í•©ë‹ˆë‹¤.</li>
    <li>ì‹¤ì „ ì ìš© ì‹œ ê¸°ì¤€ê³¼ ì£¼ì˜ì ì„ í™•ì¸í•©ë‹ˆë‹¤.</li>
  </ul>
</section>

<section class="content-section">
  <h2 id="practice-tips">ì‹¤ë¬´ íŒ</h2>
  <ul>
    <li>ì…ë ¥/ì¶œë ¥ ì˜ˆì‹œë¥¼ ê³ ì •í•´ ì¬í˜„ì„±ì„ í™•ë³´í•˜ì„¸ìš”.</li>
    <li>Ollama íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ë²”ìœ„ë¥¼ ì‘ê²Œ ì¡ê³  ë‹¨ê³„ì ìœ¼ë¡œ í™•ì¥í•˜ì„¸ìš”.</li>
    <li>ì„¤ì¹˜ ë¬¸ì œ ì¡°ê±´ì„ ë¬¸ì„œí™”í•´ ëŒ€ì‘ ì‹œê°„ì„ ì¤„ì´ì„¸ìš”.</li>
  </ul>
</section>
<nav class="page-nav"></nav>
</main>

<aside class="inline-toc">
  <div class="toc-title">ëª©ì°¨</div>
  <div class="toc-nav"></div>
</aside>
<footer class="site-footer"></footer>

</div>
<script src="../js/main.js"></script>
</body>
</html>
