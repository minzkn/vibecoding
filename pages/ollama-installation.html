<!DOCTYPE html>
<html lang="ko" data-theme="dark-kernel">
<head>
<script>
(function(){var m=document.cookie.match(/claude_theme=([^;]+)/);if(m)document.documentElement.setAttribute('data-theme',m[1]);})();
</script>
<meta charset="UTF-8">
<meta property="og:type" content="article">
<meta property="og:site_name" content="AI Vibe Coding ê°€ì´ë“œ /with MINZKN">
<meta property="og:title" content="Ollama ì„¤ì¹˜">
<meta property="og:description" content="Ollama ì„¤ì¹˜: ëª¨ë“  ìš´ì˜ì²´ì œì—ì„œ Ollamaë¥¼ ì„¤ì¹˜í•˜ê³  ì²« ë²ˆì§¸ LLM ëª¨ë¸ ì‹¤í–‰í•˜ê¸°">
<meta property="og:url" content="https://minzkn.com/claude/pages/ollama-installation.html">
<meta property="og:image" content="https://minzkn.com/claude/images/og-image.png">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="Ollama ì„¤ì¹˜: ëª¨ë“  ìš´ì˜ì²´ì œì—ì„œ Ollamaë¥¼ ì„¤ì¹˜í•˜ê³  ì²« ë²ˆì§¸ LLM ëª¨ë¸ ì‹¤í–‰í•˜ê¸°">
<meta name="keywords" content="Claude, AI, LLM, Ollama ì„¤ì¹˜, ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­, Linux ì„¤ì¹˜, macOS ì„¤ì¹˜, Windows ì„¤ì¹˜">
<meta name="author" content="MINZKN">
<title>Ollama ì„¤ì¹˜ - AI Vibe Coding ê°€ì´ë“œ /with MINZKN</title>
<link rel="icon" type="image/svg+xml" href="../images/favicon.svg">
<link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/joungkyun/font-d2coding/d2coding.css">
<link rel="stylesheet" href="../css/themes.css">
<link rel="stylesheet" href="../css/style.css">
<link rel="stylesheet" href="../css/responsive.css">
</head>
<body>
<div class="page-wrapper">

<header class="site-header"></header>
<nav class="side-nav" aria-label="ì‚¬ì´íŠ¸ ë‚´ë¹„ê²Œì´ì…˜"></nav>

<main class="main-content">
<nav class="breadcrumb"></nav>

<h1 id="top">Ollama ì„¤ì¹˜</h1>
<p class="lead">ëª¨ë“  ìš´ì˜ì²´ì œì—ì„œ Ollamaë¥¼ ì„¤ì¹˜í•˜ê³  ì²« ë²ˆì§¸ LLM ëª¨ë¸ ì‹¤í–‰í•˜ê¸°</p>

<div class="info-box warning">
  <strong>ì—…ë°ì´íŠ¸ ì•ˆë‚´:</strong> ëª¨ë¸/ìš”ê¸ˆ/ë²„ì „/ì •ì±… ë“± ì‹œì ì— ë¯¼ê°í•œ ì •ë³´ëŠ” ë³€ë™ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
  ìµœì‹  ë‚´ìš©ì€ ê³µì‹ ë¬¸ì„œë¥¼ í™•ì¸í•˜ì„¸ìš”.
</div>

<div class="info-box tip">
  <div class="info-box-title">âš¡ ë¹ ë¥¸ ì„¤ì¹˜</div>
  <p><strong>Linux/macOS (1ë¶„):</strong></p>
  <pre><code>curl -fsSL https://ollama.com/install.sh | sh</code></pre>
  <p><strong>Windows:</strong></p>
  <ol>
    <li><a href="https://ollama.com/download/windows" target="_blank">ollama.com/download/windows</a>ì—ì„œ ì„¤ì¹˜ íŒŒì¼ ë‹¤ìš´ë¡œë“œ</li>
    <li>ì‹¤í–‰ íŒŒì¼ ë”ë¸”í´ë¦­</li>
    <li>Next â†’ Install â†’ Finish</li>
  </ol>
</div>

<section class="content-section">
  <h2 id="system-requirements">ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­</h2>

  <h3 id="minimum-specs">ìµœì†Œ ì‚¬ì–‘</h3>

  <table>
    <thead>
      <tr>
        <th>í•­ëª©</th>
        <th>ìµœì†Œ ìš”êµ¬ì‚¬í•­</th>
        <th>ê¶Œì¥ ì‚¬ì–‘</th>
        <th>ì´ìƒì  í™˜ê²½</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>RAM</strong></td>
        <td>8GB (3B ëª¨ë¸)</td>
        <td>16GB (7B ëª¨ë¸)</td>
        <td>32GB+ (13B+ ëª¨ë¸)</td>
      </tr>
      <tr>
        <td><strong>ë””ìŠ¤í¬</strong></td>
        <td>10GB ì—¬ìœ  ê³µê°„</td>
        <td>50GB ì—¬ìœ  ê³µê°„</td>
        <td>100GB+ SSD</td>
      </tr>
      <tr>
        <td><strong>CPU</strong></td>
        <td>x86_64 ë˜ëŠ” ARM64</td>
        <td>4ì½”ì–´ ì´ìƒ</td>
        <td>8ì½”ì–´+ (AVX2 ì§€ì›)</td>
      </tr>
      <tr>
        <td><strong>GPU</strong></td>
        <td>ì„ íƒì‚¬í•­ (CPUë§Œìœ¼ë¡œë„ ë™ì‘)</td>
        <td>NVIDIA GPU (6GB+ VRAM)</td>
        <td>NVIDIA GPU (12GB+ VRAM)</td>
      </tr>
      <tr>
        <td><strong>OS</strong></td>
        <td colspan="3">macOS 11+, Windows 10+, Linux (kernel 4.x+)</td>
      </tr>
    </tbody>
  </table>

  <div class="info-box info">
    <div class="info-box-title">ğŸ’¡ ëª¨ë¸ë³„ ë©”ëª¨ë¦¬ ìš”êµ¬ì‚¬í•­</div>
    <ul>
      <li><strong>1-3B ëª¨ë¸</strong> (Phi-3 Mini, Gemma 2B): 8GB RAM</li>
      <li><strong>7B ëª¨ë¸</strong> (Llama 3.2 7B, Mistral 7B): 16GB RAM</li>
      <li><strong>13B ëª¨ë¸</strong> (CodeLlama 13B): 24GB RAM</li>
      <li><strong>34B+ ëª¨ë¸</strong> (Mixtral 8x7B, Llama 70B): 48GB+ RAM ë˜ëŠ” GPU</li>
    </ul>
    <p><strong>íŒ</strong>: ì–‘ìí™”(Q4_0)ë¥¼ ì‚¬ìš©í•˜ë©´ ë©”ëª¨ë¦¬ ìš”êµ¬ëŸ‰ì´ ì ˆë°˜ìœ¼ë¡œ ì¤„ì–´ë“­ë‹ˆë‹¤.</p>
  </div>

  <h3 id="gpu-support">GPU ì§€ì›</h3>

  <table>
    <thead>
      <tr>
        <th>í”Œë«í¼</th>
        <th>GPU</th>
        <th>ìë™ ê°ì§€</th>
        <th>ë¹„ê³ </th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>macOS</strong></td>
        <td>Apple Silicon (M1/M2/M3)</td>
        <td>âœ… ìë™</td>
        <td>Metal ê°€ì† (ë§¤ìš° ë¹ ë¦„)</td>
      </tr>
      <tr>
        <td><strong>Linux</strong></td>
        <td>NVIDIA (CUDA 11.8+)</td>
        <td>âœ… ìë™</td>
        <td>ë“œë¼ì´ë²„ ì„¤ì¹˜ í•„ìš”</td>
      </tr>
      <tr>
        <td><strong>Linux</strong></td>
        <td>AMD (ROCm 5.7+)</td>
        <td>âš ï¸ ìˆ˜ë™ ì„¤ì •</td>
        <td>ì‹¤í—˜ì  ì§€ì›</td>
      </tr>
      <tr>
        <td><strong>Windows</strong></td>
        <td>NVIDIA (CUDA 11.8+)</td>
        <td>âœ… ìë™</td>
        <td>WSL2 ì‚¬ìš© ê¶Œì¥</td>
      </tr>
    </tbody>
  </table>

  <div class="info-box warning">
    <div class="info-box-title">âš ï¸ GPU ì—†ì–´ë„ ê´œì°®ìŠµë‹ˆë‹¤!</div>
    <p>
      OllamaëŠ” CPUë§Œìœ¼ë¡œë„ ì¶©ë¶„íˆ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤. GPUëŠ” ì†ë„ë¥¼ 2-10ë°° í–¥ìƒì‹œí‚¤ì§€ë§Œ,
      3B-7B ëª¨ë¸ì€ CPUì—ì„œë„ ì‹¤ìš©ì ì¸ ì†ë„ë¡œ ë™ì‘í•©ë‹ˆë‹¤.
    </p>
    <ul>
      <li><strong>CPU (16GB RAM)</strong>: Llama 3.2 7B â†’ 10-20 tokens/sec</li>
      <li><strong>GPU (RTX 3060 12GB)</strong>: Llama 3.2 7B â†’ 50-100 tokens/sec</li>
    </ul>
  </div>
</section>

<section class="content-section">
  <h2 id="linux">Linux ì„¤ì¹˜</h2>

  <h3 id="linux-auto">ìë™ ì„¤ì¹˜ (ê¶Œì¥)</h3>

  <p>ê³µì‹ ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‚¬ìš©í•˜ë©´ 1ë¶„ ì•ˆì— ì„¤ì¹˜ ì™„ë£Œ:</p>

  <pre><code><span class="cmt"># Ollama ì„¤ì¹˜</span>
curl -fsSL https://ollama.com/install.sh | sh

<span class="cmt"># ì„¤ì¹˜ í™•ì¸</span>
ollama --version
<span class="cmt"># ollama version 0.1.x</span>

<span class="cmt"># ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸</span>
systemctl status ollama
<span class="cmt"># â— ollama.service - Ollama Service</span>
<span class="cmt">#    Loaded: loaded</span>
<span class="cmt">#    Active: active (running)</span></code></pre>

  <p>ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸ëŠ” ë‹¤ìŒ ì‘ì—…ì„ ìë™ìœ¼ë¡œ ìˆ˜í–‰í•©ë‹ˆë‹¤:</p>
  <ul>
    <li>Ollama ë°”ì´ë„ˆë¦¬ ë‹¤ìš´ë¡œë“œ (<code>/usr/local/bin/ollama</code>)</li>
    <li><code>ollama</code> ì‚¬ìš©ì ë° ê·¸ë£¹ ìƒì„±</li>
    <li>systemd ì„œë¹„ìŠ¤ íŒŒì¼ ìƒì„± (<code>/etc/systemd/system/ollama.service</code>)</li>
    <li>ì„œë¹„ìŠ¤ ìë™ ì‹œì‘ ì„¤ì •</li>
    <li>GPU ë“œë¼ì´ë²„ ê°ì§€ (NVIDIA CUDA, AMD ROCm)</li>
  </ul>

  <h3 id="linux-manual">ìˆ˜ë™ ì„¤ì¹˜</h3>

  <p>íŠ¹ì • ë²„ì „ì„ ì„¤ì¹˜í•˜ê±°ë‚˜ ì»¤ìŠ¤í…€ ì„¤ì •ì´ í•„ìš”í•œ ê²½ìš°:</p>

  <pre><code><span class="cmt"># 1. ë°”ì´ë„ˆë¦¬ ë‹¤ìš´ë¡œë“œ</span>
sudo curl -L https://ollama.com/download/ollama-linux-amd64 -o /usr/local/bin/ollama
sudo chmod +x /usr/local/bin/ollama

<span class="cmt"># 2. ì‚¬ìš©ì ë° ê·¸ë£¹ ìƒì„±</span>
sudo useradd -r -s /bin/false -m -d /usr/share/ollama ollama

<span class="cmt"># 3. systemd ì„œë¹„ìŠ¤ íŒŒì¼ ìƒì„±</span>
sudo tee /etc/systemd/system/ollama.service &lt;&lt;EOF
[Unit]
Description=Ollama Service
After=network-online.target

[Service]
ExecStart=/usr/local/bin/ollama serve
User=ollama
Group=ollama
Restart=always
RestartSec=3
Environment="OLLAMA_HOST=0.0.0.0:11434"

[Install]
WantedBy=default.target
EOF

<span class="cmt"># 4. ì„œë¹„ìŠ¤ ì‹œì‘</span>
sudo systemctl daemon-reload
sudo systemctl enable ollama
sudo systemctl start ollama</code></pre>

  <h3 id="linux-nvidia">NVIDIA GPU ì„¤ì • (Linux)</h3>

  <p>CUDAë¥¼ ì„¤ì¹˜í•˜ë©´ Ollamaê°€ ìë™ìœ¼ë¡œ GPUë¥¼ ê°ì§€í•©ë‹ˆë‹¤:</p>

  <pre><code><span class="cmt"># 1. NVIDIA ë“œë¼ì´ë²„ ì„¤ì¹˜ (Ubuntu/Debian)</span>
sudo apt update
sudo apt install nvidia-driver-535

<span class="cmt"># 2. CUDA Toolkit ì„¤ì¹˜ (ì„ íƒì‚¬í•­)</span>
wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.1-1_all.deb
sudo dpkg -i cuda-keyring_1.1-1_all.deb
sudo apt update
sudo apt install cuda-toolkit-12-3

<span class="cmt"># 3. ì¬ë¶€íŒ…</span>
sudo reboot

<span class="cmt"># 4. GPU í™•ì¸</span>
nvidia-smi
<span class="cmt"># +-----------------------------------------------------------------------------+</span>
<span class="cmt"># | NVIDIA-SMI 535.x       Driver Version: 535.x       CUDA Version: 12.2     |</span>

<span class="cmt"># 5. Ollamaì—ì„œ GPU ì‚¬ìš© í™•ì¸</span>
ollama run llama3.2
<span class="cmt"># GPUê°€ ê°ì§€ë˜ë©´ ìë™ìœ¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤</span></code></pre>

  <h3 id="linux-distros">ë°°í¬íŒë³„ ì„¤ì¹˜</h3>

  <h4>Ubuntu / Debian</h4>
  <pre><code>curl -fsSL https://ollama.com/install.sh | sh</code></pre>

  <h4>Fedora / RHEL / CentOS</h4>
  <pre><code>curl -fsSL https://ollama.com/install.sh | sh</code></pre>

  <h4>Arch Linux</h4>
  <pre><code><span class="cmt"># AUR ì‚¬ìš©</span>
yay -S ollama

<span class="cmt"># ë˜ëŠ” ê³µì‹ ìŠ¤í¬ë¦½íŠ¸</span>
curl -fsSL https://ollama.com/install.sh | sh</code></pre>

  <h4>Alpine Linux</h4>
  <pre><code><span class="cmt"># ìˆ˜ë™ ì„¤ì¹˜ (ê³µì‹ ìŠ¤í¬ë¦½íŠ¸ê°€ Alpine ë¯¸ì§€ì›)</span>
wget https://ollama.com/download/ollama-linux-amd64
sudo mv ollama-linux-amd64 /usr/local/bin/ollama
sudo chmod +x /usr/local/bin/ollama</code></pre>
</section>

<section class="content-section">
  <h2 id="macos">macOS ì„¤ì¹˜</h2>

  <h3 id="macos-download">ë‹¤ìš´ë¡œë“œ ë° ì„¤ì¹˜</h3>

  <p><strong>ë°©ë²• 1: ê³µì‹ ì•± ë‹¤ìš´ë¡œë“œ (ê¶Œì¥)</strong></p>
  <ol>
    <li><a href="https://ollama.com/download/mac" target="_blank">ollama.com/download/mac</a> ì ‘ì†</li>
    <li><code>Ollama-darwin.zip</code> ë‹¤ìš´ë¡œë“œ</li>
    <li>ì••ì¶• í•´ì œ í›„ <code>Ollama.app</code>ë¥¼ <code>Applications</code> í´ë”ë¡œ ì´ë™</li>
    <li><code>Ollama.app</code> ì‹¤í–‰ â†’ ë©”ë‰´ë°”ì— ë¼ë§ˆ ì•„ì´ì½˜ í‘œì‹œë¨</li>
  </ol>

  <p><strong>ë°©ë²• 2: ëª…ë ¹ì¤„ ì„¤ì¹˜</strong></p>
  <pre><code><span class="cmt"># ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰</span>
curl -fsSL https://ollama.com/install.sh | sh

<span class="cmt"># í™•ì¸</span>
ollama --version</code></pre>

  <p><strong>ë°©ë²• 3: Homebrew</strong></p>
  <pre><code><span class="cmt"># Homebrewë¡œ ì„¤ì¹˜</span>
brew install ollama

<span class="cmt"># ì„œë¹„ìŠ¤ ì‹œì‘</span>
brew services start ollama</code></pre>

  <h3 id="macos-apple-silicon">Apple Silicon (M1/M2/M3) ìµœì í™”</h3>

  <p>
    OllamaëŠ” Apple Siliconì—ì„œ <strong>Metal</strong> ê°€ì†ì„ ìë™ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
    ë³„ë„ ì„¤ì • ì—†ì´ GPU ê°€ì†ì´ ì ìš©ë©ë‹ˆë‹¤!
  </p>

  <div class="info-box tip">
    <div class="info-box-title">ğŸš€ Apple Silicon ì„±ëŠ¥</div>
    <ul>
      <li><strong>M1 (8GB)</strong>: Llama 3.2 3B â†’ 40-60 tokens/sec</li>
      <li><strong>M1 Pro (16GB)</strong>: Llama 3.2 7B â†’ 30-50 tokens/sec</li>
      <li><strong>M2 Max (32GB)</strong>: Llama 3.1 13B â†’ 20-40 tokens/sec</li>
      <li><strong>M3 Max (64GB)</strong>: Mixtral 8x7B â†’ 15-30 tokens/sec</li>
    </ul>
    <p>Metal ê°€ì†ìœ¼ë¡œ NVIDIA GPUì™€ ë¹„ìŠ·í•˜ê±°ë‚˜ ë” ë¹ ë¥¸ ì†ë„!</p>
  </div>

  <pre><code><span class="cmt"># Metal ì‚¬ìš© í™•ì¸</span>
ollama run llama3.2 <span class="str">"Hello"</span>
<span class="cmt"># ë¡œê·¸ì— "Metal" ë˜ëŠ” "GPU"ê°€ í‘œì‹œë˜ë©´ ê°€ì† ì‚¬ìš© ì¤‘</span>

<span class="cmt"># ë©”ëª¨ë¦¬ ì••ë ¥ ëª¨ë‹ˆí„°ë§</span>
<span class="kw">while</span> <span class="kw">true</span>; <span class="kw">do</span>
  vm_stat | grep <span class="str">"Pages active"</span>
  sleep 1
<span class="kw">done</span></code></pre>

  <h3 id="macos-menu-bar">ë©”ë‰´ë°” ì•± ì‚¬ìš©ë²•</h3>

  <p>Ollama.appì„ ì‹¤í–‰í•˜ë©´ ë©”ë‰´ë°”ì— ë¼ë§ˆ ì•„ì´ì½˜ì´ í‘œì‹œë©ë‹ˆë‹¤:</p>

  <ul>
    <li><strong>ì•„ì´ì½˜ í´ë¦­</strong> â†’ ëª¨ë¸ ëª©ë¡, ì‹¤í–‰ ì¤‘ì¸ ëª¨ë¸ í™•ì¸</li>
    <li><strong>Run a model</strong> â†’ ëª¨ë¸ ì„ íƒí•˜ì—¬ í„°ë¯¸ë„ì—ì„œ ì‹¤í–‰</li>
    <li><strong>Pull a model</strong> â†’ ìƒˆ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ</li>
    <li><strong>Quit Ollama</strong> â†’ ì„œë¹„ìŠ¤ ì¢…ë£Œ</li>
  </ul>

  <p>ë°±ê·¸ë¼ìš´ë“œ ì„œë¹„ìŠ¤ëŠ” ë¡œê·¸ì¸ ì‹œ ìë™ ì‹œì‘ë©ë‹ˆë‹¤.</p>
</section>

<section class="content-section">
  <h2 id="windows">Windows ì„¤ì¹˜</h2>

  <h3 id="windows-download">ì„¤ì¹˜ í”„ë¡œê·¸ë¨ ë‹¤ìš´ë¡œë“œ</h3>

  <ol>
    <li><a href="https://ollama.com/download/windows" target="_blank">ollama.com/download/windows</a> ì ‘ì†</li>
    <li><code>OllamaSetup.exe</code> ë‹¤ìš´ë¡œë“œ</li>
    <li>ë‹¤ìš´ë¡œë“œí•œ íŒŒì¼ ë”ë¸”í´ë¦­</li>
    <li>UAC í”„ë¡¬í”„íŠ¸ í—ˆìš© (ê´€ë¦¬ì ê¶Œí•œ í•„ìš”)</li>
    <li>ì„¤ì¹˜ ë§ˆë²•ì‚¬ ì§„í–‰:
      <ul>
        <li>License Agreement ë™ì˜</li>
        <li>ì„¤ì¹˜ ê²½ë¡œ ì„ íƒ (ê¸°ë³¸: <code>C:\Users\[ì‚¬ìš©ì]\AppData\Local\Programs\Ollama</code>)</li>
        <li>Install í´ë¦­</li>
      </ul>
    </li>
    <li>ì„¤ì¹˜ ì™„ë£Œ â†’ <strong>ì‹œì‘ ë©”ë‰´</strong>ì— Ollama ì¶”ê°€ë¨</li>
  </ol>

  <h3 id="windows-verify">ì„¤ì¹˜ í™•ì¸</h3>

  <pre><code><span class="cmt"># PowerShell ë˜ëŠ” CMD ì—´ê¸°</span>
ollama --version
<span class="cmt"># ollama version 0.1.x</span>

<span class="cmt"># ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸</span>
Get-Service Ollama
<span class="cmt"># Status   Name               DisplayName</span>
<span class="cmt"># ------   ----               -----------</span>
<span class="cmt"># Running  Ollama             Ollama Service</span></code></pre>

  <h3 id="windows-wsl2">WSL2 ì‚¬ìš© (Linux í™˜ê²½, ê¶Œì¥)</h3>

  <p>
    Windowsì—ì„œ ìµœìƒì˜ ì„±ëŠ¥ì„ ìœ„í•´ì„œëŠ” <strong>WSL2 + Ubuntu</strong>ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.
    íŠ¹íˆ NVIDIA GPUê°€ ìˆëŠ” ê²½ìš° WSL2ì—ì„œ CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
  </p>

  <h4>1. WSL2 ì„¤ì¹˜</h4>
  <pre><code><span class="cmt"># PowerShell (ê´€ë¦¬ì ê¶Œí•œ)</span>
wsl --install
<span class="cmt"># ì¬ë¶€íŒ…</span>

<span class="cmt"># Ubuntu ì„¤ì¹˜ í™•ì¸</span>
wsl -l -v
<span class="cmt">#   NAME            STATE           VERSION</span>
<span class="cmt"># * Ubuntu-22.04    Running         2</span></code></pre>

  <h4>2. WSL2 ë‚´ì—ì„œ Ollama ì„¤ì¹˜</h4>
  <pre><code><span class="cmt"># WSL2 (Ubuntu) í„°ë¯¸ë„ ì—´ê¸°</span>
wsl

<span class="cmt"># Ollama ì„¤ì¹˜</span>
curl -fsSL https://ollama.com/install.sh | sh

<span class="cmt"># í™•ì¸</span>
ollama --version</code></pre>

  <h4>3. NVIDIA GPU ì§€ì› (WSL2 + CUDA)</h4>
  <pre><code><span class="cmt"># 1. Windowsì— NVIDIA ë“œë¼ì´ë²„ ì„¤ì¹˜ (í˜¸ìŠ¤íŠ¸)</span>
<span class="cmt">#    https://www.nvidia.com/Download/index.aspx</span>

<span class="cmt"># 2. WSL2ì—ì„œ CUDA ì‚¬ìš© ê°€ëŠ¥ í™•ì¸</span>
nvidia-smi
<span class="cmt"># GPU ì •ë³´ í‘œì‹œë˜ë©´ ì„±ê³µ</span>

<span class="cmt"># 3. Ollamaì—ì„œ ìë™ìœ¼ë¡œ GPU ì‚¬ìš©</span>
ollama run llama3.2</code></pre>

  <div class="info-box warning">
    <div class="info-box-title">âš ï¸ Windows ë„¤ì´í‹°ë¸Œ vs WSL2</div>
    <table>
      <thead>
        <tr>
          <th>í•­ëª©</th>
          <th>Windows ë„¤ì´í‹°ë¸Œ</th>
          <th>WSL2</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td><strong>ì„¤ì¹˜ ë‚œì´ë„</strong></td>
          <td>ì‰¬ì›€ (GUI)</td>
          <td>ì¤‘ê°„ (CLI)</td>
        </tr>
        <tr>
          <td><strong>ì„±ëŠ¥</strong></td>
          <td>ë³´í†µ</td>
          <td>ë¹ ë¦„ (ë„¤ì´í‹°ë¸Œ Linux)</td>
        </tr>
        <tr>
          <td><strong>GPU ì§€ì›</strong></td>
          <td>ì‹¤í—˜ì  (CUDA ì„¤ì • ë³µì¡)</td>
          <td>ì™„ì „ ì§€ì› (CUDA ìë™)</td>
        </tr>
        <tr>
          <td><strong>ê¶Œì¥</strong></td>
          <td>ì´ˆë³´ì, GPU ì—†ìŒ</td>
          <td>ê°œë°œì, GPU ìˆìŒ</td>
        </tr>
      </tbody>
    </table>
  </div>

  <h3 id="windows-path">í™˜ê²½ ë³€ìˆ˜ ì„¤ì •</h3>

  <p>ëª…ë ¹ í”„ë¡¬í”„íŠ¸ ì–´ë””ì„œë‚˜ <code>ollama</code> ëª…ë ¹ì–´ë¥¼ ì‚¬ìš©í•˜ë ¤ë©´:</p>

  <ol>
    <li><strong>ì‹œìŠ¤í…œ ì†ì„±</strong> â†’ <strong>ê³ ê¸‰</strong> â†’ <strong>í™˜ê²½ ë³€ìˆ˜</strong></li>
    <li><strong>ì‹œìŠ¤í…œ ë³€ìˆ˜</strong>ì—ì„œ <code>Path</code> ì„ íƒ â†’ <strong>í¸ì§‘</strong></li>
    <li><strong>ìƒˆë¡œ ë§Œë“¤ê¸°</strong> â†’ <code>C:\Users\[ì‚¬ìš©ì]\AppData\Local\Programs\Ollama\bin</code> ì¶”ê°€</li>
    <li>í™•ì¸ â†’ CMD ì¬ì‹œì‘</li>
  </ol>

  <p>ì„¤ì¹˜ í”„ë¡œê·¸ë¨ì´ ìë™ìœ¼ë¡œ Pathë¥¼ ì¶”ê°€í•˜ë¯€ë¡œ ë³´í†µ ìˆ˜ë™ ì„¤ì •ì€ ë¶ˆí•„ìš”í•©ë‹ˆë‹¤.</p>
</section>

<section class="content-section">
  <h2 id="docker">Docker ì„¤ì¹˜ (ì„ íƒì‚¬í•­)</h2>

  <p>
    Dockerë¥¼ ì‚¬ìš©í•˜ë©´ ê²©ë¦¬ëœ í™˜ê²½ì—ì„œ Ollamaë¥¼ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    ì„œë²„ ë°°í¬, í…ŒìŠ¤íŠ¸ í™˜ê²½ì— ì í•©í•©ë‹ˆë‹¤.
  </p>

  <h3 id="docker-cpu">CPU ì „ìš©</h3>

  <pre><code><span class="cmt"># Ollama ì»¨í…Œì´ë„ˆ ì‹¤í–‰</span>
docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama

<span class="cmt"># ëª¨ë¸ ë‹¤ìš´ë¡œë“œ</span>
docker exec -it ollama ollama pull llama3.2

<span class="cmt"># ëŒ€í™” ì‹œì‘</span>
docker exec -it ollama ollama run llama3.2</code></pre>

  <h3 id="docker-gpu">GPU ì§€ì› (NVIDIA)</h3>

  <pre><code><span class="cmt"># NVIDIA Container Toolkit ì„¤ì¹˜ (Linux)</span>
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list

sudo apt update
sudo apt install -y nvidia-container-toolkit
sudo systemctl restart docker

<span class="cmt"># GPUë¡œ Ollama ì‹¤í–‰</span>
docker run -d --gpus=all -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama</code></pre>

  <h3 id="docker-compose">Docker Compose</h3>

  <pre><code><span class="cmt"># docker-compose.yml</span>
<span class="kw">version</span>: <span class="str">'3.8'</span>
<span class="kw">services</span>:
  <span class="kw">ollama</span>:
    <span class="kw">image</span>: ollama/ollama:latest
    <span class="kw">container_name</span>: ollama
    <span class="kw">ports</span>:
      - <span class="str">"11434:11434"</span>
    <span class="kw">volumes</span>:
      - ollama:/root/.ollama
    <span class="kw">deploy</span>:
      <span class="kw">resources</span>:
        <span class="kw">reservations</span>:
          <span class="kw">devices</span>:
            - <span class="kw">driver</span>: nvidia
              <span class="kw">count</span>: 1
              <span class="kw">capabilities</span>: [gpu]

<span class="kw">volumes</span>:
  <span class="kw">ollama</span>:</code></pre>

  <pre><code><span class="cmt"># ì‹¤í–‰</span>
docker-compose up -d

<span class="cmt"># ëª¨ë¸ ë‹¤ìš´ë¡œë“œ</span>
docker-compose exec ollama ollama pull llama3.2</code></pre>
</section>

<section class="content-section">
  <h2 id="first-model">ì²« ë²ˆì§¸ ëª¨ë¸ ì‹¤í–‰</h2>

  <p>ì„¤ì¹˜ê°€ ì™„ë£Œë˜ì—ˆìœ¼ë©´ ì²« ë²ˆì§¸ ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•˜ê³  ì‹¤í–‰í•´ë´…ì‹œë‹¤!</p>

  <h3 id="pull-model">ëª¨ë¸ ë‹¤ìš´ë¡œë“œ</h3>

  <pre><code><span class="cmt"># Llama 3.2 (3B, ê°€ì¥ ì‘ê³  ë¹ ë¦„) ë‹¤ìš´ë¡œë“œ</span>
ollama pull llama3.2

<span class="cmt"># ë‹¤ìš´ë¡œë“œ ì§„í–‰ ìƒí™©</span>
pulling manifest
pulling 3b6e8c9b9a5d... 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– 2.0 GB
pulling 87d7562b0aa3... 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– 1.4 KB
pulling a47b0e3d78c0... 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  251 B
pulling af9f6c30658f... 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  487 B
verifying sha256 digest
writing manifest
removing any unused layers
success</code></pre>

  <p>ë‹¤ìš´ë¡œë“œ ì‹œê°„: ì¸í„°ë„· ì†ë„ì— ë”°ë¼ 1-5ë¶„ ì†Œìš” (2GB)</p>

  <h3 id="run-model">ëª¨ë¸ ì‹¤í–‰</h3>

  <pre><code><span class="cmt"># ëŒ€í™”í˜• ëª¨ë“œë¡œ ì‹¤í–‰</span>
ollama run llama3.2

<span class="cmt"># í”„ë¡¬í”„íŠ¸ í‘œì‹œ</span>
<span class="str">&gt;&gt;&gt; </span></code></pre>

  <p>ì´ì œ ëŒ€í™”ë¥¼ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!</p>

  <h3 id="first-conversation">ì²« ë²ˆì§¸ ëŒ€í™” ì˜ˆì œ</h3>

  <pre><code><span class="str">&gt;&gt;&gt; ì•ˆë…•! Pythonìœ¼ë¡œ í”¼ë³´ë‚˜ì¹˜ ìˆ˜ì—´ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜ë¥¼ ë§Œë“¤ì–´ì¤˜</span>

Here's a simple implementation of a Fibonacci sequence generator in Python:

<span class="kw">def</span> <span class="fn">fibonacci</span>(n):
    <span class="str">"""
    Generate Fibonacci sequence up to n terms
    """</span>
    fib_sequence = []
    a, b = <span class="num">0</span>, <span class="num">1</span>

    <span class="kw">for</span> _ <span class="kw">in</span> range(n):
        fib_sequence.append(a)
        a, b = b, a + b

    <span class="kw">return</span> fib_sequence

<span class="cmt"># Example usage</span>
<span class="kw">print</span>(fibonacci(<span class="num">10</span>))
<span class="cmt"># Output: [0, 1, 1, 2, 3, 5, 8, 13, 21, 34]</span>

<span class="str">&gt;&gt;&gt; ì¬ê·€ í•¨ìˆ˜ë¡œë„ ë§Œë“¤ì–´ì¤˜</span>

Sure! Here's a recursive version:

<span class="kw">def</span> <span class="fn">fibonacci_recursive</span>(n):
    <span class="kw">if</span> n &lt;= <span class="num">1</span>:
        <span class="kw">return</span> n
    <span class="kw">return</span> fibonacci_recursive(n-<span class="num">1</span>) + fibonacci_recursive(n-<span class="num">2</span>)

<span class="cmt"># Generate first 10 Fibonacci numbers</span>
result = [fibonacci_recursive(i) <span class="kw">for</span> i <span class="kw">in</span> range(<span class="num">10</span>)]
<span class="kw">print</span>(result)

<span class="str">&gt;&gt;&gt; /bye</span></code></pre>

  <h3 id="cli-commands">ìœ ìš©í•œ CLI ëª…ë ¹ì–´</h3>

  <pre><code><span class="cmt"># ì„¤ì¹˜ëœ ëª¨ë¸ ëª©ë¡</span>
ollama list
<span class="cmt"># NAME              ID              SIZE      MODIFIED</span>
<span class="cmt"># llama3.2:latest   3b6e8c9b9a5d    2.0 GB    2 minutes ago</span>

<span class="cmt"># ë‹¨ì¼ í”„ë¡¬í”„íŠ¸ ì‹¤í–‰ (ëŒ€í™”í˜• ì•„ë‹˜)</span>
ollama run llama3.2 <span class="str">"Explain quantum computing in one sentence"</span>

<span class="cmt"># ëª¨ë¸ ì‚­ì œ</span>
ollama rm llama3.2

<span class="cmt"># ì‹¤í–‰ ì¤‘ì¸ ëª¨ë¸ í™•ì¸</span>
ollama ps
<span class="cmt"># NAME              ID              SIZE      PROCESSOR    UNTIL</span>
<span class="cmt"># llama3.2:latest   3b6e8c9b9a5d    2.0 GB    100% GPU     4 minutes from now</span>

<span class="cmt"># ëª¨ë¸ ì •ë³´ í™•ì¸</span>
ollama show llama3.2</code></pre>
</section>

<section class="content-section">
  <h2 id="configuration">ê³ ê¸‰ ì„¤ì •</h2>

  <h3 id="env-vars">í™˜ê²½ ë³€ìˆ˜</h3>

  <p>OllamaëŠ” í™˜ê²½ ë³€ìˆ˜ë¡œ ë™ì‘ì„ ì œì–´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:</p>

  <table>
    <thead>
      <tr>
        <th>ë³€ìˆ˜ëª…</th>
        <th>ê¸°ë³¸ê°’</th>
        <th>ì„¤ëª…</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><code>OLLAMA_HOST</code></td>
        <td><code>127.0.0.1:11434</code></td>
        <td>ì„œë²„ ì£¼ì†Œ (ì›ê²© ì ‘ì† ì‹œ 0.0.0.0)</td>
      </tr>
      <tr>
        <td><code>OLLAMA_MODELS</code></td>
        <td><code>~/.ollama/models</code></td>
        <td>ëª¨ë¸ ì €ì¥ ê²½ë¡œ</td>
      </tr>
      <tr>
        <td><code>OLLAMA_NUM_PARALLEL</code></td>
        <td><code>1</code></td>
        <td>ë™ì‹œ ì²˜ë¦¬í•  ìš”ì²­ ìˆ˜</td>
      </tr>
      <tr>
        <td><code>OLLAMA_MAX_LOADED_MODELS</code></td>
        <td><code>1</code></td>
        <td>ë©”ëª¨ë¦¬ì— ë¡œë“œí•  ëª¨ë¸ ìˆ˜</td>
      </tr>
      <tr>
        <td><code>OLLAMA_KEEP_ALIVE</code></td>
        <td><code>5m</code></td>
        <td>ëª¨ë¸ ì–¸ë¡œë“œ ëŒ€ê¸° ì‹œê°„</td>
      </tr>
      <tr>
        <td><code>OLLAMA_DEBUG</code></td>
        <td><code>false</code></td>
        <td>ë””ë²„ê·¸ ë¡œê·¸ í™œì„±í™”</td>
      </tr>
    </tbody>
  </table>

  <h4>Linux/macOS</h4>
  <pre><code><span class="cmt"># ~/.bashrc ë˜ëŠ” ~/.zshrcì— ì¶”ê°€</span>
<span class="kw">export</span> OLLAMA_HOST=<span class="str">"0.0.0.0:11434"</span>
<span class="kw">export</span> OLLAMA_MODELS=<span class="str">"/data/ollama/models"</span>
<span class="kw">export</span> OLLAMA_NUM_PARALLEL=<span class="num">4</span>

<span class="cmt"># systemd ì„œë¹„ìŠ¤ í™˜ê²½ ë³€ìˆ˜ (Linux)</span>
sudo systemctl edit ollama
<span class="cmt"># ë‹¤ìŒ ë‚´ìš© ì¶”ê°€:</span>
[Service]
Environment=<span class="str">"OLLAMA_HOST=0.0.0.0:11434"</span>

sudo systemctl restart ollama</code></pre>

  <h4>Windows</h4>
  <pre><code><span class="cmt"># PowerShell (ì˜êµ¬ ì„¤ì •)</span>
[System.Environment]::SetEnvironmentVariable(<span class="str">'OLLAMA_HOST'</span>, <span class="str">'0.0.0.0:11434'</span>, <span class="str">'User'</span>)

<span class="cmt"># CMD (í˜„ì¬ ì„¸ì…˜ë§Œ)</span>
<span class="kw">set</span> OLLAMA_HOST=0.0.0.0:11434</code></pre>

  <h3 id="remote-access">ì›ê²© ì ‘ì† ì„¤ì •</h3>

  <p>ë‹¤ë¥¸ ì»´í“¨í„°ì—ì„œ Ollama APIì— ì ‘ê·¼í•˜ë ¤ë©´:</p>

  <pre><code><span class="cmt"># 1. OLLAMA_HOSTë¥¼ 0.0.0.0ìœ¼ë¡œ ì„¤ì • (ëª¨ë“  ì¸í„°í˜ì´ìŠ¤)</span>
<span class="kw">export</span> OLLAMA_HOST=<span class="str">"0.0.0.0:11434"</span>

<span class="cmt"># 2. ë°©í™”ë²½ í¬íŠ¸ ì—´ê¸° (Linux)</span>
sudo ufw allow <span class="num">11434</span>/tcp

<span class="cmt"># 3. ì¬ì‹œì‘</span>
sudo systemctl restart ollama

<span class="cmt"># 4. ë‹¤ë¥¸ ì»´í“¨í„°ì—ì„œ ì ‘ì†</span>
curl http://192.168.1.100:11434/api/generate -d <span class="str">'{
  "model": "llama3.2",
  "prompt": "Hello"
}'</span></code></pre>

  <div class="info-box warning">
    <div class="info-box-title">âš ï¸ ë³´ì•ˆ ì£¼ì˜</div>
    <p>
      ì›ê²© ì ‘ì†ì„ í™œì„±í™”í•˜ë©´ ë„¤íŠ¸ì›Œí¬ì— ìˆëŠ” ëˆ„êµ¬ë‚˜ Ollama APIì— ì ‘ê·¼í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
      í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œëŠ” ì¸ì¦, HTTPS, ë°©í™”ë²½ ì„¤ì •ì„ ë°˜ë“œì‹œ ì¶”ê°€í•˜ì„¸ìš”.
    </p>
  </div>

  <h3 id="storage-location">ëª¨ë¸ ì €ì¥ ìœ„ì¹˜ ë³€ê²½</h3>

  <p>ë””ìŠ¤í¬ ê³µê°„ì´ ë¶€ì¡±í•˜ë©´ ëª¨ë¸ì„ ë‹¤ë¥¸ ë“œë¼ì´ë¸Œì— ì €ì¥:</p>

  <pre><code><span class="cmt"># 1. ìƒˆ ë””ë ‰í† ë¦¬ ìƒì„±</span>
sudo mkdir -p /mnt/storage/ollama

<span class="cmt"># 2. ê¶Œí•œ ì„¤ì •</span>
sudo chown -R ollama:ollama /mnt/storage/ollama

<span class="cmt"># 3. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •</span>
sudo systemctl edit ollama
<span class="cmt"># ì¶”ê°€:</span>
[Service]
Environment=<span class="str">"OLLAMA_MODELS=/mnt/storage/ollama"</span>

<span class="cmt"># 4. ì¬ì‹œì‘</span>
sudo systemctl restart ollama</code></pre>
</section>

<section class="content-section">
  <h2 id="troubleshooting">íŠ¸ëŸ¬ë¸”ìŠˆíŒ…</h2>

  <h3 id="common-issues">ì¼ë°˜ì ì¸ ë¬¸ì œ</h3>

  <h4>1. "ollama: command not found"</h4>
  <pre><code><span class="cmt"># ì›ì¸: PATHì— ollamaê°€ ì—†ìŒ</span>
<span class="cmt"># í•´ê²°: ì„¤ì¹˜ í™•ì¸</span>
which ollama
<span class="cmt"># /usr/local/bin/ollama (ìˆì–´ì•¼ í•¨)</span>

<span class="cmt"># ì—†ìœ¼ë©´ ì¬ì„¤ì¹˜</span>
curl -fsSL https://ollama.com/install.sh | sh</code></pre>

  <h4>2. "connection refused" (í¬íŠ¸ 11434 ì ‘ì† ë¶ˆê°€)</h4>
  <pre><code><span class="cmt"># ì›ì¸: Ollama ì„œë¹„ìŠ¤ ë¯¸ì‹¤í–‰</span>
<span class="cmt"># í•´ê²°: ì„œë¹„ìŠ¤ ì‹œì‘</span>
<span class="cmt"># Linux</span>
sudo systemctl start ollama

<span class="cmt"># macOS</span>
ollama serve &amp;

<span class="cmt"># í™•ì¸</span>
curl http://localhost:11434/api/tags</code></pre>

  <h4>3. GPU ë¯¸ê°ì§€ (NVIDIA)</h4>
  <pre><code><span class="cmt"># í™•ì¸: nvidia-smi ì‹¤í–‰</span>
nvidia-smi
<span class="cmt"># ì—ëŸ¬ ë°œìƒ ì‹œ â†’ ë“œë¼ì´ë²„ ì¬ì„¤ì¹˜</span>

<span class="cmt"># CUDA ë²„ì „ í™•ì¸</span>
nvcc --version
<span class="cmt"># 11.8 ì´ìƒ í•„ìš”</span>

<span class="cmt"># Ollama ì¬ì‹œì‘</span>
sudo systemctl restart ollama</code></pre>

  <h4>4. ë©”ëª¨ë¦¬ ë¶€ì¡± (OOM)</h4>
  <pre><code><span class="cmt"># ì›ì¸: ëª¨ë¸ì´ RAMë³´ë‹¤ í¼</span>
<span class="cmt"># í•´ê²°: ë” ì‘ì€ ëª¨ë¸ ì‚¬ìš©</span>
ollama pull phi3:mini   <span class="cmt"># 3B, 8GB RAM</span>
ollama pull llama3.2    <span class="cmt"># 3B, 8GB RAM</span>

<span class="cmt"># ë˜ëŠ” ì–‘ìí™” ë²„ì „</span>
ollama pull llama3.2:7b-q4_0  <span class="cmt"># 4GB</span></code></pre>

  <h3 id="logs">ë¡œê·¸ í™•ì¸</h3>

  <pre><code><span class="cmt"># Linux (systemd)</span>
sudo journalctl -u ollama -f

<span class="cmt"># macOS</span>
tail -f ~/.ollama/logs/server.log

<span class="cmt"># Docker</span>
docker logs -f ollama</code></pre>
</section>

<section class="content-section">
  <h2 id="next-steps">ë‹¤ìŒ ë‹¨ê³„</h2>

  <p>Ollama ì„¤ì¹˜ë¥¼ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤! ì´ì œ ë‹¤ì–‘í•œ ëª¨ë¸ì„ íƒìƒ‰í•´ë´…ì‹œë‹¤.</p>

  <div class="info-box info">
    <div class="info-box-title">ğŸ“š ê³„ì† í•™ìŠµí•˜ê¸°</div>
    <ol>
      <li><a href="ollama-models.html">ëª¨ë¸ ê°€ì´ë“œ</a> - Llama, Mistral, CodeLlama ë¹„êµ ë° ì„ íƒ</li>
      <li><a href="ollama-usage.html">ê¸°ë³¸ ì‚¬ìš©ë²•</a> - CLI ëª…ë ¹ì–´, API ì‚¬ìš©, ê³ ê¸‰ ì˜µì…˜</li>
      <li><a href="ollama-integration.html">ë„êµ¬ ì—°ë™</a> - Continue, Aiderì™€ í†µí•©í•˜ì—¬ ì½”ë”© ì‹œì‘</li>
    </ol>
  </div>
</section>

<section class="content-section">
  <h2 id="summary">í•µì‹¬ ì •ë¦¬</h2>
  <ul>
    <li>Ollama ì„¤ì¹˜ì˜ í•µì‹¬ ê°œë…ê³¼ íë¦„ì„ ì •ë¦¬í•©ë‹ˆë‹¤.</li>
    <li>ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ë¥¼ ë‹¨ê³„ë³„ë¡œ ì´í•´í•©ë‹ˆë‹¤.</li>
    <li>ì‹¤ì „ ì ìš© ì‹œ ê¸°ì¤€ê³¼ ì£¼ì˜ì ì„ í™•ì¸í•©ë‹ˆë‹¤.</li>
  </ul>
</section>

<section class="content-section">
  <h2 id="practice-tips">ì‹¤ë¬´ íŒ</h2>
  <ul>
    <li>ì…ë ¥/ì¶œë ¥ ì˜ˆì‹œë¥¼ ê³ ì •í•´ ì¬í˜„ì„±ì„ í™•ë³´í•˜ì„¸ìš”.</li>
    <li>Ollama ì„¤ì¹˜ ë²”ìœ„ë¥¼ ì‘ê²Œ ì¡ê³  ë‹¨ê³„ì ìœ¼ë¡œ í™•ì¥í•˜ì„¸ìš”.</li>
    <li>ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ ì¡°ê±´ì„ ë¬¸ì„œí™”í•´ ëŒ€ì‘ ì‹œê°„ì„ ì¤„ì´ì„¸ìš”.</li>
  </ul>
</section>
<nav class="page-nav"></nav>
</main>

<aside class="inline-toc">
  <div class="toc-title">ëª©ì°¨</div>
  <div class="toc-nav"></div>
</aside>
<footer class="site-footer"></footer>

</div>
<script src="../js/main.js"></script>
</body>
</html>
