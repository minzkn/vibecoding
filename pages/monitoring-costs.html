<!DOCTYPE html>
<html lang="ko" data-theme="dark-kernel">
<head>
<!-- BEGIN: Google adsense -->
<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-2110881342960271" crossorigin="anonymous"></script>
<!-- END: Google adsense -->
<!-- BEGIN: Google adsense repair -->
<script async src="https://fundingchoicesmessages.google.com/i/pub-2110881342960271?ers=1" nonce="laI6FT8gpRxugDJv5AGJRA"></script><script nonce="laI6FT8gpRxugDJv5AGJRA">(function() {function signalGooglefcPresent() {if (!window.frames['googlefcPresent']) {if (document.body) {const iframe = document.createElement('iframe'); iframe.style = 'width: 0; height: 0; border: none; z-index: -1000; left: -1000px; top: -1000px;'; iframe.style.display = 'none'; iframe.name = 'googlefcPresent'; document.body.appendChild(iframe);} else {setTimeout(signalGooglefcPresent, 0);}}}signalGooglefcPresent();})();</script>
<!-- END: Google adsense repair -->
<!-- BEGIN: Google analytics -->
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-1VWQF060SX"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-1VWQF060SX');
</script>
<!-- END: Google analytics -->

<script>
(function(){var m=document.cookie.match(/claude_theme=([^;]+)/);if(m)document.documentElement.setAttribute('data-theme',m[1]);})();
</script>
<meta charset="UTF-8">
<meta property="og:type" content="article">
<meta property="og:site_name" content="AI Vibe Coding ê°€ì´ë“œ /with MINZKN">
<meta property="og:title" content="LLM API ë¹„ìš© ëª¨ë‹ˆí„°ë§">
<meta property="og:description" content="LLM API ë¹„ìš© ëª¨ë‹ˆí„°ë§: Claude, GPT, Gemini API ë¹„ìš©ì„ ì¶”ì í•˜ê³  ìµœì í™”í•˜ëŠ” ìš´ì˜ ê°€ì´ë“œ">
<meta property="og:url" content="https://minzkn.com/claude/pages/monitoring-costs.html">
<meta property="og:image" content="https://minzkn.com/claude/images/og-image.png">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="LLM API ë¹„ìš© ëª¨ë‹ˆí„°ë§: Claude, GPT, Gemini API ë¹„ìš©ì„ ì¶”ì í•˜ê³  ìµœì í™”í•˜ëŠ” ìš´ì˜ ê°€ì´ë“œ">
<meta name="keywords" content="Claude, AI, LLM, LLM API ë¹„ìš© ëª¨ë‹ˆí„°ë§, LLM ê°€ê²© êµ¬ì¡°, ì‚¬ìš©ëŸ‰ ì¶”ì , ì•Œë¦¼ ì„¤ì •, ë¹„ìš© ìµœì í™” ì „ëµ">
<meta name="author" content="MINZKN">
<title>LLM API ë¹„ìš© ëª¨ë‹ˆí„°ë§ - AI Vibe Coding ê°€ì´ë“œ /with MINZKN</title>
<link rel="icon" type="image/svg+xml" href="../images/favicon.svg">
<link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/joungkyun/font-d2coding/d2coding.css">
<link rel="stylesheet" href="../css/themes.css">
<link rel="stylesheet" href="../css/style.css">
<link rel="stylesheet" href="../css/responsive.css">
</head>
<body>
<div class="page-wrapper">

<header class="site-header"></header>
<nav class="side-nav" aria-label="ì‚¬ì´íŠ¸ ë‚´ë¹„ê²Œì´ì…˜"></nav>

<main class="main-content">
<div class="breadcrumb"></div>

<h1 id="top">LLM API ë¹„ìš© ëª¨ë‹ˆí„°ë§</h1>
<p class="lead">Claude, GPT, Gemini API ë¹„ìš©ì„ íš¨ê³¼ì ìœ¼ë¡œ ì¶”ì í•˜ê³  ìµœì í™”í•˜ëŠ” ì™„ë²½í•œ ê°€ì´ë“œ</p>

<div class="info-box warning">
  <strong>ì—…ë°ì´íŠ¸ ì•ˆë‚´:</strong> ëª¨ë¸/ìš”ê¸ˆ/ë²„ì „/ì •ì±… ë“± ì‹œì ì— ë¯¼ê°í•œ ì •ë³´ëŠ” ë³€ë™ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
  ìµœì‹  ë‚´ìš©ì€ ê³µì‹ ë¬¸ì„œë¥¼ í™•ì¸í•˜ì„¸ìš”.
</div>

<div class="info-box warning">
  <div class="info-box-title">âš ï¸ ë¹„ìš© ê´€ë¦¬ê°€ ì¤‘ìš”í•œ ì´ìœ </div>
  <ul>
    <li><strong>ì˜ˆìƒì¹˜ ëª»í•œ ì²­êµ¬</strong>: ë¬´ì œí•œ ì‚¬ìš© ì‹œ ìˆ˜ì²œ~ìˆ˜ë§Œ ë‹¬ëŸ¬ ì²­êµ¬ ê°€ëŠ¥</li>
    <li><strong>í…ŒìŠ¤íŠ¸ ì‹¤ìˆ˜</strong>: ë¬´í•œ ë£¨í”„ë‚˜ ëŒ€ëŸ‰ í˜¸ì¶œë¡œ ì¸í•œ ë¹„ìš© í­ì¦</li>
    <li><strong>ì•…ì˜ì  ì‚¬ìš©</strong>: API í‚¤ ìœ ì¶œ ì‹œ íƒ€ì¸ì˜ ì•…ìš©</li>
    <li><strong>ë¹„íš¨ìœ¨</strong>: ë¶ˆí•„ìš”í•˜ê²Œ ê¸´ ì»¨í…ìŠ¤íŠ¸ë‚˜ í° ëª¨ë¸ ì‚¬ìš©</li>
  </ul>
  <p><strong>ëª¨ë‹ˆí„°ë§ê³¼ ì•Œë¦¼ ì„¤ì •ì€ í•„ìˆ˜ì…ë‹ˆë‹¤!</strong></p>
</div>

<section class="content-section">
  <h2 id="pricing">LLM ê°€ê²© êµ¬ì¡°</h2>

  <h3 id="pricing-models">ì£¼ìš” ëª¨ë¸ ê°€ê²© (2024ë…„ ê¸°ì¤€)</h3>

  <table>
    <thead>
      <tr>
        <th>ì œê³µì</th>
        <th>ëª¨ë¸</th>
        <th>ì…ë ¥ (1M í† í°)</th>
        <th>ì¶œë ¥ (1M í† í°)</th>
        <th>ì»¨í…ìŠ¤íŠ¸</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td rowspan="3"><strong>Anthropic</strong></td>
        <td>Claude Opus (ê³ ì„±ëŠ¥)</td>
        <td>ë³€ë™</td>
        <td>ë³€ë™</td>
        <td>200K</td>
      </tr>
      <tr>
        <td>Claude Sonnet (ê· í˜•)</td>
        <td>ë³€ë™</td>
        <td>ë³€ë™</td>
        <td>200K</td>
      </tr>
      <tr>
        <td>Claude Haiku (ê²½ëŸ‰)</td>
        <td>ë³€ë™</td>
        <td>ë³€ë™</td>
        <td>200K</td>
      </tr>
      <tr>
        <td rowspan="4"><strong>OpenAI</strong></td>
        <td>GPT-4 Turbo</td>
        <td>ë³€ë™</td>
        <td>ë³€ë™</td>
        <td>128K</td>
      </tr>
      <tr>
        <td>GPT-4o</td>
        <td>ë³€ë™</td>
        <td>ë³€ë™</td>
        <td>128K</td>
      </tr>
      <tr>
        <td>GPT-4o mini</td>
        <td>ë³€ë™</td>
        <td>ë³€ë™</td>
        <td>128K</td>
      </tr>
      <tr>
        <td>o1-preview</td>
        <td>ë³€ë™</td>
        <td>ë³€ë™</td>
        <td>128K</td>
      </tr>
      <tr>
        <td rowspan="2"><strong>Google</strong></td>
        <td>Gemini 1.5 Pro</td>
        <td>ë³€ë™</td>
        <td>ë³€ë™</td>
        <td>2M</td>
      </tr>
      <tr>
        <td>Gemini 1.5 Flash</td>
        <td>ë³€ë™</td>
        <td>ë³€ë™</td>
        <td>1M</td>
      </tr>
      <tr>
        <td><strong>Ollama</strong></td>
        <td>ëª¨ë“  ëª¨ë¸</td>
        <td colspan="3">ë¬´ë£Œ (ë¡œì»¬ ì‹¤í–‰, í•˜ë“œì›¨ì–´ ë¹„ìš©ë§Œ)</td>
      </tr>
    </tbody>
  </table>

  <h3 id="pricing-calc">ë¹„ìš© ê³„ì‚°ê¸°</h3>

<pre><code><span class="kw">def</span> <span class="fn">calculate_cost</span>(
    model: str,
    input_tokens: int,
    output_tokens: int
) -> float:
    <span class="str">"""í† í° ìˆ˜ë¡œ ë¹„ìš© ê³„ì‚°"""</span>

    <span class="cmt"># ê°€ê²©í‘œ (ì…ë ¥, ì¶œë ¥ per 1M tokens)</span>
    prices = {
        <span class="str">"claude-<tier>"</span>: (15.00, 75.00),
        <span class="str">"claude-<tier>"</span>: (3.00, 15.00),
        <span class="str">"claude-<tier>"</span>: (0.80, 4.00),
        <span class="str">"gpt-4-turbo"</span>: (10.00, 30.00),
        <span class="str">"gpt-4o"</span>: (2.50, 10.00),
        <span class="str">"gpt-4o-mini"</span>: (0.15, 0.60),
        <span class="str">"gemini-1.5-pro"</span>: (3.50, 10.50),
        <span class="str">"gemini-1.5-flash"</span>: (0.075, 0.30),
    }

    <span class="kw">if</span> model <span class="kw">not in</span> prices:
        <span class="kw">raise</span> ValueError(f<span class="str">"Unknown model: {model}"</span>)

    input_price, output_price = prices[model]

    input_cost = (input_tokens / 1_000_000) * input_price
    output_cost = (output_tokens / 1_000_000) * output_price

    <span class="kw">return</span> input_cost + output_cost

<span class="cmt"># ì‚¬ìš© ì˜ˆì‹œ</span>
cost = calculate_cost(
    model=<span class="str">"claude-<tier>"</span>,
    input_tokens=5000,
    output_tokens=1000
)
print(f<span class="str">"ë¹„ìš©: ${cost:.4f}"</span>)
<span class="cmt"># ì¶œë ¥: ë¹„ìš©: ë³€ë™</span></code></pre>

  <h3 id="pricing-examples">ì‹¤ì œ ì‚¬ìš© ì˜ˆì‹œ</h3>

  <table>
    <thead>
      <tr>
        <th>ì‘ì—…</th>
        <th>ëª¨ë¸</th>
        <th>í† í° (ì…ë ¥/ì¶œë ¥)</th>
        <th>ë¹„ìš©</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>ê°„ë‹¨í•œ ì§ˆë¬¸</td>
        <td>Sonnet (ê· í˜•)</td>
        <td>100 / 50</td>
        <td>ë³€ë™</td>
      </tr>
      <tr>
        <td>ì½”ë“œ ë¦¬ë·° (1íŒŒì¼)</td>
        <td>Sonnet (ê· í˜•)</td>
        <td>2,000 / 500</td>
        <td>ë³€ë™</td>
      </tr>
      <tr>
        <td>ë¬¸ì„œ ìš”ì•½ (10í˜ì´ì§€)</td>
        <td>Haiku (ê²½ëŸ‰)</td>
        <td>8,000 / 300</td>
        <td>ë³€ë™</td>
      </tr>
      <tr>
        <td>PR ì „ì²´ ë¦¬ë·°</td>
        <td>Sonnet (ê· í˜•)</td>
        <td>15,000 / 3,000</td>
        <td>ë³€ë™</td>
      </tr>
      <tr>
        <td>ëŒ€ê·œëª¨ ì½”ë“œë² ì´ìŠ¤ ë¶„ì„</td>
        <td>Opus (ê³ ì„±ëŠ¥)</td>
        <td>100,000 / 5,000</td>
        <td>ë³€ë™</td>
      </tr>
    </tbody>
  </table>

  <div class="info-box info">
    <div class="info-box-title">ğŸ’¡ ë¹„ìš© ì ˆê° íŒ</div>
    <ul>
      <li><strong>ì‘ì€ ëª¨ë¸ ìš°ì„ </strong>: Haiku â†’ Sonnet â†’ Opus ìˆœìœ¼ë¡œ ì‹œë„</li>
      <li><strong>ìºì‹± í™œìš©</strong>: Prompt Cachingìœ¼ë¡œ ë°˜ë³µ ì»¨í…ìŠ¤íŠ¸ 90% ì ˆê°</li>
      <li><strong>ì»¨í…ìŠ¤íŠ¸ ìµœì†Œí™”</strong>: í•„ìš”í•œ ì •ë³´ë§Œ ì „ë‹¬</li>
      <li><strong>ì¶œë ¥ ê¸¸ì´ ì œí•œ</strong>: max_tokens ì ì ˆíˆ ì„¤ì •</li>
      <li><strong>ë¡œì»¬ ëª¨ë¸</strong>: Ollamaë¡œ ë¹„ìš© ì œë¡œí™” (í•˜ë“œì›¨ì–´ ë¹„ìš©ë§Œ)</li>
    </ul>
  </div>
</section>

<section class="content-section">
  <h2 id="tracking">ì‚¬ìš©ëŸ‰ ì¶”ì </h2>

  <h3 id="tracking-basic">ê¸°ë³¸ ë¡œê¹…</h3>

  <h4>Python ì˜ˆì œ</h4>

<pre><code><span class="kw">import</span> anthropic
<span class="kw">import</span> os
<span class="kw">import</span> json
<span class="kw">import</span> logging
<span class="kw">from</span> datetime <span class="kw">import</span> datetime

<span class="cmt"># ë¡œê¹… ì„¤ì •</span>
logging.basicConfig(
    filename=<span class="str">'llm_usage.log'</span>,
    level=logging.INFO,
    format=<span class="str">'%(asctime)s - %(message)s'</span>
)

<span class="kw">class</span> <span class="type">CostTracker</span>:
    <span class="kw">def</span> <span class="fn">__init__</span>(self):
        self.client = anthropic.Anthropic(
            api_key=os.getenv(<span class="str">"ANTHROPIC_API_KEY"</span>)
        )

    <span class="kw">def</span> <span class="fn">log_usage</span>(self, model, usage, cost):
        <span class="str">"""API ì‚¬ìš©ëŸ‰ ë¡œê¹…"""</span>
        log_entry = {
            <span class="str">"timestamp"</span>: datetime.utcnow().isoformat(),
            <span class="str">"model"</span>: model,
            <span class="str">"input_tokens"</span>: usage.input_tokens,
            <span class="str">"output_tokens"</span>: usage.output_tokens,
            <span class="str">"cost_usd"</span>: cost,
        }
        logging.info(json.dumps(log_entry))

    <span class="kw">def</span> <span class="fn">create_message</span>(self, model, messages, **kwargs):
        <span class="str">"""ë¹„ìš© ì¶”ì ê³¼ í•¨ê»˜ ë©”ì‹œì§€ ìƒì„±"""</span>
        message = self.client.messages.create(
            model=model,
            messages=messages,
            **kwargs
        )

        <span class="cmt"># ë¹„ìš© ê³„ì‚°</span>
        cost = calculate_cost(
            model=model,
            input_tokens=message.usage.input_tokens,
            output_tokens=message.usage.output_tokens
        )

        <span class="cmt"># ë¡œê¹…</span>
        self.log_usage(model, message.usage, cost)

        <span class="kw">return</span> message

<span class="cmt"># ì‚¬ìš©</span>
tracker = CostTracker()
message = tracker.create_message(
    model=<span class="str">"claude-<tier>"</span>,
    max_tokens=1024,
    messages=[{<span class="str">"role"</span>: <span class="str">"user"</span>, <span class="str">"content"</span>: <span class="str">"Hello!"</span>}]
)</code></pre>

  <h3 id="tracking-db">ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥</h3>

  <h4>SQLite ì˜ˆì œ</h4>

<pre><code><span class="kw">import</span> sqlite3
<span class="kw">from</span> datetime <span class="kw">import</span> datetime

<span class="kw">class</span> <span class="type">UsageDatabase</span>:
    <span class="kw">def</span> <span class="fn">__init__</span>(self, db_path=<span class="str">"llm_usage.db"</span>):
        self.conn = sqlite3.connect(db_path)
        self.create_table()

    <span class="kw">def</span> <span class="fn">create_table</span>(self):
        self.conn.execute(<span class="str">"""
            CREATE TABLE IF NOT EXISTS api_usage (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp TEXT NOT NULL,
                user_id TEXT,
                model TEXT NOT NULL,
                input_tokens INTEGER NOT NULL,
                output_tokens INTEGER NOT NULL,
                cost_usd REAL NOT NULL,
                request_id TEXT,
                metadata TEXT
            )
        """</span>)
        self.conn.commit()

    <span class="kw">def</span> <span class="fn">log_request</span>(
        self,
        model: str,
        input_tokens: int,
        output_tokens: int,
        cost: float,
        user_id: str = <span class="kw">None</span>,
        request_id: str = <span class="kw">None</span>,
        metadata: dict = <span class="kw">None</span>
    ):
        self.conn.execute(
            <span class="str">"""
            INSERT INTO api_usage
            (timestamp, user_id, model, input_tokens, output_tokens, cost_usd, request_id, metadata)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            """</span>,
            (
                datetime.utcnow().isoformat(),
                user_id,
                model,
                input_tokens,
                output_tokens,
                cost,
                request_id,
                json.dumps(metadata) <span class="kw">if</span> metadata <span class="kw">else</span> <span class="kw">None</span>
            )
        )
        self.conn.commit()

    <span class="kw">def</span> <span class="fn">get_daily_cost</span>(self, date: str = <span class="kw">None</span>) -> float:
        <span class="str">"""íŠ¹ì • ë‚ ì§œì˜ ì´ ë¹„ìš©"""</span>
        <span class="kw">if not</span> date:
            date = datetime.utcnow().strftime(<span class="str">"%Y-%m-%d"</span>)

        cursor = self.conn.execute(
            <span class="str">"SELECT SUM(cost_usd) FROM api_usage WHERE DATE(timestamp) = ?"</span>,
            (date,)
        )
        result = cursor.fetchone()[0]
        <span class="kw">return</span> result <span class="kw">or</span> 0.0

    <span class="kw">def</span> <span class="fn">get_user_usage</span>(self, user_id: str, days: int = 30):
        <span class="str">"""ì‚¬ìš©ìë³„ ì‚¬ìš©ëŸ‰ í†µê³„"""</span>
        cursor = self.conn.execute(
            <span class="str">"""
            SELECT
                model,
                COUNT(*) as requests,
                SUM(input_tokens) as total_input,
                SUM(output_tokens) as total_output,
                SUM(cost_usd) as total_cost
            FROM api_usage
            WHERE user_id = ?
              AND timestamp &gt;= datetime('now', '-' || ? || ' days')
            GROUP BY model
            """</span>,
            (user_id, days)
        )
        <span class="kw">return</span> cursor.fetchall()

<span class="cmt"># ì‚¬ìš©</span>
db = UsageDatabase()
db.log_request(
    model=<span class="str">"claude-<tier>"</span>,
    input_tokens=5000,
    output_tokens=1000,
    cost=0.03,
    user_id=<span class="str">"user123"</span>
)

print(f<span class="str">"Today's cost: ${db.get_daily_cost():.2f}"</span>)</code></pre>

  <h3 id="tracking-wrapper">API ë˜í¼ í´ë˜ìŠ¤</h3>

<pre><code><span class="kw">class</span> <span class="type">MonitoredAnthropicClient</span>:
    <span class="str">"""ë¹„ìš© ì¶”ì  ê¸°ëŠ¥ì´ ë‚´ì¥ëœ Anthropic í´ë¼ì´ì–¸íŠ¸"""</span>

    <span class="kw">def</span> <span class="fn">__init__</span>(self, api_key: str, db: UsageDatabase):
        self.client = anthropic.Anthropic(api_key=api_key)
        self.db = db

    <span class="kw">def</span> <span class="fn">messages_create</span>(
        self,
        user_id: str = <span class="kw">None</span>,
        **kwargs
    ) -> anthropic.types.Message:
        <span class="str">"""ë©”ì‹œì§€ ìƒì„± + ìë™ ë¡œê¹…"""</span>

        <span class="cmt"># API í˜¸ì¶œ</span>
        message = self.client.messages.create(**kwargs)

        <span class="cmt"># ë¹„ìš© ê³„ì‚°</span>
        cost = calculate_cost(
            model=kwargs[<span class="str">'model'</span>],
            input_tokens=message.usage.input_tokens,
            output_tokens=message.usage.output_tokens
        )

        <span class="cmt"># DB ë¡œê¹…</span>
        self.db.log_request(
            model=kwargs[<span class="str">'model'</span>],
            input_tokens=message.usage.input_tokens,
            output_tokens=message.usage.output_tokens,
            cost=cost,
            user_id=user_id,
            request_id=message.id
        )

        <span class="kw">return</span> message

<span class="cmt"># ì‚¬ìš©</span>
db = UsageDatabase()
client = MonitoredAnthropicClient(
    api_key=os.getenv(<span class="str">"ANTHROPIC_API_KEY"</span>),
    db=db
)

message = client.messages_create(
    model=<span class="str">"claude-<tier>"</span>,
    max_tokens=1024,
    messages=[{<span class="str">"role"</span>: <span class="str">"user"</span>, <span class="str">"content"</span>: <span class="str">"Hello!"</span>}],
    user_id=<span class="str">"user123"</span>
)</code></pre>
</section>

<section class="content-section">
  <h2 id="alerts">ì•Œë¦¼ ì„¤ì •</h2>

  <h3 id="alerts-threshold">ë¹„ìš© ì„ê³„ê°’ ì•Œë¦¼</h3>

  <h4>ì´ë©”ì¼ ì•Œë¦¼</h4>

<pre><code><span class="kw">import</span> smtplib
<span class="kw">from</span> email.mime.text <span class="kw">import</span> MIMEText

<span class="kw">class</span> <span class="type">CostAlerter</span>:
    <span class="kw">def</span> <span class="fn">__init__</span>(
        self,
        daily_limit: float = 100.0,
        monthly_limit: float = 1000.0,
        email_to: str = <span class="str">"admin@example.com"</span>
    ):
        self.daily_limit = daily_limit
        self.monthly_limit = monthly_limit
        self.email_to = email_to
        self.db = UsageDatabase()

    <span class="kw">def</span> <span class="fn">check_limits</span>(self):
        <span class="str">"""ë¹„ìš© ì œí•œ í™•ì¸ ë° ì•Œë¦¼"""</span>
        daily_cost = self.db.get_daily_cost()
        monthly_cost = self.get_monthly_cost()

        <span class="cmt"># ì¼ì¼ í•œë„ ì´ˆê³¼</span>
        <span class="kw">if</span> daily_cost &gt; self.daily_limit:
            self.send_alert(
                subject=<span class="str">"âš ï¸ Daily API cost limit exceeded"</span>,
                message=f<span class="str">"Daily cost: ${daily_cost:.2f} (limit: ${self.daily_limit})"</span>
            )

        <span class="cmt"># ì›”ê°„ í•œë„ ì´ˆê³¼</span>
        <span class="kw">if</span> monthly_cost &gt; self.monthly_limit:
            self.send_alert(
                subject=<span class="str">"ğŸš¨ Monthly API cost limit exceeded"</span>,
                message=f<span class="str">"Monthly cost: ${monthly_cost:.2f} (limit: ${self.monthly_limit})"</span>
            )

        <span class="cmt"># ê²½ê³  (80% ë„ë‹¬)</span>
        <span class="kw">if</span> daily_cost &gt; self.daily_limit * 0.8:
            self.send_alert(
                subject=<span class="str">"âš ï¸ Approaching daily limit"</span>,
                message=f<span class="str">"Daily cost: ${daily_cost:.2f} (80% of limit)"</span>
            )

    <span class="kw">def</span> <span class="fn">send_alert</span>(self, subject: str, message: str):
        <span class="str">"""ì´ë©”ì¼ ì•Œë¦¼ ì „ì†¡"""</span>
        msg = MIMEText(message)
        msg[<span class="str">'Subject'</span>] = subject
        msg[<span class="str">'From'</span>] = <span class="str">'alerts@example.com'</span>
        msg[<span class="str">'To'</span>] = self.email_to

        <span class="kw">with</span> smtplib.SMTP(<span class="str">'smtp.gmail.com'</span>, 587) <span class="kw">as</span> server:
            server.starttls()
            server.login(
                os.getenv(<span class="str">'SMTP_USER'</span>),
                os.getenv(<span class="str">'SMTP_PASS'</span>)
            )
            server.send_message(msg)

    <span class="kw">def</span> <span class="fn">get_monthly_cost</span>(self) -> float:
        cursor = self.db.conn.execute(
            <span class="str">"SELECT SUM(cost_usd) FROM api_usage WHERE strftime('%Y-%m', timestamp) = strftime('%Y-%m', 'now')"</span>
        )
        result = cursor.fetchone()[0]
        <span class="kw">return</span> result <span class="kw">or</span> 0.0

<span class="cmt"># ì£¼ê¸°ì ìœ¼ë¡œ ì‹¤í–‰ (cron ë˜ëŠ” ìŠ¤ì¼€ì¤„ëŸ¬)</span>
alerter = CostAlerter(daily_limit=50.0, monthly_limit=500.0)
alerter.check_limits()</code></pre>

  <h3 id="alerts-slack">Slack ì•Œë¦¼</h3>

<pre><code><span class="kw">import</span> requests

<span class="kw">def</span> <span class="fn">send_slack_alert</span>(webhook_url: str, message: str):
    <span class="str">"""Slackìœ¼ë¡œ ì•Œë¦¼ ì „ì†¡"""</span>
    payload = {
        <span class="str">"text"</span>: message,
        <span class="str">"blocks"</span>: [
            {
                <span class="str">"type"</span>: <span class="str">"section"</span>,
                <span class="str">"text"</span>: {
                    <span class="str">"type"</span>: <span class="str">"mrkdwn"</span>,
                    <span class="str">"text"</span>: message
                }
            }
        ]
    }

    response = requests.post(webhook_url, json=payload)
    response.raise_for_status()

<span class="cmt"># ì‚¬ìš©</span>
<span class="kw">if</span> daily_cost &gt; 100:
    send_slack_alert(
        webhook_url=os.getenv(<span class="str">"SLACK_WEBHOOK_URL"</span>),
        message=f<span class="str">"ğŸš¨ *API Cost Alert*\n\nDaily cost: ${daily_cost:.2f}\nLimit: ë³€ë™"</span>
    )</code></pre>

  <h3 id="alerts-realtime">ì‹¤ì‹œê°„ ì•Œë¦¼</h3>

<pre><code><span class="kw">class</span> <span class="type">RealtimeMonitor</span>:
    <span class="kw">def</span> <span class="fn">__init__</span>(self, threshold_per_request: float = 1.0):
        self.threshold = threshold_per_request

    <span class="kw">def</span> <span class="fn">monitor_request</span>(self, cost: float, model: str, user_id: str):
        <span class="str">"""ë‹¨ì¼ ìš”ì²­ì´ ì„ê³„ê°’ ì´ˆê³¼ ì‹œ ì¦‰ì‹œ ì•Œë¦¼"""</span>
        <span class="kw">if</span> cost &gt; self.threshold:
            self.alert_expensive_request(cost, model, user_id)

    <span class="kw">def</span> <span class="fn">alert_expensive_request</span>(self, cost: float, model: str, user_id: str):
        message = f<span class="str">"""
âš ï¸ ê³ ë¹„ìš© API ìš”ì²­ ê°ì§€

ì‚¬ìš©ì: {user_id}
ëª¨ë¸: {model}
ë¹„ìš©: ${cost:.2f}
ì„ê³„ê°’: ${self.threshold:.2f}

ì´ ë‹¨ì¼ ìš”ì²­ì´ ìš”ì²­ë‹¹ ë¹„ìš© ì„ê³„ê°’ì„ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤.
ì‚¬ìš© íŒ¨í„´ì„ ê²€í† í•´ ì£¼ì„¸ìš”.
        """</span>

        send_slack_alert(
            webhook_url=os.getenv(<span class="str">"SLACK_WEBHOOK_URL"</span>),
            message=message
        )</code></pre>
</section>

<section class="content-section">
  <h2 id="optimization">ë¹„ìš© ìµœì í™” ì „ëµ</h2>

  <h3 id="opt-caching">í”„ë¡¬í”„íŠ¸ ìºì‹±</h3>

  <div class="info-box tip">
    <div class="info-box-title">ğŸ’¡ Prompt Caching (Claude)</div>
    <p>
      ë°˜ë³µë˜ëŠ” ì»¨í…ìŠ¤íŠ¸ë¥¼ ìºì‹œí•˜ë©´ ì…ë ¥ í† í° ë¹„ìš©ì„ 90% ì ˆê°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
      ìºì‹œëŠ” 5ë¶„ê°„ ìœ ì§€ë˜ë©°, 1024 í† í° ì´ìƒì˜ ë¸”ë¡ì— ì ìš©ë©ë‹ˆë‹¤.
    </p>
  </div>

<pre><code><span class="cmt"># ìºì‹± ì—†ì´ (ë§¤ë²ˆ ì „ì²´ ë¹„ìš© ì§€ë¶ˆ)</span>
message = client.messages.create(
    model=<span class="str">"claude-<tier>"</span>,
    max_tokens=1024,
    system=<span class="str">"You are a code reviewer. [ê¸´ ê°€ì´ë“œë¼ì¸...]"</span>,  <span class="cmt"># 10,000 í† í°</span>
    messages=[{<span class="str">"role"</span>: <span class="str">"user"</span>, <span class="str">"content"</span>: <span class="str">"Review this code..."</span>}]
)
<span class="cmt"># ë¹„ìš©: ë³€ë™ (10,000 ì…ë ¥ í† í°)</span>

<span class="cmt"># ìºì‹± ì‚¬ìš© (ì²« ìš”ì²­ í›„ 90% ì ˆê°)</span>
message = client.messages.create(
    model=<span class="str">"claude-<tier>"</span>,
    max_tokens=1024,
    system=[
        {
            <span class="str">"type"</span>: <span class="str">"text"</span>,
            <span class="str">"text"</span>: <span class="str">"You are a code reviewer. [ê¸´ ê°€ì´ë“œë¼ì¸...]"</span>,
            <span class="str">"cache_control"</span>: {<span class="str">"type"</span>: <span class="str">"ephemeral"</span>}  <span class="cmt"># ìºì‹± í™œì„±í™”</span>
        }
    ],
    messages=[{<span class="str">"role"</span>: <span class="str">"user"</span>, <span class="str">"content"</span>: <span class="str">"Review this code..."</span>}]
)
<span class="cmt"># ì²« ìš”ì²­: ë³€ë™ (ìºì‹œ ì“°ê¸° ë¹„ìš© 25% ì¶”ê°€)</span>
<span class="cmt"># ì´í›„ ìš”ì²­: ë³€ë™ (ìºì‹œ ì½ê¸° ë¹„ìš© 90% ì ˆê°)</span></code></pre>

  <h3 id="opt-model">ëª¨ë¸ ì„ íƒ ì „ëµ</h3>

  <h4>ê³„ì¸µì  ëª¨ë¸ ì‚¬ìš©</h4>

<pre><code><span class="kw">def</span> <span class="fn">smart_generate</span>(prompt: str, complexity: str = <span class="str">"auto"</span>):
    <span class="str">"""ë³µì¡ë„ì— ë”°ë¼ ì ì ˆí•œ ëª¨ë¸ ì„ íƒ"""</span>

    <span class="kw">if</span> complexity == <span class="str">"auto"</span>:
        <span class="cmt"># ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±ìœ¼ë¡œ ë³µì¡ë„ íŒë‹¨</span>
        <span class="kw">if</span> len(prompt) &lt; 500:
            complexity = <span class="str">"simple"</span>
        <span class="kw">elif</span> <span class="str">"analyze"</span> <span class="kw">in</span> prompt.lower() <span class="kw">or</span> <span class="str">"complex"</span> <span class="kw">in</span> prompt.lower():
            complexity = <span class="str">"complex"</span>
        <span class="kw">else</span>:
            complexity = <span class="str">"medium"</span>

    <span class="cmt"># ëª¨ë¸ ì„ íƒ</span>
    model_map = {
        <span class="str">"simple"</span>: <span class="str">"claude-<tier>"</span>,      <span class="cmt"># ë³€ë™/ë³€ë™ per 1M</span>
        <span class="str">"medium"</span>: <span class="str">"claude-<tier>"</span>,     <span class="cmt"># ë³€ë™/ë³€ë™ per 1M</span>
        <span class="str">"complex"</span>: <span class="str">"claude-<tier>"</span>,      <span class="cmt"># ë³€ë™/ë³€ë™ per 1M</span>
    }

    model = model_map[complexity]

    message = client.messages.create(
        model=model,
        max_tokens=1024,
        messages=[{<span class="str">"role"</span>: <span class="str">"user"</span>, <span class="str">"content"</span>: prompt}]
    )

    <span class="kw">return</span> message

<span class="cmt"># ì‚¬ìš©</span>
<span class="cmt"># ê°„ë‹¨í•œ ì‘ì—… â†’ Haiku (ì €ë ´)</span>
response = smart_generate(<span class="str">"What is 2+2?"</span>)

<span class="cmt"># ë³µì¡í•œ ì‘ì—… â†’ Opus (ë¹„ì‹¸ì§€ë§Œ ì •í™•)</span>
response = smart_generate(<span class="str">"Analyze this complex algorithm..."</span>, complexity=<span class="str">"complex"</span>)</code></pre>

  <h3 id="opt-context">ì»¨í…ìŠ¤íŠ¸ ìµœì í™”</h3>

<pre><code><span class="kw">def</span> <span class="fn">summarize_context</span>(long_text: str, max_tokens: int = 2000) -> str:
    <span class="str">"""ê¸´ ì»¨í…ìŠ¤íŠ¸ë¥¼ ìš”ì•½í•˜ì—¬ í† í° ì ˆì•½"""</span>

    <span class="cmt"># ì´ë¯¸ ì¶©ë¶„íˆ ì§§ìœ¼ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜</span>
    estimated_tokens = len(long_text) / 4  <span class="cmt"># ëŒ€ëµì ì¸ ì¶”ì •</span>
    <span class="kw">if</span> estimated_tokens &lt; max_tokens:
        <span class="kw">return</span> long_text

    <span class="cmt"># Haikuë¡œ ìš”ì•½ (ì €ë ´)</span>
    message = client.messages.create(
        model=<span class="str">"claude-<tier>"</span>,
        max_tokens=max_tokens,
        messages=[{
            <span class="str">"role"</span>: <span class="str">"user"</span>,
            <span class="str">"content"</span>: f<span class="str">"Summarize the following in {max_tokens} tokens or less:\n\n{long_text}"</span>
        }]
    )

    <span class="kw">return</span> message.content[0].text

<span class="cmt"># ì‚¬ìš©</span>
large_codebase = read_all_files()  <span class="cmt"># 100,000 í† í°</span>
summary = summarize_context(large_codebase, max_tokens=5000)  <span class="cmt"># 5,000 í† í°ìœ¼ë¡œ ì¶•ì†Œ</span>

<span class="cmt"># ìš”ì•½ëœ ì»¨í…ìŠ¤íŠ¸ë¡œ ë©”ì¸ ì‘ì—… ìˆ˜í–‰</span>
result = client.messages.create(
    model=<span class="str">"claude-<tier>"</span>,
    max_tokens=2048,
    messages=[{
        <span class="str">"role"</span>: <span class="str">"user"</span>,
        <span class="str">"content"</span>: f<span class="str">"Based on this codebase summary:\n{summary}\n\nGenerate a README."</span>
    }]
)
<span class="cmt"># ì´ ë¹„ìš©: ìš”ì•½(ë³€ë™) + ë©”ì¸(ë³€ë™) = ë³€ë™</span>
<span class="cmt"># vs ì „ì²´ ì „ë‹¬: ë³€ë™ (3ë°° ì ˆê°!)</span></code></pre>

  <h3 id="opt-batch">ë°°ì¹˜ ì²˜ë¦¬</h3>

<pre><code><span class="kw">def</span> <span class="fn">batch_reviews</span>(files: list[str]) -> list[str]:
    <span class="str">"""ì—¬ëŸ¬ íŒŒì¼ì„ í•œ ë²ˆì— ë¦¬ë·°í•˜ì—¬ ì˜¤ë²„í—¤ë“œ ê°ì†Œ"""</span>

    <span class="cmt"># ê°œë³„ ìš”ì²­ (ë¹„íš¨ìœ¨)</span>
    <span class="cmt"># for file in files:</span>
    <span class="cmt">#     review(file)  # ê°ê° ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì „ì†¡</span>

    <span class="cmt"># ë°°ì¹˜ ìš”ì²­ (íš¨ìœ¨ì )</span>
    combined_content = <span class="str">"\n\n---\n\n"</span>.join([
        f<span class="str">"## File: {file}\n```\n{read_file(file)}\n```"</span>
        <span class="kw">for</span> file <span class="kw">in</span> files
    ])

    message = client.messages.create(
        model=<span class="str">"claude-<tier>"</span>,
        max_tokens=8192,
        system=<span class="str">"You are a code reviewer..."</span>,  <span class="cmt"># í•œ ë²ˆë§Œ ì „ì†¡</span>
        messages=[{
            <span class="str">"role"</span>: <span class="str">"user"</span>,
            <span class="str">"content"</span>: f<span class="str">"Review these {len(files)} files:\n\n{combined_content}"</span>
        }]
    )

    <span class="cmt"># íŒŒì¼ë³„ ë¦¬ë·° íŒŒì‹±</span>
    reviews = message.content[0].text.split(<span class="str">"## File:"</span>)
    <span class="kw">return</span> reviews[1:]  <span class="cmt"># ì²« ë²ˆì§¸ëŠ” ë¹ˆ ë¬¸ìì—´</span></code></pre>

  <h3 id="opt-local">ë¡œì»¬ ëª¨ë¸ í™œìš©</h3>

<pre><code><span class="kw">import</span> ollama

<span class="kw">def</span> <span class="fn">hybrid_generation</span>(prompt: str, require_accuracy: bool = <span class="kw">False</span>):
    <span class="str">"""ê°„ë‹¨í•œ ì‘ì—…ì€ Ollama, ë³µì¡í•œ ì‘ì—…ì€ Claude"""</span>

    <span class="kw">if not</span> require_accuracy:
        <span class="cmt"># Ollama ì‚¬ìš© (ë¬´ë£Œ)</span>
        response = ollama.generate(
            model=<span class="str">'llama3.2:3b'</span>,
            prompt=prompt
        )
        <span class="kw">return</span> response[<span class="str">'response'</span>]
    <span class="kw">else</span>:
        <span class="cmt"># Claude ì‚¬ìš© (ìœ ë£Œ, ë” ì •í™•)</span>
        message = client.messages.create(
            model=<span class="str">"claude-<tier>"</span>,
            max_tokens=1024,
            messages=[{<span class="str">"role"</span>: <span class="str">"user"</span>, <span class="str">"content"</span>: prompt}]
        )
        <span class="kw">return</span> message.content[0].text

<span class="cmt"># ì‚¬ìš©</span>
<span class="cmt"># ê°„ë‹¨í•œ ì§ˆë¬¸ â†’ Ollama (ë¬´ë£Œ)</span>
answer = hybrid_generation(<span class="str">"Explain what a for loop is"</span>)

<span class="cmt"># ì¤‘ìš”í•œ ì‘ì—… â†’ Claude (ìœ ë£Œ, ì •í™•)</span>
review = hybrid_generation(
    <span class="str">"Review this security-critical code..."</span>,
    require_accuracy=<span class="kw">True</span>
)</code></pre>
</section>

<section class="content-section">
  <h2 id="dashboard">ëŒ€ì‹œë³´ë“œ êµ¬ì¶•</h2>

  <h3 id="dash-streamlit">Streamlit ëŒ€ì‹œë³´ë“œ</h3>

  <h4>dashboard.py</h4>

<pre><code><span class="kw">import</span> streamlit <span class="kw">as</span> st
<span class="kw">import</span> pandas <span class="kw">as</span> pd
<span class="kw">import</span> plotly.express <span class="kw">as</span> px
<span class="kw">from</span> datetime <span class="kw">import</span> datetime, timedelta

st.set_page_config(page_title=<span class="str">"LLM Cost Dashboard"</span>, layout=<span class="str">"wide"</span>)

<span class="cmt"># ë°ì´í„° ë¡œë“œ</span>
db = UsageDatabase()

<span class="cmt"># ì œëª©</span>
st.title(<span class="str">"ğŸ¤– LLM API Cost Dashboard"</span>)

<span class="cmt"># ë‚ ì§œ ë²”ìœ„ ì„ íƒ</span>
col1, col2 = st.columns(2)
<span class="kw">with</span> col1:
    start_date = st.date_input(<span class="str">"Start Date"</span>, datetime.now() - timedelta(days=30))
<span class="kw">with</span> col2:
    end_date = st.date_input(<span class="str">"End Date"</span>, datetime.now())

<span class="cmt"># ì „ì²´ í†µê³„</span>
st.header(<span class="str">"ğŸ“Š Overall Statistics"</span>)

cursor = db.conn.execute(
    <span class="str">"""
    SELECT
        COUNT(*) as total_requests,
        SUM(input_tokens) as total_input,
        SUM(output_tokens) as total_output,
        SUM(cost_usd) as total_cost
    FROM api_usage
    WHERE DATE(timestamp) BETWEEN ? AND ?
    """</span>,
    (start_date.isoformat(), end_date.isoformat())
)
stats = cursor.fetchone()

col1, col2, col3, col4 = st.columns(4)
col1.metric(<span class="str">"Total Requests"</span>, f<span class="str">"{stats[0]:,}"</span>)
col2.metric(<span class="str">"Total Tokens"</span>, f<span class="str">"{(stats[1] + stats[2]):,}"</span>)
col3.metric(<span class="str">"Total Cost"</span>, f<span class="str">"${stats[3]:.2f}"</span>)
col4.metric(<span class="str">"Avg Cost/Request"</span>, f<span class="str">"${stats[3]/max(stats[0],1):.4f}"</span>)

<span class="cmt"># ì¼ë³„ ë¹„ìš© ê·¸ë˜í”„</span>
st.header(<span class="str">"ğŸ“ˆ Daily Cost Trend"</span>)

df = pd.read_sql_query(
    <span class="str">"""
    SELECT
        DATE(timestamp) as date,
        model,
        SUM(cost_usd) as cost
    FROM api_usage
    WHERE DATE(timestamp) BETWEEN ? AND ?
    GROUP BY DATE(timestamp), model
    ORDER BY date
    """</span>,
    db.conn,
    params=(start_date.isoformat(), end_date.isoformat())
)

<span class="kw">if not</span> df.empty:
    fig = px.line(
        df,
        x=<span class="str">"date"</span>,
        y=<span class="str">"cost"</span>,
        color=<span class="str">"model"</span>,
        title=<span class="str">"Daily Cost by Model"</span>
    )
    st.plotly_chart(fig, use_container_width=<span class="kw">True</span>)

<span class="cmt"># ëª¨ë¸ë³„ í†µê³„</span>
st.header(<span class="str">"ğŸ¤– Cost by Model"</span>)

model_stats = pd.read_sql_query(
    <span class="str">"""
    SELECT
        model,
        COUNT(*) as requests,
        SUM(cost_usd) as total_cost,
        AVG(cost_usd) as avg_cost
    FROM api_usage
    WHERE DATE(timestamp) BETWEEN ? AND ?
    GROUP BY model
    ORDER BY total_cost DESC
    """</span>,
    db.conn,
    params=(start_date.isoformat(), end_date.isoformat())
)

<span class="kw">if not</span> model_stats.empty:
    fig = px.pie(
        model_stats,
        values=<span class="str">"total_cost"</span>,
        names=<span class="str">"model"</span>,
        title=<span class="str">"Cost Distribution by Model"</span>
    )
    st.plotly_chart(fig, use_container_width=<span class="kw">True</span>)

    st.dataframe(model_stats, use_container_width=<span class="kw">True</span>)

<span class="cmt"># ì‚¬ìš©ìë³„ í†µê³„ (ìˆëŠ” ê²½ìš°)</span>
st.header(<span class="str">"ğŸ‘¤ Top Users by Cost"</span>)

user_stats = pd.read_sql_query(
    <span class="str">"""
    SELECT
        user_id,
        COUNT(*) as requests,
        SUM(cost_usd) as total_cost
    FROM api_usage
    WHERE user_id IS NOT NULL
      AND DATE(timestamp) BETWEEN ? AND ?
    GROUP BY user_id
    ORDER BY total_cost DESC
    LIMIT 10
    """</span>,
    db.conn,
    params=(start_date.isoformat(), end_date.isoformat())
)

<span class="kw">if not</span> user_stats.empty:
    st.dataframe(user_stats, use_container_width=<span class="kw">True</span>)
<span class="kw">else</span>:
    st.info(<span class="str">"No user data available"</span>)

<span class="cmt"># ìµœê·¼ ìš”ì²­</span>
st.header(<span class="str">"ğŸ• Recent Requests"</span>)

recent = pd.read_sql_query(
    <span class="str">"""
    SELECT
        timestamp,
        user_id,
        model,
        input_tokens,
        output_tokens,
        cost_usd
    FROM api_usage
    ORDER BY timestamp DESC
    LIMIT 50
    """</span>,
    db.conn
)

st.dataframe(recent, use_container_width=<span class="kw">True</span>)</code></pre>

  <h4>ì‹¤í–‰</h4>

<pre><code><span class="cmt"># ì˜ì¡´ì„± ì„¤ì¹˜</span>
pip install streamlit pandas plotly

<span class="cmt"># ëŒ€ì‹œë³´ë“œ ì‹¤í–‰</span>
streamlit run dashboard.py

<span class="cmt"># ë¸Œë¼ìš°ì €ì—ì„œ http://localhost:8501 ì—´ê¸°</span></code></pre>

  <h3 id="dash-grafana">Grafana + Prometheus</h3>

  <h4>prometheus.yml</h4>

<pre><code><span class="kw">global</span>:
  <span class="kw">scrape_interval</span>: 15s

<span class="kw">scrape_configs</span>:
  - <span class="kw">job_name</span>: <span class="str">'llm-api'</span>
    <span class="kw">static_configs</span>:
      - <span class="kw">targets</span>: [<span class="str">'localhost:8000'</span>]</code></pre>

  <h4>FastAPI ë©”íŠ¸ë¦­ ì—”ë“œí¬ì¸íŠ¸</h4>

<pre><code><span class="kw">from</span> fastapi <span class="kw">import</span> FastAPI
<span class="kw">from</span> prometheus_client <span class="kw">import</span> Counter, Histogram, generate_latest

app = FastAPI()

<span class="cmt"># ë©”íŠ¸ë¦­ ì •ì˜</span>
api_requests = Counter(
    <span class="str">'llm_api_requests_total'</span>,
    <span class="str">'Total API requests'</span>,
    [<span class="str">'model'</span>, <span class="str">'status'</span>]
)

api_cost = Counter(
    <span class="str">'llm_api_cost_usd_total'</span>,
    <span class="str">'Total cost in USD'</span>,
    [<span class="str">'model'</span>]
)

api_tokens = Counter(
    <span class="str">'llm_api_tokens_total'</span>,
    <span class="str">'Total tokens used'</span>,
    [<span class="str">'model'</span>, <span class="str">'type'</span>]  <span class="cmt"># type: input or output</span>
)

api_latency = Histogram(
    <span class="str">'llm_api_latency_seconds'</span>,
    <span class="str">'API request latency'</span>,
    [<span class="str">'model'</span>]
)

<span class="pp">@app.get</span>(<span class="str">"/metrics"</span>)
<span class="kw">def</span> <span class="fn">metrics</span>():
    <span class="kw">return</span> generate_latest()

<span class="pp">@app.post</span>(<span class="str">"/chat"</span>)
<span class="kw">async def</span> <span class="fn">chat</span>(request: ChatRequest):
    <span class="kw">with</span> api_latency.labels(model=request.model).time():
        <span class="kw">try</span>:
            message = client.messages.create(
                model=request.model,
                max_tokens=1024,
                messages=request.messages
            )

            <span class="cmt"># ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸</span>
            api_requests.labels(model=request.model, status=<span class="str">'success'</span>).inc()

            cost = calculate_cost(
                model=request.model,
                input_tokens=message.usage.input_tokens,
                output_tokens=message.usage.output_tokens
            )
            api_cost.labels(model=request.model).inc(cost)

            api_tokens.labels(model=request.model, type=<span class="str">'input'</span>).inc(message.usage.input_tokens)
            api_tokens.labels(model=request.model, type=<span class="str">'output'</span>).inc(message.usage.output_tokens)

            <span class="kw">return</span> {<span class="str">"response"</span>: message.content[0].text}

        <span class="kw">except</span> Exception <span class="kw">as</span> e:
            api_requests.labels(model=request.model, status=<span class="str">'error'</span>).inc()
            <span class="kw">raise</span></code></pre>
</section>

<section class="content-section">
  <h2 id="best-practices">ì¢…í•© ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤</h2>

  <h3 id="best-monitoring">ëª¨ë‹ˆí„°ë§</h3>

  <ul>
    <li><strong>ì‹¤ì‹œê°„ ì¶”ì </strong>: ëª¨ë“  API í˜¸ì¶œ ë¡œê¹…</li>
    <li><strong>ì¼ì¼ ë¦¬í¬íŠ¸</strong>: ë§¤ì¼ ë¹„ìš© ìš”ì•½ ì´ë©”ì¼</li>
    <li><strong>ëŒ€ì‹œë³´ë“œ</strong>: ì‹œê°í™”ëœ í†µê³„ í™•ì¸</li>
    <li><strong>ì•Œë¦¼ ì„¤ì •</strong>: ì„ê³„ê°’ ì´ˆê³¼ ì‹œ ì¦‰ì‹œ ì•Œë¦¼</li>
  </ul>

  <h3 id="best-optimization">ìµœì í™”</h3>

  <ul>
    <li><strong>ìºì‹±</strong>: ë°˜ë³µ ì»¨í…ìŠ¤íŠ¸ëŠ” Prompt Caching ì‚¬ìš©</li>
    <li><strong>ëª¨ë¸ ì„ íƒ</strong>: ì‘ì—…ì— ë§ëŠ” ìµœì†Œ ëª¨ë¸ ì‚¬ìš©</li>
    <li><strong>ì»¨í…ìŠ¤íŠ¸ ì¶•ì†Œ</strong>: ë¶ˆí•„ìš”í•œ ì •ë³´ ì œê±°</li>
    <li><strong>ë°°ì¹˜ ì²˜ë¦¬</strong>: ì—¬ëŸ¬ ì‘ì—…ì„ í•œ ë²ˆì— ì²˜ë¦¬</li>
    <li><strong>ë¡œì»¬ ëª¨ë¸</strong>: ê°„ë‹¨í•œ ì‘ì—…ì€ Ollama í™œìš©</li>
  </ul>

  <h3 id="best-budgeting">ì˜ˆì‚° ê´€ë¦¬</h3>

  <ul>
    <li><strong>ì¼ì¼/ì›”ê°„ í•œë„</strong>: ëª…í™•í•œ ì˜ˆì‚° ì„¤ì •</li>
    <li><strong>ì‚¬ìš©ìë³„ í• ë‹¹</strong>: íŒ€ì›ë³„ ì˜ˆì‚° ë¶„ë°°</li>
    <li><strong>ìë™ ì°¨ë‹¨</strong>: í•œë„ ì´ˆê³¼ ì‹œ API í˜¸ì¶œ ì¤‘ë‹¨</li>
    <li><strong>ì •ê¸° ë¦¬ë·°</strong>: ì£¼ê°„/ì›”ê°„ ë¹„ìš© ë¶„ì„</li>
  </ul>

  <div class="info-box success">
    <div class="info-box-title">âœ… ë¹„ìš© ê´€ë¦¬ ì²´í¬ë¦¬ìŠ¤íŠ¸</div>
    <ul>
      <li>ëª¨ë“  API í˜¸ì¶œì„ ë°ì´í„°ë² ì´ìŠ¤ì— ë¡œê¹…</li>
      <li>ì¼ì¼ ë¹„ìš© ì•Œë¦¼ ì„¤ì • (ì˜ˆ: ë³€ë™ ì´ˆê³¼ ì‹œ)</li>
      <li>ì›”ê°„ ì˜ˆì‚° ì„¤ì • ë° ëª¨ë‹ˆí„°ë§</li>
      <li>Prompt Caching ì ìš© (ë°˜ë³µ ì»¨í…ìŠ¤íŠ¸)</li>
      <li>ì‘ì—…ë³„ë¡œ ì ì ˆí•œ ëª¨ë¸ ì„ íƒ (Haiku/Sonnet/Opus)</li>
      <li>ëŒ€ì‹œë³´ë“œ êµ¬ì¶• (Streamlit ë˜ëŠ” Grafana)</li>
      <li>ì‚¬ìš©ìë³„ í• ë‹¹ëŸ‰ ì„¤ì •</li>
      <li>ì£¼ê°„ ë¹„ìš© ë¦¬í¬íŠ¸ ìë™í™”</li>
    </ul>
  </div>

  <div class="info-box tip">
    <div class="info-box-title">ğŸš€ ë‹¤ìŒ ë‹¨ê³„</div>
    <ul>
      <li><a href="api-key-management.html">API í‚¤ ê´€ë¦¬</a> - ë³´ì•ˆì„ í†µí•œ ë¹„ìš© ëˆ„ìˆ˜ ë°©ì§€</li>
      <li><a href="ci-cd-llm.html">CI/CD & LLM</a> - ìë™í™” íŒŒì´í”„ë¼ì¸ ë¹„ìš© ìµœì í™”</li>
      <li><a href="dev-environment.html">LLM ê°œë°œ í™˜ê²½</a> - ë¡œì»¬ ê°œë°œë¡œ ë¹„ìš© ì ˆê°</li>
      <li><a href="../pages/ollama-intro.html">Ollama ì†Œê°œ</a> - ë¬´ë£Œ ë¡œì»¬ LLM í™œìš©</li>
    </ul>
  </div>
</section>

<section class="content-section">
  <h2 id="summary">í•µì‹¬ ì •ë¦¬</h2>
  <ul>
    <li>LLM API ë¹„ìš© ëª¨ë‹ˆí„°ë§ì˜ í•µì‹¬ ê°œë…ê³¼ íë¦„ì„ ì •ë¦¬í•©ë‹ˆë‹¤.</li>
    <li>LLM ê°€ê²© êµ¬ì¡°ë¥¼ ë‹¨ê³„ë³„ë¡œ ì´í•´í•©ë‹ˆë‹¤.</li>
    <li>ì‹¤ì „ ì ìš© ì‹œ ê¸°ì¤€ê³¼ ì£¼ì˜ì ì„ í™•ì¸í•©ë‹ˆë‹¤.</li>
  </ul>
</section>

<section class="content-section">
  <h2 id="practice-tips">ì‹¤ë¬´ íŒ</h2>
  <ul>
    <li>ì…ë ¥/ì¶œë ¥ ì˜ˆì‹œë¥¼ ê³ ì •í•´ ì¬í˜„ì„±ì„ í™•ë³´í•˜ì„¸ìš”.</li>
    <li>LLM API ë¹„ìš© ëª¨ë‹ˆí„°ë§ ë²”ìœ„ë¥¼ ì‘ê²Œ ì¡ê³  ë‹¨ê³„ì ìœ¼ë¡œ í™•ì¥í•˜ì„¸ìš”.</li>
    <li>LLM ê°€ê²© êµ¬ì¡° ì¡°ê±´ì„ ë¬¸ì„œí™”í•´ ëŒ€ì‘ ì‹œê°„ì„ ì¤„ì´ì„¸ìš”.</li>
  </ul>
</section>
<nav class="page-nav"></nav>
</main>

<aside class="inline-toc">
  <div class="toc-title">ëª©ì°¨</div>
  <div class="toc-nav"></div>
</aside>
<footer class="site-footer"></footer>

</div>
<script src="../js/main.js"></script>
</body>
</html>
