<!DOCTYPE html>
<html lang="ko" data-theme="dark-kernel">
<head>
<!-- BEGIN: Google adsense -->
<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-2110881342960271" crossorigin="anonymous"></script>
<!-- END: Google adsense -->
<!-- BEGIN: Google adsense repair -->
<script async src="https://fundingchoicesmessages.google.com/i/pub-2110881342960271?ers=1" nonce="laI6FT8gpRxugDJv5AGJRA"></script><script nonce="laI6FT8gpRxugDJv5AGJRA">(function() {function signalGooglefcPresent() {if (!window.frames['googlefcPresent']) {if (document.body) {const iframe = document.createElement('iframe'); iframe.style = 'width: 0; height: 0; border: none; z-index: -1000; left: -1000px; top: -1000px;'; iframe.style.display = 'none'; iframe.name = 'googlefcPresent'; document.body.appendChild(iframe);} else {setTimeout(signalGooglefcPresent, 0);}}}signalGooglefcPresent();})();</script>
<!-- END: Google adsense repair -->
<!-- BEGIN: Google analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-1VWQF060SX"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-1VWQF060SX');
</script>
<!-- END: Google analytics -->

<!-- Flash 방지: 쿠키에서 테마 즉시 적용 -->
<script>
(function(){var m=document.cookie.match(/claude_theme=([^;]+)/);if(m)document.documentElement.setAttribute('data-theme',m[1]);})();
</script>
<meta charset="UTF-8">
<meta property="og:type" content="article">
<meta property="og:site_name" content="AI Vibe Coding 가이드 /with MINZKN">
<meta property="og:title" content="GPT를 밑바닥부터 (3): 학습·Adam·추론·LLM 비교 — microgpt.py 해설">
<meta property="og:description" content="microgpt.py의 Cross-Entropy 손실 계산, Adam 옵티마이저, 이름 생성 추론, MicroGPT vs 현대 LLM 비교를 해설합니다.">
<meta property="og:url" content="https://minzkn.com/claude/pages/microgpt-training.html">
<meta property="og:image" content="https://minzkn.com/claude/images/og-image.png">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="microgpt.py의 Cross-Entropy 손실 계산, Adam 옵티마이저, 이름 생성 추론, MicroGPT vs 현대 LLM 비교를 해설합니다.">
<meta name="keywords" content="microgpt gpt 학습 adam 옵티마이저 크로스엔트로피 추론 temperature llm 비교 nanogpt weight-tying karpathy">
<meta name="author" content="MINZKN">
<title>GPT를 밑바닥부터 (3): 학습·Adam·추론·LLM 비교 - AI Vibe Coding 가이드 /with MINZKN</title>
<link rel="icon" type="image/svg+xml" href="../images/favicon.svg">
<link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/joungkyun/font-d2coding/d2coding.css">
<link rel="stylesheet" href="../css/themes.css">
<link rel="stylesheet" href="../css/style.css">
<link rel="stylesheet" href="../css/responsive.css">
</head>
<body>
<div class="page-wrapper">

<!-- ===== Header ===== -->
<header class="site-header">
</header>

<!-- ===== Side Navigation ===== -->
<nav class="side-nav" aria-label="사이트 내비게이션">
</nav>

<!-- ===== Main Content ===== -->
<main class="main-content">
<div class="breadcrumb"></div>

<h1 id="top">GPT를 밑바닥부터 (3): 학습·Adam·추론·LLM 비교</h1>
<p class="page-description">microgpt.py의 Cross-Entropy 손실 계산, 완전한 학습 루프, Adam 옵티마이저, 이름 생성 추론, MicroGPT vs 현대 LLM 비교, 그리고 200줄 코드에서 얻은 교훈을 정리합니다.</p>

<section class="content-section">
  <h2 id="series">시리즈 안내</h2>
  <ul>
    <li><a href="microgpt-intro.html">1부: 저자 소개, 전체 흐름, 데이터 파이프라인, 토크나이저, Autograd 엔진</a></li>
    <li><a href="microgpt-architecture.html">2부: 모델 설계도, 임베딩, Attention, 트랜스포머 블록, GPT 파이프라인</a></li>
    <li><strong>3부 (현재)</strong>: 손실 계산, Adam 옵티마이저, 추론, LLM 비교, 요약</li>
  </ul>
</section>

<section class="content-section">
  <h2 id="loss">Cross-Entropy 손실 계산</h2>
  <p>Cross-Entropy는 정보 이론에서 나온 개념입니다. 모델의 예측 분포 P가 정답에서 얼마나 멀리 있는지를 측정합니다.</p>

  <pre><code><span class="cmt"># 수식: L = -log(P(정답 토큰))</span>
<span class="cmt"># 코드: loss_t = -probs[target_id].log()</span>

<span class="cmt"># 직관: 정답 토큰에 높은 확률 부여 → log(높은 값) → 음수 작음 → loss 작음</span>
<span class="cmt"># P(target) = 0.9  → loss = -log(0.9) ≈ 0.105  (잘 맞춤)</span>
<span class="cmt"># P(target) = 0.5  → loss = -log(0.5) ≈ 0.693</span>
<span class="cmt"># P(target) = 0.1  → loss = -log(0.1) ≈ 2.303  (많이 틀림)</span>
<span class="cmt"># P(target) = 0.01 → loss = -log(0.01) ≈ 4.605 (매우 틀림)</span>

<span class="cmt"># 전체 이름에 대한 평균:</span>
<span class="cmt"># loss = (1/n) * Σ -log(P(tokens[pos+1] | tokens[:pos+1]))</span></code></pre>

  <h3 id="loss-examples">위치별 손실 계산 예시 — "ab" 이름</h3>
  <p>이름 "ab"를 BOS로 감싸면 토큰 시퀀스: <code>[BOS, a, b, BOS]</code></p>
  <table>
    <thead><tr><th>Pos</th><th>Input</th><th>Target</th><th>P(target) 예시</th><th>Loss ≈</th></tr></thead>
    <tbody>
      <tr><td>0</td><td>BOS</td><td>'a'</td><td>0.10</td><td>2.30</td></tr>
      <tr><td>1</td><td>'a'</td><td>'b'</td><td>0.50</td><td>0.69</td></tr>
      <tr><td>2</td><td>'b'</td><td>BOS</td><td>0.20</td><td>1.61</td></tr>
    </tbody>
    <tfoot>
      <tr><th>평균</th><td>—</td><td>—</td><td>—</td><th>L₀ = (2.30 + 0.69 + 1.61) / 3 ≈ 1.53</th></tr>
    </tfoot>
  </table>

  <div class="info-box info">
    <strong>학습 = 각 위치에서 다음 토큰을 예측하도록 훈련</strong>
    BOS → 첫 글자 예측. 각 글자 → 다음 글자 예측. 마지막 글자 → BOS(종료) 예측.
    위치별 cross-entropy를 전부 합산하고 평균을 냅니다.
  </div>

  <h3 id="baseline">랜덤 베이스라인 — 학습 시작점 3.30</h3>
  <pre><code><span class="cmt"># 완전 랜덤 모델: 27개 토큰에 균등 확률 1/27 ≈ 0.037</span>
<span class="cmt"># 랜덤 베이스라인 손실 = -log(1/27) = log(27) ≈ 3.296</span>

<span class="cmt"># 정보 이론적 해석: 27개 심볼의 엔트로피 = log₂(27) ≈ 4.75 bits</span>
<span class="cmt"># (완전히 무작위인 시스템의 불확실성)</span>

<span class="cmt"># Perplexity = exp(loss)</span>
<span class="cmt"># 랜덤 Perplexity = exp(3.30) = 27  (= 어휘 크기, 직관적!)</span>
<span class="cmt"># 학습 후 Perplexity = exp(2.37) ≈ 10.7</span>
<span class="cmt"># → 모델이 평균적으로 약 11개 후보 중 정답을 고르는 수준</span></code></pre>

  <h3 id="loss-trajectory">손실 궤적 — 3.30 → 2.37 (1000 스텝)</h3>
  <table>
    <thead><tr><th>스텝</th><th>Loss</th><th>Perplexity</th><th>의미</th></tr></thead>
    <tbody>
      <tr><td>0</td><td>3.30</td><td>27.1</td><td>완전 랜덤 (초기화 직후)</td></tr>
      <tr><td>100</td><td>~2.90</td><td>~18</td><td>기본 통계 습득 시작</td></tr>
      <tr><td>500</td><td>~2.60</td><td>~13</td><td>모음-자음 패턴 학습 중</td></tr>
      <tr><td>1000</td><td>2.37</td><td>10.7</td><td>이름 구조 패턴 학습 완료</td></tr>
    </tbody>
  </table>

  <p>손실 3.30→2.37, 개선폭 <strong>0.93</strong> — Perplexity 27→10.7으로
  <strong>약 2.5배</strong> 향상. 4,192개 파라미터만으로 이름의 통계적 패턴을 학습한 결과입니다.</p>

  <div class="info-box info">
    <strong>왜 loss=0이 되지 않는가?</strong> 이름 데이터 자체에 불확실성이 있습니다.
    예: "k" 다음에 "a", "e", "i" 등 여러 글자가 올 수 있습니다.
    모델이 할 수 있는 최선은 이 확률 분포를 정확히 학습하는 것이며,
    완벽해도 엔트로피(최솟값) 이하로는 내려갈 수 없습니다.
  </div>
</section>

<section class="content-section">
  <h2 id="training-loop">완전한 학습 루프</h2>
  <pre><code>learning_rate, beta1, beta2, eps_adam = <span class="num">0.01</span>, <span class="num">0.85</span>, <span class="num">0.99</span>, <span class="num">1e-8</span>
m = [<span class="num">0.0</span>] * <span class="fn">len</span>(params)   <span class="cmt"># 1차 모멘텀 (방향)</span>
v = [<span class="num">0.0</span>] * <span class="fn">len</span>(params)   <span class="cmt"># 2차 모멘텀 (크기)</span>

num_steps = <span class="num">1000</span>
<span class="kw">for</span> step <span class="kw">in</span> <span class="fn">range</span>(num_steps):

    <span class="cmt"># ① 문서 선택: 셔플된 docs를 순환 (step % len(docs))</span>
    doc = docs[step % <span class="fn">len</span>(docs)]
    tokens = [BOS] + [uchars.<span class="fn">index</span>(ch) <span class="kw">for</span> ch <span class="kw">in</span> doc] + [BOS]
    n = <span class="fn">min</span>(block_size, <span class="fn">len</span>(tokens) - <span class="num">1</span>)   <span class="cmt"># 최대 block_size까지만</span>

    <span class="cmt"># ② 순전파: 각 위치에서 다음 토큰 예측 + loss 누적</span>
    keys, values = [[] <span class="kw">for</span> _ <span class="kw">in</span> <span class="fn">range</span>(n_layer)], [[] <span class="kw">for</span> _ <span class="kw">in</span> <span class="fn">range</span>(n_layer)]
    losses = []
    <span class="kw">for</span> pos_id <span class="kw">in</span> <span class="fn">range</span>(n):
        token_id, target_id = tokens[pos_id], tokens[pos_id + <span class="num">1</span>]
        logits = <span class="fn">gpt</span>(token_id, pos_id, keys, values)
        probs  = <span class="fn">softmax</span>(logits)
        losses.<span class="fn">append</span>(-probs[target_id].<span class="fn">log</span>())   <span class="cmt"># cross-entropy (스칼라)</span>
    loss = (<span class="num">1</span> / n) * <span class="fn">sum</span>(losses)   <span class="cmt"># 위치별 평균 loss</span>

    <span class="cmt"># ③ 역전파: 모든 파라미터의 기울기 계산</span>
    loss.<span class="fn">backward</span>()

    <span class="cmt"># ④ Adam 파라미터 업데이트 (기울기 초기화 포함)</span>
    lr_t = learning_rate * (<span class="num">1</span> - step / num_steps)   <span class="cmt"># 선형 감쇠</span>
    <span class="kw">for</span> i, p <span class="kw">in</span> <span class="fn">enumerate</span>(params):
        m[i] = beta1 * m[i] + (<span class="num">1</span> - beta1) * p.grad
        v[i] = beta2 * v[i] + (<span class="num">1</span> - beta2) * p.grad**<span class="num">2</span>
        m_hat = m[i] / (<span class="num">1</span> - beta1**(step+<span class="num">1</span>))   <span class="cmt"># 바이어스 보정</span>
        v_hat = v[i] / (<span class="num">1</span> - beta2**(step+<span class="num">1</span>))
        p.data -= lr_t * m_hat / (v_hat**<span class="num">0.5</span> + eps_adam)
        p.grad = <span class="num">0</span>   <span class="cmt"># ← 기울기 초기화 (업데이트 직후)</span></code></pre>

  <div class="info-box tip">
    <strong>학습 루프 4단계:</strong>
    ①문서 순환 → ②순전파(loss 누적) → ③역전파(grad 계산) → ④Adam 업데이트 + grad 초기화.
    이 1,000번 반복이 microgpt의 학습 전부입니다.
    <code>p.grad = 0</code>은 업데이트 <em>후</em>에 수행됩니다
    (인덱스 기반 for 루프이므로 별도의 초기화 루프가 필요 없습니다).
  </div>

  <h3 id="lr-decay">학습률 감쇠 (Linear Learning Rate Decay)</h3>
  <pre><code><span class="cmt"># 선형 학습률 감쇠: step이 진행될수록 lr 감소</span>
lr_t = learning_rate * (<span class="num">1</span> - step / num_steps)
<span class="cmt"># step=0:    lr_t = 0.01 × (1 - 0/1000)   = 0.01000  ← 최대</span>
<span class="cmt"># step=500:  lr_t = 0.01 × (1 - 500/1000) = 0.00500  ← 중간</span>
<span class="cmt"># step=999:  lr_t = 0.01 × (1 - 999/1000) = 0.00001  ← 최소</span>

<span class="cmt"># 주의: step=1000 (num_steps)에 도달하면 lr_t=0이 되므로</span>
<span class="cmt"># 실제로는 step 0~999 (총 1000번)만 실행됩니다.</span></code></pre>

  <div class="info-box info">
    <strong>왜 학습률을 줄이는가?</strong> 학습 초반에는 파라미터가 최적값에서 많이 떨어져 있으므로
    큰 보폭(높은 lr)으로 빠르게 이동합니다. 후반에는 최적값 근처에서 세밀하게 조정해야
    수렴 포인트를 지나치지 않도록 작은 보폭을 씁니다.
    nanoGPT는 더 정교한 "Cosine decay with linear warmup" (GPT-3 방식)을 사용합니다.
  </div>
</section>

<section class="content-section">
  <h2 id="adam">Adam 옵티마이저</h2>
  <p>단순한 SGD(확률적 경사 하강법) 대신, 적응적 학습률을 갖는 <strong>Adam</strong>을 구현합니다.
  Adam = <strong>Ada</strong>ptive <strong>M</strong>oment estimation.</p>

  <table>
    <thead><tr><th>옵티마이저</th><th>업데이트 규칙</th><th>단점</th></tr></thead>
    <tbody>
      <tr><td><strong>SGD</strong></td><td>p -= lr × grad</td><td>학습률 수동 조정, 느린 수렴</td></tr>
      <tr><td><strong>Momentum SGD</strong></td><td>v = β×v + grad; p -= lr×v</td><td>학습률 여전히 전역 고정</td></tr>
      <tr><td><strong>RMSProp</strong></td><td>v = β×v + (1-β)×grad²; p -= lr×grad/√v</td><td>모멘텀 없음</td></tr>
      <tr><td><strong>Adam ✓</strong></td><td>m, v 둘 다 추적; 바이어스 보정</td><td>메모리 2배 (m, v 버퍼)</td></tr>
    </tbody>
  </table>

  <h3 id="adam-code">Adam 단계별 코드 해설</h3>
  <pre><code><span class="cmt"># ── 초기화 (학습 루프 전) ──</span>
m = [<span class="num">0.0</span>] * <span class="fn">len</span>(params)   <span class="cmt"># 1차 모멘텀 버퍼 (지수 이동 평균 of grad)</span>
v = [<span class="num">0.0</span>] * <span class="fn">len</span>(params)   <span class="cmt"># 2차 모멘텀 버퍼 (지수 이동 평균 of grad²)</span>
beta1, beta2 = <span class="num">0.85</span>, <span class="num">0.99</span>

<span class="cmt"># ── 각 스텝의 업데이트 ──</span>
<span class="kw">for</span> i, p <span class="kw">in</span> <span class="fn">enumerate</span>(params):
    g = p.grad

    <span class="cmt"># ─ 1단계: 1차 모멘텀 (방향) ─</span>
    <span class="cmt"># 현재 기울기 g와 이전 방향 m[i]의 지수 가중 평균</span>
    m[i] = beta1 * m[i] + (<span class="num">1</span> - beta1) * g
    <span class="cmt"># beta1=0.85 → 현재 기울기에 15% 가중, 이전 방향에 85% 가중</span>
    <span class="cmt"># (기본값 0.9보다 작아 더 빠르게 현재 기울기에 반응)</span>

    <span class="cmt"># ─ 2단계: 2차 모멘텀 (크기) ─</span>
    <span class="cmt"># 기울기 제곱의 지수 이동 평균 → "이 파라미터의 기울기가 얼마나 크게 변하는가"</span>
    v[i] = beta2 * v[i] + (<span class="num">1</span> - beta2) * g**<span class="num">2</span>
    <span class="cmt"># beta2=0.99 → 기울기 크기의 장기 기억</span>

    <span class="cmt"># ─ 3단계: 바이어스 보정 ─</span>
    <span class="cmt"># 초기(step 작을 때) m,v가 0으로 초기화되어 있어 편향됨을 보정</span>
    t = step + <span class="num">1</span>
    m_hat = m[i] / (<span class="num">1</span> - beta1**t)   <span class="cmt"># step=1: 1/(1-0.85)=6.67배 증폭</span>
    v_hat = v[i] / (<span class="num">1</span> - beta2**t)   <span class="cmt"># step=1: 1/(1-0.99)=100배 증폭</span>

    <span class="cmt"># ─ 4단계: 파라미터 업데이트 ─</span>
    <span class="cmt"># 효과적 학습률 = lr_t / √v_hat → 기울기가 크게 변하는 파라미터는 작게 업데이트</span>
    p.data -= lr_t * m_hat / (v_hat**<span class="num">0.5</span> + eps_adam)
    p.grad = <span class="num">0</span>   <span class="cmt"># 초기화</span></code></pre>

  <h3 id="bias-correction">바이어스 보정 상세 — 왜 필요한가?</h3>
  <pre><code><span class="cmt"># step=1, beta1=0.85, grad=1.0 인 경우:</span>
m[i] = <span class="num">0.85</span> * <span class="num">0.0</span> + <span class="num">0.15</span> * <span class="num">1.0</span> = <span class="num">0.15</span>   <span class="cmt"># 실제 gradient=1.0인데 m=0.15만 저장됨</span>
m_hat = <span class="num">0.15</span> / (<span class="num">1</span> - <span class="num">0.85</span>**<span class="num">1</span>) = <span class="num">0.15</span> / <span class="num">0.15</span> = <span class="num">1.0</span>  <span class="cmt"># 보정 후 1.0 ← 올바른 값</span>

<span class="cmt"># step=2, beta1=0.85, grad=[1.0, 1.0]:</span>
<span class="cmt"># m = 0.85*0.15 + 0.15*1.0 = 0.2775</span>
<span class="cmt"># m_hat = 0.2775 / (1 - 0.85²) = 0.2775 / 0.2775 ≈ 1.0 (여전히 보정)</span>

<span class="cmt"># step=100, beta1=0.85:</span>
<span class="cmt"># 1 - 0.85**100 ≈ 1.0 (완전 포화)</span>
<span class="cmt"># → 보정 인자가 거의 1이 되어 바이어스 보정이 자동으로 무력화됨</span>
<span class="cmt"># → 초반 스텝에서만 중요하고, 충분한 스텝 후엔 영향 없음</span></code></pre>

  <table>
    <thead><tr><th>파라미터</th><th>기본값 (논문)</th><th>microgpt</th><th>효과</th></tr></thead>
    <tbody>
      <tr>
        <td>beta1</td><td>0.9</td><td><strong>0.85</strong></td>
        <td>현재 기울기에 15% (기본 10%) → 더 빠른 방향 전환</td>
      </tr>
      <tr>
        <td>beta2</td><td>0.999</td><td><strong>0.99</strong></td>
        <td>기울기 크기 기억 더 짧게 → 더 빠른 적응</td>
      </tr>
    </tbody>
  </table>

  <p>1,000 스텝 밖에 없는 짧은 학습에서는 빠른 적응이 중요합니다.
  기본값(β1=0.9, β2=0.999)은 수십만 스텝의 장기 학습을 위한 값입니다.</p>

  <div class="info-box tip">
    <strong>Adam의 적응적 학습률 직관:</strong> 평탄한 길(기울기 작음) → 큰 보폭으로 빠르게.
    가파른 경사(기울기 큼) → 작은 보폭으로 조심스럽게.
    <code>lr / √v_hat</code>에서 v_hat(기울기 제곱 평균)이 크면 분모가 커져 실제 학습률이 줄어듭니다.
    파라미터마다 각자에게 맞는 학습률을 자동 조절합니다.
  </div>
</section>

<section class="content-section">
  <h2 id="inference">추론 (Inference) &amp; 이름 생성</h2>
  <pre><code><span class="cmt"># may the model babble back to us</span>
temperature = <span class="num">0.5</span>  <span class="cmt"># (0, 1] — 낮을수록 보수적, 높을수록 다양한 출력</span>
<span class="kw">for</span> sample_idx <span class="kw">in</span> <span class="fn">range</span>(<span class="num">20</span>):   <span class="cmt"># 총 20개의 이름 생성</span>
    keys, values = [[] <span class="kw">for</span> _ <span class="kw">in</span> <span class="fn">range</span>(n_layer)], [[] <span class="kw">for</span> _ <span class="kw">in</span> <span class="fn">range</span>(n_layer)]
    token_id = BOS   <span class="cmt"># 시작 토큰</span>
    sample = []
    <span class="cmt"># BOS → 첫 글자 → 두 번째 글자 → ... → BOS(종료)</span>
    <span class="kw">for</span> pos_id <span class="kw">in</span> <span class="fn">range</span>(block_size):
        logits = <span class="fn">gpt</span>(token_id, pos_id, keys, values)
        probs = <span class="fn">softmax</span>([l / temperature <span class="kw">for</span> l <span class="kw">in</span> logits])
        token_id = random.<span class="fn">choices</span>(<span class="fn">range</span>(vocab_size), weights=[p.data <span class="kw">for</span> p <span class="kw">in</span> probs])[<span class="num">0</span>]
        <span class="kw">if</span> token_id == BOS: <span class="kw">break</span>
        sample.<span class="fn">append</span>(uchars[token_id])
    <span class="fn">print</span>(<span class="str">f"sample {sample_idx+1:2d}: {''.join(sample)}"</span>)</code></pre>

  <h3 id="autoregressive-trace">자기회귀 생성 — "kamon" 단계별 추적</h3>
  <pre><code><span class="cmt"># "kamon" 생성 과정 (temperature=0.5 적용)</span>
keys = [[] <span class="kw">for</span> _ <span class="kw">in</span> <span class="fn">range</span>(n_layer)]
values = [[] <span class="kw">for</span> _ <span class="kw">in</span> <span class="fn">range</span>(n_layer)]
token_id = BOS  <span class="cmt"># 26</span>

<span class="cmt"># ── step 0 (pos=0) ──</span>
logits = <span class="fn">gpt</span>(<span class="num">26</span>, <span class="num">0</span>, keys, values)           <span class="cmt"># BOS 입력</span>
probs  = <span class="fn">softmax</span>([l/<span class="num">0.5</span> <span class="kw">for</span> l <span class="kw">in</span> logits])   <span class="cmt"># temperature=0.5 적용</span>
<span class="cmt"># probs 중 'k'(10)의 확률이 가장 높다고 가정</span>
token_id = random.<span class="fn">choices</span>(<span class="fn">range</span>(<span class="num">27</span>), weights=[p.data <span class="kw">for</span> p <span class="kw">in</span> probs])[<span class="num">0</span>]
<span class="cmt"># → token_id = 10 ('k')</span>

<span class="cmt"># ── step 1 (pos=1) ──</span>
logits = <span class="fn">gpt</span>(<span class="num">10</span>, <span class="num">1</span>, keys, values)   <span class="cmt"># 'k' 입력, pos=1</span>
<span class="cmt"># keys[0]에는 이제 K_BOS, K_k 두 개가 있음</span>
<span class="cmt"># → 'a'(0)가 높은 확률로 샘플링됨 → token_id = 0</span>

<span class="cmt"># ── step 2 (pos=2) ── 'm'(12) 샘플링</span>
<span class="cmt"># keys[0] = [K_BOS, K_k, K_a] (3개 누적)</span>

<span class="cmt"># ── step 3 (pos=3) ── 'o'(14) 샘플링</span>
<span class="cmt"># ── step 4 (pos=4) ── 'n'(13) 샘플링</span>
<span class="cmt"># ── step 5 (pos=5) ── BOS(26)가 샘플링되면 break → "kamon" 완성</span>
<span class="cmt"># → sample = ['k', 'a', 'm', 'o', 'n'] → "kamon"</span></code></pre>

  <h3 id="block-size-limit">block_size=16 제한의 의미</h3>
  <pre><code><span class="kw">for</span> pos_id <span class="kw">in</span> <span class="fn">range</span>(block_size):   <span class="cmt"># 0~15, 최대 16 스텝</span>
    logits = <span class="fn">gpt</span>(token_id, pos_id, keys, values)
    <span class="cmt"># ...</span>
    <span class="kw">if</span> token_id == BOS: <span class="kw">break</span>   <span class="cmt"># 보통 여기서 멈춤</span>

<span class="cmt"># 만약 BOS가 16 스텝 안에 생성되지 않으면:</span>
<span class="cmt"># pos_id가 block_size에 도달해 루프 종료 → 이름이 잘림</span>
<span class="cmt"># names.txt의 최장 이름이 15글자이므로, 정상 이름은 항상 16 이내에 BOS 생성</span>
<span class="cmt"># 반면 wpe(위치 임베딩)는 0~15만 정의됨</span>
<span class="cmt"># → pos_id=16 이상을 입력하면 IndexError 발생</span></code></pre>

  <h3 id="temperature">Temperature — 창의성 조절</h3>
  <pre><code><span class="cmt"># 원본 logits (학습된 모델 출력, 일부):</span>
<span class="cmt"># 'a'=2.1, 'k'=1.8, 'e'=1.5, 나머지≈0</span>

<span class="cmt"># Temperature T=1.0 (원본 유지):</span>
<span class="cmt"># softmax([2.1, 1.8, 1.5, ...]) ≈ [0.30, 0.22, 0.17, ...]</span>
<span class="cmt">#   → 적당히 다양한 선택</span>

<span class="cmt"># Temperature T=0.5 (낮게, 로짓 2배 증폭):</span>
<span class="cmt"># softmax([4.2, 3.6, 3.0, ...]) ≈ [0.52, 0.28, 0.14, ...]</span>
<span class="cmt">#   → 'a'에 집중, 더 안정적</span>

<span class="cmt"># Temperature T=0.1 (매우 낮게, 로짓 10배 증폭):</span>
<span class="cmt"># softmax([21, 18, 15, ...]) ≈ [0.95, 0.05, ~0, ...]</span>
<span class="cmt">#   → 거의 항상 'a' 선택 (greedy에 가까움)</span>

<span class="cmt"># Temperature T=2.0 (높게, 로짓 절반):</span>
<span class="cmt"># softmax([1.05, 0.9, 0.75, ...]) ≈ [0.20, 0.18, 0.16, ...]</span>
<span class="cmt">#   → 거의 균등 → 무작위에 가까움</span></code></pre>

  <dl>
    <dt>Temperature = 0.1 — 보수적 (Greedy에 가까움)</dt>
    <dd>로짓에 10을 곱한 효과 → 가장 높은 확률 토큰에 95%+ 집중.
    반복적이지만 안정적. 동일한 BOS에서 항상 같은 이름이 나올 수 있음.</dd>

    <dt>Temperature = 0.5 (microgpt.py 기본값) — 품질과 다양성의 균형</dt>
    <dd>로짓에 2를 곱한 효과 → 상위 토큰들 간의 차이를 2배로 강조.
    발음 가능하고 다양한 이름 생성.</dd>

    <dt>Temperature = 1.0 — 원본 확률 그대로</dt>
    <dd>모델이 학습한 분포를 그대로 사용. 이론적으로 "올바른" 샘플링.
    가끔 발음 어려운 이름도 생성.</dd>

    <dt>Temperature &gt; 1.0 — 창의적 (과도한 무작위)</dt>
    <dd>로짓 차이를 줄여 균등 분포에 가까워짐. 이상한 결과 빈도 증가.</dd>
  </dl>

  <div class="info-box warning">
    <strong>⚠️ Temperature = 0 → ZeroDivisionError:</strong>
    <code>logits / 0</code>은 Python에서 ZeroDivisionError를 냅니다.
    Greedy Decoding을 원한다면 temperature 나눗셈을 제거하고
    <code>token_id = max(range(vocab_size), key=lambda i: logits[i].data)</code>를 사용하세요.
  </div>

  <h3 id="inference-results">실제 생성 결과 — 1000 스텝 학습 후 샘플 20개</h3>
  <table>
    <thead><tr><th>샘플</th><th>이름</th><th>특징</th></tr></thead>
    <tbody>
      <tr><td>1~5</td><td>kamon, ann, karai, jaire, vialan</td><td>모음-자음 교대 패턴</td></tr>
      <tr><td>6~10</td><td>karia, yeran, anna, areli, kaina</td><td>"ar-", "an-" 흔한 이름 조각</td></tr>
      <tr><td>11~15</td><td>konna, keylen, liole, alerin, earan</td><td>2~6글자 적절한 길이</td></tr>
      <tr><td>16~20</td><td>lenne, kana, lara, alela, anton</td><td>발음 가능한 이름 구조</td></tr>
    </tbody>
  </table>

  <div class="info-box info">
    <strong>4,192개 파라미터가 학습한 통계적 패턴:</strong>
    "k-", "l-", "a-" 로 시작하는 이름이 많음.
    모음(a, e, i, o) 뒤에 자음이 자주 옴.
    "-an", "-ra", "-la", "-on" 같은 접미사 패턴.
    평균 4~5글자 (훈련 데이터 분포 반영).
  </div>

  <h3 id="train-vs-infer">학습 vs 추론의 차이점</h3>
  <table>
    <thead><tr><th>측면</th><th>학습 (Training)</th><th>추론 (Inference)</th></tr></thead>
    <tbody>
      <tr><td>입력 방식</td><td>이름 전체 토큰을 위치별로 순전파</td><td>BOS부터 시작, 토큰 하나씩 생성</td></tr>
      <tr><td>역전파</td><td>O (loss.backward())</td><td>X (grad 불필요)</td></tr>
      <tr><td>KV 캐시</td><td>매 이름마다 초기화</td><td>매 샘플마다 초기화</td></tr>
      <tr><td>종료 조건</td><td>모든 위치 처리 완료</td><td>BOS 샘플링 or block_size 도달</td></tr>
      <tr><td>결정성</td><td>결정적 (seed 고정)</td><td>확률적 (temperature 샘플링)</td></tr>
    </tbody>
  </table>
</section>

<section class="content-section">
  <h2 id="comparison">MicroGPT vs 현대 LLM</h2>

  <table>
    <thead>
      <tr><th>항목</th><th>microgpt.py</th><th>Modern LLMs (GPT-4 등)</th></tr>
    </thead>
    <tbody>
      <tr><td>파라미터</td><td><strong>4,192개</strong></td><td><strong>수조(Trillions)개</strong></td></tr>
      <tr><td>데이터</td><td>32,033개의 이름</td><td>인터넷 전체 텍스트 (수조 토큰)</td></tr>
      <tr><td>하드웨어</td><td>노트북 CPU, 수 분</td><td>수천 개의 H100 GPU</td></tr>
      <tr><td>코드</td><td>~200줄 순수 파이썬</td><td>수백만 줄, 수천억 원 학습 비용</td></tr>
      <tr><td>목적</td><td>영어 이름 생성</td><td>범용 언어 이해 &amp; 생성</td></tr>
    </tbody>
  </table>

  <div class="info-box tip">
    <strong>하지만 원리는 완전히 동일합니다.</strong> MicroGPT와 GPT-4의 아키텍처적 차이는
    규모(scale)뿐입니다. 토큰 임베딩, 트랜스포머 레이어, 어텐션, MLP, 잔차 연결,
    RMSNorm, 소프트맥스, Adam 옵티마이저 — 모두 같은 원리입니다.
  </div>

  <h3 id="detail-comparison">구성요소별 상세 비교</h3>
  <table>
    <thead><tr><th>구성요소</th><th>microgpt.py</th><th>GPT-4 / LLaMA 등</th></tr></thead>
    <tbody>
      <tr><td>데이터</td><td>32,033개의 이름 (names.txt)</td><td>인터넷 전체 텍스트 (수조 토큰)</td></tr>
      <tr><td>토크나이저</td><td>26 알파벳 + BOS = 27개 토큰</td><td>BPE/SentencePiece, 50K~256K 토큰</td></tr>
      <tr><td>Autograd</td><td>스칼라 Value 클래스, 순수 Python</td><td>PyTorch/JAX 텐서 자동미분 (GPU 병렬)</td></tr>
      <tr><td>아키텍처</td><td>n_embd=16, n_head=4, n_layer=1</td><td>n_embd=12288, n_head=96, n_layer=96+</td></tr>
      <tr><td>파라미터</td><td>4,192개</td><td>수천억~수조 개</td></tr>
      <tr><td>학습</td><td>1000 스텝, 노트북 CPU, 수 분</td><td>수천억 스텝, H100 수천 개, 수개월</td></tr>
      <tr><td>정규화</td><td>RMSNorm (학습 파라미터 없음)</td><td>LayerNorm or RMSNorm (γ 파라미터 있음)</td></tr>
      <tr><td>활성화</td><td>ReLU</td><td>GELU (GPT-2/3), SwiGLU (LLaMA)</td></tr>
      <tr><td>사후 학습</td><td>없음 (순수 사전학습만)</td><td>SFT + RLHF/DPO (인간 피드백)</td></tr>
      <tr><td>추론 최적화</td><td>KV캐시 + temperature 샘플링</td><td>Flash Attention, 양자화, 분산 추론</td></tr>
    </tbody>
  </table>

  <div class="info-box info">
    <strong>Karpathy의 말:</strong>
    "The most atomic way to train and run inference for a GPT in pure, dependency-free Python.
    <em>This file is the complete algorithm. Everything else is just efficiency.</em>"
  </div>
</section>

<section class="content-section">
  <h2 id="nanogpt">nanoGPT — microgpt의 Production 확장판</h2>
  <p><strong>nanoGPT</strong>는 Karpathy가 2022년 12월 공개한 "가장 간결하고 빠른 GPT 훈련 코드베이스"입니다.
  OpenAI GPT-2(124M 파라미터)를 완전히 재현하고, 단일 GPU에서 실용적인 언어 모델을 훈련할 수 있게 설계되었습니다.
  microgpt가 "이해를 위한 원자적 구현"이라면, nanoGPT는 "실제 사용을 위한 최소 구현"입니다.</p>

  <h3 id="nanogpt-compare">microgpt vs nanoGPT 핵심 차이점</h3>
  <table>
    <thead><tr><th>항목</th><th>microgpt.py</th><th>nanoGPT</th></tr></thead>
    <tbody>
      <tr><td><strong>연산 단위</strong></td><td>스칼라 <code>Value</code> 객체</td><td>PyTorch 텐서 (GPU 병렬 연산)</td></tr>
      <tr><td><strong>자동미분</strong></td><td>수동 구현 (<code>_backward</code>, 위상 정렬)</td><td>PyTorch <code>.backward()</code> 완전 자동화</td></tr>
      <tr><td><strong>토크나이저</strong></td><td>26 알파벳 + BOS = vocab_size 27 (문자 수준)</td><td>tiktoken BPE — vocab_size 50,257 (서브워드)</td></tr>
      <tr><td><strong>아키텍처</strong></td><td>n_layer=1, n_head=4, n_embd=16</td><td>n_layer=12, n_head=12, n_embd=768 (GPT-2 small)</td></tr>
      <tr><td><strong>파라미터 수</strong></td><td>4,192개</td><td>124,439,808개 (GPT-2 small) ~ 1.5B (XL)</td></tr>
      <tr><td><strong>배치 크기</strong></td><td>1 (이름 1개/스텝)</td><td>32~512+ (gradient accumulation)</td></tr>
      <tr><td><strong>컨텍스트 길이</strong></td><td>최대 15 토큰 (가장 긴 이름)</td><td>1,024 토큰 (GPT-2 기본)</td></tr>
      <tr><td><strong>Attention 구현</strong></td><td>스칼라 루프 기반 수동 계산</td><td>Flash Attention (<code>F.scaled_dot_product_attention</code>)</td></tr>
      <tr><td><strong>정규화</strong></td><td>RMSNorm (수동 구현)</td><td>LayerNorm (<code>nn.LayerNorm</code>) + Dropout</td></tr>
      <tr><td><strong>활성화 함수</strong></td><td>ReLU</td><td>GELU (<code>F.gelu</code>) — GPT-2/3 표준</td></tr>
      <tr><td><strong>옵티마이저</strong></td><td>Adam 수동 구현 (β1=0.85, β2=0.99)</td><td>PyTorch <code>optim.AdamW</code> + weight decay (β1=0.9, β2=0.95)</td></tr>
      <tr><td><strong>LR 스케줄</strong></td><td>선형 감쇠</td><td>Cosine decay with linear warmup (GPT-3 방식)</td></tr>
      <tr><td><strong>Gradient Clipping</strong></td><td>없음</td><td><code>clip_grad_norm_</code> (max_norm=1.0)</td></tr>
      <tr><td><strong>분산학습</strong></td><td>없음 (단일 프로세스)</td><td>PyTorch DDP — 다중 GPU/노드</td></tr>
      <tr><td><strong>Checkpoint</strong></td><td>없음</td><td>자동 저장/복원, best val loss 기준</td></tr>
      <tr><td><strong>Weight Tying</strong></td><td>wte/lm_head 별도 초기화</td><td><code>lm_head.weight = transformer.wte.weight</code> 명시적 공유</td></tr>
      <tr><td><strong>외부 의존성</strong></td><td>os, math, random (표준 라이브러리만)</td><td>torch, tiktoken, numpy, datasets 등</td></tr>
    </tbody>
  </table>

  <h3 id="nanogpt-files">nanoGPT 파일 구조</h3>
  <table>
    <thead><tr><th>파일</th><th>역할</th><th>microgpt 대응</th></tr></thead>
    <tbody>
      <tr>
        <td><code>model.py</code></td>
        <td>GPT 모델 정의 — <code>CausalSelfAttention</code>, <code>MLP</code>, <code>Block</code>, <code>GPT</code> 클래스, Flash Attention 지원</td>
        <td><code>def gpt()</code> 함수 전체</td>
      </tr>
      <tr>
        <td><code>train.py</code></td>
        <td>훈련 루프 — DDP 분산학습, gradient accumulation, cosine LR, checkpoint 저장/복원</td>
        <td><code>for step in range(num_steps):</code> 루프</td>
      </tr>
      <tr>
        <td><code>data/</code></td>
        <td>데이터 준비 스크립트 — Shakespeare, OpenWebText 다운로드·토크나이징·바이너리 저장</td>
        <td>코드 내 <code>names.txt</code> 직접 로딩 3줄</td>
      </tr>
      <tr>
        <td><code>config/</code></td>
        <td>사전 정의 훈련 설정 — GPT-2 small/medium/large/XL, Shakespeare 등 하이퍼파라미터 세트</td>
        <td>코드 내 하드코딩된 <code>n_embd=16</code> 등</td>
      </tr>
      <tr>
        <td><code>sample.py</code></td>
        <td>텍스트 샘플링 — checkpoint 로드 후 <code>temperature</code>·<code>top_k</code> 기반 텍스트 생성</td>
        <td><code>for sample_idx in range(20):</code> 루프</td>
      </tr>
      <tr>
        <td><code>bench.py</code></td>
        <td>성능 벤치마크 — throughput(token/s) 측정, Flash Attention 효과 비교</td>
        <td>해당 없음</td>
      </tr>
    </tbody>
  </table>

  <h3 id="nanogpt-model-py">nanoGPT model.py — 클래스 구조</h3>
  <p>microgpt의 함수형 구현과 달리 nanoGPT는 PyTorch <code>nn.Module</code> 클래스 계층 구조를 사용합니다.
  아키텍처는 동일하지만, 텐서화·자동미분·GPU 최적화가 추가됩니다.</p>
  <pre><code><span class="cmt"># nanoGPT model.py 클래스 구조 (요약)</span>

<span class="kw">class</span> <span class="type">CausalSelfAttention</span>(nn.<span class="type">Module</span>):
    <span class="cmt"># Q, K, V 프로젝션을 단일 c_attn(3*n_embd)로 통합 — 효율적 행렬 곱</span>
    <span class="cmt"># Flash Attention: F.scaled_dot_product_attention() 자동 사용</span>
    <span class="cmt"># microgpt: 스칼라 루프로 직접 qkv 계산</span>
    <span class="cmt"># attn_dropout + resid_dropout 정규화</span>

<span class="kw">class</span> <span class="type">MLP</span>(nn.<span class="type">Module</span>):
    <span class="cmt"># Linear(n_embd → 4*n_embd) → GELU → Linear(4*n_embd → n_embd)</span>
    <span class="cmt"># microgpt: ReLU 사용, nanoGPT: GELU (GPT-2/3 표준)</span>
    <span class="cmt"># dropout으로 과적합 방지</span>

<span class="kw">class</span> <span class="type">Block</span>(nn.<span class="type">Module</span>):
    <span class="cmt"># Pre-LayerNorm 구조: ln_1 → attention → residual</span>
    <span class="cmt">#                      ln_2 → mlp       → residual</span>
    <span class="cmt"># microgpt: RMSNorm 사용, nanoGPT: LayerNorm</span>

<span class="kw">class</span> <span class="type">GPT</span>(nn.<span class="type">Module</span>):
    <span class="cmt"># transformer.wte : 토큰 임베딩  [vocab_size, n_embd]</span>
    <span class="cmt"># transformer.wpe : 위치 임베딩  [block_size, n_embd]  ← 학습 가능</span>
    <span class="cmt"># transformer.drop: Dropout</span>
    <span class="cmt"># transformer.h   : ModuleList([Block(...)] * n_layer)</span>
    <span class="cmt"># transformer.ln_f: 최종 LayerNorm</span>
    <span class="cmt"># lm_head         : Linear(n_embd → vocab_size, bias=False)</span>
    <span class="cmt">#                   → wte와 가중치 공유(weight tying)</span>
    <span class="cmt">#</span>
    <span class="cmt"># generate(): top_k 샘플링 + temperature 스케일링</span>
    <span class="cmt"># from_pretrained(): HuggingFace 없이 GPT-2 가중치 직접 로드</span></code></pre>

  <h3 id="nanogpt-roadmap">microgpt → nanoGPT 확장 경로 — 6단계 로드맵</h3>
  <ol>
    <li>
      <strong>① 스칼라 → NumPy 벡터화</strong><br>
      <code>Value</code> 스칼라 루프 → NumPy 배열 연산 → 속도 10~100배 향상.<br>
      역전파도 NumPy 브로드캐스팅으로 표현 가능. 텐서 차원 개념 도입.
    </li>
    <li>
      <strong>② 배치(Batch) 처리 도입</strong><br>
      이름 1개/스텝 → 32개/스텝 미니배치 → 그래디언트 분산 감소, 학습 안정화.<br>
      텐서 차원이 <code>[T]</code>에서 <code>[B, T]</code>로 확장됨.
    </li>
    <li>
      <strong>③ PyTorch 텐서 교체</strong><br>
      NumPy → PyTorch Tensor → <code>.backward()</code> 자동미분, GPU 가속.<br>
      수동으로 작성한 <code>_backward</code> 체인이 완전히 자동화됨.
    </li>
    <li>
      <strong>④ 표준 모듈 &amp; 정규화 적용</strong><br>
      수동 구현한 <code>linear</code>, <code>rmsnorm</code>, <code>Adam</code> →
      <code>nn.Linear</code>, <code>nn.LayerNorm</code>, <code>optim.AdamW</code>.<br>
      Dropout, gradient clipping, weight decay 정규화 기법 추가.
    </li>
    <li>
      <strong>⑤ 아키텍처 확장</strong><br>
      n_layer=1 → 12+, n_embd=16 → 768+, vocab_size=27 → 50,257 (BPE).<br>
      Flash Attention, 컨텍스트 길이 1024+ 지원, top-k 샘플링 추가.
    </li>
    <li>
      <strong>⑥ 분산학습 &amp; 생산화</strong><br>
      DDP로 다중 GPU 학습, gradient accumulation, cosine LR with warmup,<br>
      자동 checkpoint 저장/복원 → 실제 GPT-2 수준 모델 훈련 가능.
    </li>
  </ol>

  <h3 id="nanogpt-examples">nanoGPT 대표 사용 예시</h3>
  <dl>
    <dt>Shakespeare 문자 수준 언어 모델</dt>
    <dd>셰익스피어 전집(~1MB)으로 훈련 → 셰익스피어 스타일 문장 생성.
    단일 GPU로 수 분~수 시간 안에 완료. GPT의 원리를 실감하는 입문 예제.
    microgpt의 names.txt → 셰익스피어 텍스트로 도메인만 교체한 확장 버전.</dd>

    <dt>GPT-2 (124M) 완전 재현</dt>
    <dd>OpenWebText(~38GB) 데이터셋으로 훈련 →
    OpenAI GPT-2와 동등한 검증 손실(val loss ≈ 2.85) 달성.
    A100 GPU 8개로 약 4일 소요. Karpathy가 직접 재현 결과를 공개함.</dd>

    <dt>GPT-2 공식 가중치 파인튜닝</dt>
    <dd>OpenAI 공개 GPT-2 가중치를 HuggingFace 없이 직접 로드.
    <code>--init_from=gpt2</code> 플래그 하나로 gpt2/gpt2-medium/gpt2-large/gpt2-xl 중 선택.
    특정 도메인 텍스트로 파인튜닝하여 도메인 특화 모델 제작 가능.</dd>
  </dl>

  <h3 id="karpathy-projects">관련 Karpathy 프로젝트</h3>
  <dl>
    <dt>micrograd — autograd 전신</dt>
    <dd>스칼라 autograd 엔진 (Value 클래스) — microgpt Value 클래스의 직접적 원형.
    역전파 원리를 이해하기 위한 가장 작은 구현. 약 150줄.</dd>

    <dt>makemore — 데이터셋·토크나이저 전신</dt>
    <dd>names.txt 데이터셋 출처, 문자 수준 언어 모델. Bigram부터 Transformer까지
    단계적으로 발전시키는 강의 시리즈. microgpt 토크나이저 구조 계승.</dd>

    <dt>nanoGPT — 실용 확장 버전</dt>
    <dd>microgpt에서 배운 원리를 실제 규모로 구현. PyTorch + GPU 기반,
    GPT-2 완전 재현, 분산학습 지원. 연구/실험용 GPT 훈련의 현실적 최소 기준점.</dd>

    <dt>llm.c — 의존성 최소화 극한</dt>
    <dd>C/CUDA로만 LLM 훈련 — PyTorch조차 제거. microgpt의 "의존성 없이 직접 구현" 철학을
    시스템 레벨까지 밀고 나간 프로젝트.</dd>
  </dl>
</section>

<section class="content-section">
  <h2 id="summary">200줄 코드로 얻은 것들</h2>

  <table>
    <thead><tr><th>구성요소</th><th>microgpt 코드</th><th>현대 프레임워크 등가</th></tr></thead>
    <tbody>
      <tr><td><strong>Autograd 엔진</strong></td><td><code>class Value</code></td><td>PyTorch <code>torch.autograd</code></td></tr>
      <tr><td><strong>신경망 레이어</strong></td><td><code>linear, softmax, rmsnorm</code></td><td>PyTorch <code>nn.Linear, nn.Softmax, nn.RMSNorm</code></td></tr>
      <tr><td><strong>GPT 아키텍처</strong></td><td><code>def gpt(...)</code></td><td>HuggingFace <code>GPT2Model</code></td></tr>
      <tr><td><strong>Adam 옵티마이저</strong></td><td>m, v 리스트 + 4줄 업데이트</td><td>PyTorch <code>optim.Adam</code></td></tr>
    </tbody>
  </table>

  <div class="info-box tip">
    <strong>KARPATHY의 핵심 메시지:</strong><br>
    "우리는 실행 규칙이 아니라, <em>학습 규칙</em>을 정의한 것입니다."<br>
    GPT-4도, LLaMA도, 우리의 microgpt도 — 모두 손실을 줄이는 숫자들을 조정합니다.
    4,192개(또는 수조 개)의 차이일 뿐, 원리는 완전히 동일합니다.
    블랙박스는 열렸습니다.
  </div>

  <h3 id="experiments">실험 아이디어 — 파라미터를 바꿔보자</h3>
  <table>
    <thead><tr><th>실험</th><th>변경 내용</th><th>예상 효과</th></tr></thead>
    <tbody>
      <tr><td>더 깊게</td><td><code>n_layer = 4</code></td><td>파라미터 4배↑ 더 자연스러운 이름</td></tr>
      <tr><td>더 넓게</td><td><code>n_embd = 64, n_head = 8</code></td><td>파라미터 ~16배↑ 표현력↑</td></tr>
      <tr><td>더 오래</td><td><code>num_steps = 10000</code></td><td>더 낮은 loss, 더 정확한 이름 패턴</td></tr>
      <tr><td>다른 데이터</td><td>셰익스피어 텍스트</td><td>문장 수준 언어 생성</td></tr>
      <tr><td>온도 실험</td><td><code>temperature = 0.1, 0.5, 1.0, 2.0</code></td><td>창의성 vs 안정성 트레이드오프</td></tr>
    </tbody>
  </table>
</section>

<section class="content-section">
  <h2 id="references">시리즈 전체 &amp; 참고 자료</h2>

  <h3>이 시리즈</h3>
  <ul>
    <li><a href="microgpt-intro.html">1부: 저자 소개, 데이터 파이프라인, 토크나이저, Autograd 엔진</a></li>
    <li><a href="microgpt-architecture.html">2부: 모델 설계도, 임베딩, Attention, 트랜스포머 블록, GPT 파이프라인</a></li>
  </ul>

  <h3>원본 소스</h3>
  <ul>
    <li><a href="https://karpathy.github.io/2026/02/12/microgpt/" target="_blank" rel="noopener">Andrej Karpathy — microgpt 블로그 포스트 (2026.02.12)</a> — 저자 직접 작성 공식 가이드. 데이터셋~추론 전체 해설</li>
    <li><a href="https://gist.github.com/karpathy/8627fe009c40f57531cb18360106ce95" target="_blank" rel="noopener">microgpt.py 원본 소스 코드 (GitHub Gist)</a> — ~200줄 순수 파이썬. 토크나이저·Autograd·GPT·Adam·추론 전체</li>
    <li><a href="https://github.com/karpathy" target="_blank" rel="noopener">@karpathy GitHub</a> — 저자의 모든 오픈소스 교육용 AI 프로젝트</li>
    <li><a href="https://x.com/karpathy" target="_blank" rel="noopener">@karpathy X(Twitter)</a> — 최신 연구·강의 업데이트</li>
  </ul>

  <h3>관련 Karpathy 프로젝트</h3>
  <table>
    <thead><tr><th>프로젝트</th><th>설명</th><th>관계</th></tr></thead>
    <tbody>
      <tr>
        <td><a href="https://github.com/karpathy/micrograd" target="_blank" rel="noopener">micrograd</a></td>
        <td>스칼라 autograd 엔진 — Value 클래스, 위상 정렬 역전파 (~150줄)</td>
        <td>microgpt Value 클래스의 직접적 원형</td>
      </tr>
      <tr>
        <td><a href="https://github.com/karpathy/makemore" target="_blank" rel="noopener">makemore</a></td>
        <td>문자 수준 언어 모델 — Bigram → MLP → Transformer 단계별 강의</td>
        <td>names.txt 데이터셋 출처, 토크나이저 구조 계승</td>
      </tr>
      <tr>
        <td><a href="https://github.com/karpathy/nanoGPT" target="_blank" rel="noopener">nanoGPT</a></td>
        <td>실용적 GPT 훈련 — PyTorch, Flash Attention, DDP, GPT-2 재현, Shakespeare 예제</td>
        <td>microgpt의 "Production 확장" — 같은 원리, 실용적 규모</td>
      </tr>
      <tr>
        <td><a href="https://github.com/karpathy/llm.c" target="_blank" rel="noopener">llm.c</a></td>
        <td>C/CUDA로 LLM 훈련 — PyTorch조차 없이 구현</td>
        <td>의존성 제거 철학의 극한 — microgpt 방향의 시스템 레벨 확장</td>
      </tr>
    </tbody>
  </table>

  <h3>학습 영상 (Karpathy YouTube)</h3>
  <ul>
    <li><a href="https://www.youtube.com/watch?v=5NQ2NBRfctU" target="_blank" rel="noopener">안드레 카파시가 들려주는 GPT 이야기 | microgpt.py</a> — microgpt.py 전체 코드를 직접 설명하는 한국어 해설 영상</li>
    <li><a href="https://www.youtube.com/watch?v=kCc8FmEb1nY" target="_blank" rel="noopener">Let's build GPT: from scratch, in code, spelled out</a> — nanoGPT 라이브 코딩 (2시간). 학습 루프·Adam·추론을 실시간으로 구현</li>
    <li><a href="https://www.youtube.com/watch?v=VMj-3S1tku0" target="_blank" rel="noopener">The spelled-out intro to neural networks and backpropagation: building micrograd</a> — 역전파·Adam·기울기 소실 강의 (2시간25분)</li>
    <li><a href="https://www.youtube.com/watch?v=TCH_1BHY58I" target="_blank" rel="noopener">Let's build a GPT Tokenizer</a> — BPE 토크나이저 (2시간). microgpt 문자 수준 토크나이저와 비교</li>
    <li><a href="https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ" target="_blank" rel="noopener">Neural Networks: Zero to Hero (재생목록)</a> — 위 강의 전체 시리즈 (micrograd → makemore → GPT)</li>
  </ul>

  <h3>핵심 논문</h3>
  <ul>
    <li><a href="https://arxiv.org/abs/1412.6980" target="_blank" rel="noopener">Adam: A Method for Stochastic Optimization (Kingma &amp; Ba, 2014)</a> — microgpt가 구현한 Adam 옵티마이저 원논문. β1=0.9, β2=0.999, ε=1e-8 기본값 출처</li>
    <li><a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">Attention Is All You Need (Vaswani et al., 2017)</a> — Transformer 원논문. Cross-Entropy 손실 + Scaled Dot-Product 학습 방법 출처</li>
    <li><a href="https://openai.com/research/language-unsupervised" target="_blank" rel="noopener">GPT-2: Language Models are Unsupervised Multitask Learners (OpenAI, 2019)</a> — microgpt의 기준 아키텍처. 사전학습(Pretraining) 방식의 근거</li>
    <li><a href="https://arxiv.org/abs/2005.14165" target="_blank" rel="noopener">GPT-3: Language Models are Few-Shot Learners (Brown et al., 2020)</a> — nanoGPT의 Cosine LR with warmup 스케줄 출처</li>
    <li><a href="https://arxiv.org/abs/1910.07467" target="_blank" rel="noopener">Root Mean Square Layer Normalization (Zhang &amp; Sennrich, 2019)</a> — microgpt의 <code>rmsnorm</code> 수식 근거</li>
    <li><a href="https://arxiv.org/abs/2205.14135" target="_blank" rel="noopener">FlashAttention: Fast and Memory-Efficient Exact Attention (Dao et al., 2022)</a> — nanoGPT가 채택한 IO-aware 어텐션 알고리즘</li>
  </ul>

  <h3>이 주제와 관련된 vibecoding 문서</h3>
  <ul>
    <li><a href="llm-theory-foundations.html">LLM 이론 기초</a></li>
    <li><a href="llm-theory-advanced.html">LLM 이론 심화 (Transformer, Attention 상세)</a></li>
    <li><a href="llm-theory-math-appendix.html">LLM 이론 수학 부록 (어텐션 수식 유도)</a></li>
    <li><a href="llm-handbook-training.html">LLM 핸드북: 학습·정렬·추론</a></li>
    <li><a href="rag-guide.html">RAG 완전 가이드</a></li>
  </ul>
</section>

<!-- Page Navigation (이전/다음) -->
<div class="page-nav"></div>

</main>

<aside class="inline-toc">
  <div class="toc-title">목차</div>
  <div class="toc-nav"></div>
</aside>

<!-- ===== Footer ===== -->
<footer class="site-footer">
</footer>

</div>

<script src="../js/main.js"></script>
</body>
</html>
