<!DOCTYPE html>
<html lang="ko" data-theme="dark-kernel">
<head>
<!-- BEGIN: Google adsense -->
<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-2110881342960271" crossorigin="anonymous"></script>
<!-- END: Google adsense -->
<!-- BEGIN: Google adsense repair -->
<script async src="https://fundingchoicesmessages.google.com/i/pub-2110881342960271?ers=1" nonce="laI6FT8gpRxugDJv5AGJRA"></script><script nonce="laI6FT8gpRxugDJv5AGJRA">(function() {function signalGooglefcPresent() {if (!window.frames['googlefcPresent']) {if (document.body) {const iframe = document.createElement('iframe'); iframe.style = 'width: 0; height: 0; border: none; z-index: -1000; left: -1000px; top: -1000px;'; iframe.style.display = 'none'; iframe.name = 'googlefcPresent'; document.body.appendChild(iframe);} else {setTimeout(signalGooglefcPresent, 0);}}}signalGooglefcPresent();})();</script>
<!-- END: Google adsense repair -->
<!-- BEGIN: Google analytics -->
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-1VWQF060SX"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-1VWQF060SX');
</script>
<!-- END: Google analytics -->

<script>
(function(){var m=document.cookie.match(/claude_theme=([^;]+)/);if(m)document.documentElement.setAttribute('data-theme',m[1]);})();
</script>
<meta charset="UTF-8">
<meta property="og:type" content="article">
<meta property="og:site_name" content="AI Vibe Coding ê°€ì´ë“œ /with MINZKN">
<meta property="og:title" content="Ollama ì‚¬ìš©ë²•">
<meta property="og:description" content="Ollama ì‚¬ìš©ë²•: Ollamaì˜ CLI ëª…ë ¹ì–´, REST API, íŒŒë¼ë¯¸í„° ì„¤ì • ë“± ì‹¤ì „ ì‚¬ìš© ë°©ë²•ì„ ìƒì„¸íˆ ë‹¤ë£¹ë‹ˆë‹¤.">
<meta property="og:url" content="https://minzkn.com/claude/pages/ollama-usage.html">
<meta property="og:image" content="https://minzkn.com/claude/images/og-image.png">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="Ollama ì‚¬ìš©ë²•: Ollamaì˜ CLI ëª…ë ¹ì–´, REST API, íŒŒë¼ë¯¸í„° ì„¤ì • ë“± ì‹¤ì „ ì‚¬ìš© ë°©ë²•ì„ ìƒì„¸íˆ ë‹¤ë£¹ë‹ˆë‹¤.">
<meta name="keywords" content="Claude, AI, LLM, Ollama ì‚¬ìš©ë²•, CLI ê¸°ë³¸ ëª…ë ¹ì–´, REST API ì‚¬ìš©ë²•, íŒŒë¼ë¯¸í„° ì„¤ì •, ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬">
<meta name="author" content="MINZKN">
<title>Ollama ì‚¬ìš©ë²• - AI Vibe Coding ê°€ì´ë“œ /with MINZKN</title>
<link rel="icon" type="image/svg+xml" href="../images/favicon.svg">

<link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/joungkyun/font-d2coding/d2coding.css">
<link rel="stylesheet" href="../css/themes.css">
<link rel="stylesheet" href="../css/style.css">
<link rel="stylesheet" href="../css/responsive.css">
</head>
<body>
<div class="page-wrapper">
<header class="site-header"></header>
<nav class="side-nav" aria-label="ì‚¬ì´íŠ¸ ë‚´ë¹„ê²Œì´ì…˜"></nav>
<main class="main-content">
<div class="breadcrumb"></div>

<h1 id="top">Ollama ì‚¬ìš©ë²•</h1>
<p class="lead">Ollamaì˜ CLI ëª…ë ¹ì–´, REST API, íŒŒë¼ë¯¸í„° ì„¤ì • ë“± ì‹¤ì „ ì‚¬ìš© ë°©ë²•ì„ ìƒì„¸íˆ ë‹¤ë£¹ë‹ˆë‹¤.</p>

<div class="info-box warning">
  <strong>ì—…ë°ì´íŠ¸ ì•ˆë‚´:</strong> ëª¨ë¸/ìš”ê¸ˆ/ë²„ì „/ì •ì±… ë“± ì‹œì ì— ë¯¼ê°í•œ ì •ë³´ëŠ” ë³€ë™ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
  ìµœì‹  ë‚´ìš©ì€ ê³µì‹ ë¬¸ì„œë¥¼ í™•ì¸í•˜ì„¸ìš”.
</div>

<div class="info-box tip">
  <div class="info-box-title">âš¡ ë¹ ë¥¸ ì‹œì‘</div>
  <ol>
    <li><code>ollama pull llama3.2</code> - ëª¨ë¸ ë‹¤ìš´ë¡œë“œ</li>
    <li><code>ollama run llama3.2</code> - ëŒ€í™”í˜• ì‹¤í–‰</li>
    <li><code>ollama list</code> - ì„¤ì¹˜ëœ ëª¨ë¸ í™•ì¸</li>
    <li>APIë¡œ í”„ë¡œê·¸ë˜ë° ë°©ì‹ ì ‘ê·¼</li>
    <li>íŒŒë¼ë¯¸í„°ë¡œ ì¶œë ¥ í’ˆì§ˆ ì¡°ì •</li>
  </ol>
</div>

<section class="content-section">
  <h2 id="cli-basics">CLI ê¸°ë³¸ ëª…ë ¹ì–´</h2>

  <p>Ollama CLIëŠ” ëª¨ë¸ ê´€ë¦¬ì™€ ì‹¤í–‰ì„ ìœ„í•œ ê°„ë‹¨í•˜ê³  ì§ê´€ì ì¸ ì¸í„°í˜ì´ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤.</p>

  <h3 id="pull">ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (pull)</h3>

  <pre><code><span class="cmt"># ê¸°ë³¸ ì‚¬ìš©ë²•</span>
ollama pull &lt;ëª¨ë¸ëª…&gt;

<span class="cmt"># ì˜ˆì œ</span>
ollama pull llama3.2          <span class="cmt"># ìµœì‹  ë²„ì „ (latest íƒœê·¸)</span>
ollama pull llama3.2:7b       <span class="cmt"># íŠ¹ì • í¬ê¸°</span>
ollama pull llama3.2:7b-instruct-q4_0  <span class="cmt"># íŠ¹ì • ì–‘ìí™”</span>

<span class="cmt"># ë‹¤ìš´ë¡œë“œ ì§„í–‰ ìƒí™©</span>
pulling manifest
pulling 8eeb52dfb3bb... 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– 4.7 GB
pulling 73b313b5552d... 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  11 KB
pulling 0ba8f0e314b4... 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  12 KB
pulling 56bb8bd477a5... 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  96 B
pulling 1a4c3c319823... 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– 487 B
verifying sha256 digest
writing manifest
success</code></pre>

  <h3 id="run">ëª¨ë¸ ì‹¤í–‰ (run)</h3>

  <p><code>run</code> ëª…ë ¹ì–´ëŠ” ëª¨ë¸ì„ ì‹¤í–‰í•˜ê³  ëŒ€í™”í˜• ì„¸ì…˜ì„ ì‹œì‘í•©ë‹ˆë‹¤.</p>

  <pre><code><span class="cmt"># ëŒ€í™”í˜• ëª¨ë“œ</span>
ollama run llama3.2

<span class="cmt"># ì¶œë ¥ ì˜ˆì‹œ</span>
<span class="str">&gt;&gt;&gt; ì•ˆë…•í•˜ì„¸ìš”?</span>
ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?

<span class="str">&gt;&gt;&gt; íŒŒì´ì¬ìœ¼ë¡œ í”¼ë³´ë‚˜ì¹˜ ìˆ˜ì—´ í•¨ìˆ˜ë¥¼ ì‘ì„±í•´ì¤˜</span>
ë¬¼ë¡ ì…ë‹ˆë‹¤. ì¬ê·€ì™€ ë©”ëª¨ì´ì œì´ì…˜ì„ ì‚¬ìš©í•œ íš¨ìœ¨ì ì¸ êµ¬í˜„ì…ë‹ˆë‹¤:

<span class="kw">def</span> <span class="fn">fibonacci</span>(n, memo={}):
    <span class="kw">if</span> n <span class="kw">in</span> memo:
        <span class="kw">return</span> memo[n]
    <span class="kw">if</span> n &lt;= <span class="num">1</span>:
        <span class="kw">return</span> n
    memo[n] = fibonacci(n<span class="num">-1</span>, memo) + fibonacci(n<span class="num">-2</span>, memo)
    <span class="kw">return</span> memo[n]

<span class="str">&gt;&gt;&gt; /bye</span>
<span class="cmt"># ëŒ€í™” ì¢…ë£Œ</span></code></pre>

  <h3 id="single-prompt">ë‹¨ì¼ í”„ë¡¬í”„íŠ¸ ì‹¤í–‰</h3>

  <p>ëŒ€í™”í˜• ëª¨ë“œ ì—†ì´ ì¦‰ì‹œ ì‘ë‹µì„ ë°›ê³  ì¢…ë£Œí•©ë‹ˆë‹¤.</p>

  <pre><code><span class="cmt"># ì¸ë¼ì¸ í”„ë¡¬í”„íŠ¸</span>
ollama run llama3.2 <span class="str">"Explain quantum computing in one sentence"</span>

<span class="cmt"># íŒŒì¼ì—ì„œ í”„ë¡¬í”„íŠ¸ ì½ê¸°</span>
ollama run llama3.2 &lt; prompt.txt

<span class="cmt"># ì¶œë ¥ì„ íŒŒì¼ë¡œ ì €ì¥</span>
ollama run llama3.2 <span class="str">"Write a haiku about coding"</span> &gt; haiku.txt

<span class="cmt"># íŒŒì´í”„ë¼ì¸ì—ì„œ ì‚¬ìš©</span>
<span class="fn">cat</span> document.txt | ollama run llama3.2 <span class="str">"Summarize this text"</span></code></pre>

  <h3 id="list">ì„¤ì¹˜ëœ ëª¨ë¸ í™•ì¸ (list)</h3>

  <pre><code><span class="cmt"># ëª¨ë“  ë¡œì»¬ ëª¨ë¸ ë‚˜ì—´</span>
ollama list

<span class="cmt"># ì¶œë ¥ ì˜ˆì‹œ</span>
NAME                          ID              SIZE      MODIFIED
llama3.2:latest               a80c4f17acd5    4.7 GB    3 minutes ago
mistral:7b-instruct           2ae6f6dd7a3d    4.1 GB    2 days ago
codellama:7b                  8fdf8f752f6e    3.8 GB    1 week ago
phi3:mini                     64c1188f2485    2.3 GB    2 weeks ago</code></pre>

  <h3 id="show">ëª¨ë¸ ì •ë³´ í™•ì¸ (show)</h3>

  <pre><code><span class="cmt"># ëª¨ë¸ ìƒì„¸ ì •ë³´</span>
ollama show llama3.2

<span class="cmt"># ì¶œë ¥ ì˜ˆì‹œ</span>
  Model
    architecture        llama
    parameters          7.2B
    context length      131072
    embedding length    4096
    quantization        Q4_0

  Parameters
    stop    "&lt;|start_header_id|&gt;"
    stop    "&lt;|end_header_id|&gt;"
    stop    "&lt;|eot_id|&gt;"

  License
    LLAMA 3.2 COMMUNITY LICENSE AGREEMENT

  System
    You are a helpful assistant.

<span class="cmt"># Modelfile ì¶œë ¥</span>
ollama show --modelfile llama3.2

FROM /path/to/model/weights
TEMPLATE <span class="str">"""{{ .System }}
{{ .Prompt }}"""</span>
PARAMETER stop <span class="str">"&lt;|start_header_id|&gt;"</span>
SYSTEM <span class="str">"You are a helpful assistant."</span></code></pre>

  <h3 id="rm">ëª¨ë¸ ì‚­ì œ (rm)</h3>

  <pre><code><span class="cmt"># ëª¨ë¸ ì œê±°</span>
ollama rm llama3.2:7b-instruct-q4_0

<span class="cmt"># ì—¬ëŸ¬ ëª¨ë¸ í•œ ë²ˆì— ì œê±°</span>
ollama rm model1 model2 model3

<span class="cmt"># í™•ì¸ ë©”ì‹œì§€</span>
deleted <span class="str">'llama3.2:7b-instruct-q4_0'</span></code></pre>

  <h3 id="ps">ì‹¤í–‰ ì¤‘ì¸ ëª¨ë¸ í™•ì¸ (ps)</h3>

  <pre><code><span class="cmt"># í˜„ì¬ ì‹¤í–‰ ì¤‘ì¸ ëª¨ë¸</span>
ollama ps

<span class="cmt"># ì¶œë ¥ ì˜ˆì‹œ</span>
NAME              ID              SIZE      PROCESSOR    UNTIL
llama3.2:latest   a80c4f17acd5    7.4 GB    100% GPU     4 minutes from now

<span class="cmt"># ì„¤ëª…</span>
<span class="cmt"># - SIZE: ë©”ëª¨ë¦¬ì— ë¡œë“œëœ í¬ê¸°</span>
<span class="cmt"># - PROCESSOR: GPU ë˜ëŠ” CPU ì‚¬ìš©ë¥ </span>
<span class="cmt"># - UNTIL: ì–¸ë¡œë“œ ì˜ˆì • ì‹œê°„ (idle timeout)</span></code></pre>

  <h3 id="cp">ëª¨ë¸ ë³µì‚¬ (cp)</h3>

  <pre><code><span class="cmt"># ëª¨ë¸ì„ ìƒˆ ì´ë¦„ìœ¼ë¡œ ë³µì‚¬</span>
ollama cp llama3.2:latest my-custom-llama

<span class="cmt"># ì‚¬ìš© ì‚¬ë¡€: ì»¤ìŠ¤í…€ ë²„ì „ ê´€ë¦¬</span>
ollama cp llama3.2 llama3.2-backup
<span class="cmt"># ì´í›„ llama3.2ë¥¼ ì»¤ìŠ¤í„°ë§ˆì´ì§•í•˜ê³  ì›ë³¸ì€ ë°±ì—…ìœ¼ë¡œ ìœ ì§€</span></code></pre>

  <h3 id="create">ì»¤ìŠ¤í…€ ëª¨ë¸ ìƒì„± (create)</h3>

  <pre><code><span class="cmt"># Modelfileì„ ì‚¬ìš©í•´ ìƒˆ ëª¨ë¸ ìƒì„±</span>
ollama create my-model -f Modelfile

<span class="cmt"># Modelfile ì˜ˆì‹œ</span>
FROM llama3.2:7b

<span class="cmt"># ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì»¤ìŠ¤í„°ë§ˆì´ì§•</span>
SYSTEM <span class="str">"""ë‹¹ì‹ ì€ í•œêµ­ì–´ ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
í•­ìƒ ì •ì¤‘í•˜ê³  ìƒì„¸í•˜ê²Œ ë‹µë³€í•©ë‹ˆë‹¤."""</span>

<span class="cmt"># íŒŒë¼ë¯¸í„° ì¡°ì •</span>
PARAMETER temperature <span class="num">0.8</span>
PARAMETER top_p <span class="num">0.9</span></code></pre>

  <h3 id="serve">ì„œë²„ ëª¨ë“œ (serve)</h3>

  <pre><code><span class="cmt"># Ollama API ì„œë²„ ì‹œì‘ (ë³´í†µ ìë™ ì‹œì‘ë¨)</span>
ollama serve

<span class="cmt"># ê¸°ë³¸ í¬íŠ¸: 11434</span>
<span class="cmt"># í™˜ê²½ ë³€ìˆ˜ë¡œ í¬íŠ¸ ë³€ê²½</span>
OLLAMA_HOST=<span class="num">0.0.0.0</span>:<span class="num">8080</span> ollama serve

<span class="cmt"># ë¡œê·¸ í™•ì¸</span>
time=2024-01-15T10:30:45.123+09:00 level=INFO source=server.go:123 msg=<span class="str">"Listening on 127.0.0.1:11434"</span></code></pre>

  <div class="info-box info">
    <div class="info-box-title">ğŸ’¡ CLI íŒ</div>
    <ul>
      <li>ëŒ€í™” ì¤‘ <code>/bye</code> ì…ë ¥ìœ¼ë¡œ ì¢…ë£Œ</li>
      <li><code>/set parameter value</code>ë¡œ ì‹¤í–‰ ì¤‘ íŒŒë¼ë¯¸í„° ë³€ê²½</li>
      <li><code>/show</code>ë¡œ í˜„ì¬ ì„¤ì • í™•ì¸</li>
      <li><code>/load &lt;filepath&gt;</code>ë¡œ íŒŒì¼ ë‚´ìš©ì„ ì»¨í…ìŠ¤íŠ¸ì— ì¶”ê°€</li>
      <li><kbd>Ctrl+D</kbd>ë¡œ ëŒ€í™” ì¢…ë£Œ</li>
    </ul>
  </div>

  <h3 id="cli-workflow">ì¼ë°˜ì ì¸ CLI ì›Œí¬í”Œë¡œìš°</h3>

  <svg viewBox="0 0 800 450" style="max-width: 100%; height: auto; background: var(--diagram-fill); border-radius: 8px; padding: 20px;">
    <defs>
      <marker id="ollama-usage-1-arrow" markerWidth="10" markerHeight="10" refX="9" refY="3" orient="auto">
        <polygon points="0 0, 10 3, 0 6" fill="var(--accent-primary)" />
      </marker>
    </defs>

    <!-- Steps -->
    <rect x="50" y="50" width="180" height="60" rx="8" fill="var(--accent-primary)" stroke="var(--diagram-stroke)" stroke-width="2"/>
    <text x="140" y="75" text-anchor="middle" fill="var(--bg-primary)" font-weight="bold" font-size="14">1. ollama pull</text>
    <text x="140" y="95" text-anchor="middle" fill="var(--bg-primary)" font-size="12">ëª¨ë¸ ë‹¤ìš´ë¡œë“œ</text>

    <path d="M 230 80 L 280 80" stroke="var(--accent-primary)" stroke-width="2" marker-end="url(#ollama-usage-1-arrow)"/>

    <rect x="280" y="50" width="180" height="60" rx="8" fill="var(--accent-secondary)" stroke="var(--diagram-stroke)" stroke-width="2"/>
    <text x="370" y="75" text-anchor="middle" fill="var(--text-primary)" font-weight="bold" font-size="14">2. ollama list</text>
    <text x="370" y="95" text-anchor="middle" fill="var(--text-secondary)" font-size="12">ì„¤ì¹˜ í™•ì¸</text>

    <path d="M 460 80 L 510 80" stroke="var(--accent-primary)" stroke-width="2" marker-end="url(#ollama-usage-1-arrow)"/>

    <rect x="510" y="50" width="180" height="60" rx="8" fill="var(--accent-tertiary)" stroke="var(--diagram-stroke)" stroke-width="2"/>
    <text x="600" y="75" text-anchor="middle" fill="var(--text-primary)" font-weight="bold" font-size="14">3. ollama run</text>
    <text x="600" y="95" text-anchor="middle" fill="var(--text-secondary)" font-size="12">ì‹¤í–‰</text>

    <path d="M 600 110 L 600 160" stroke="var(--accent-primary)" stroke-width="2" marker-end="url(#ollama-usage-1-arrow)"/>

    <rect x="510" y="160" width="180" height="60" rx="8" fill="var(--accent-primary)" stroke="var(--diagram-stroke)" stroke-width="2"/>
    <text x="600" y="185" text-anchor="middle" fill="var(--text-primary)" font-weight="bold" font-size="14">4. ëŒ€í™”/ì‘ì—…</text>
    <text x="600" y="205" text-anchor="middle" fill="var(--text-secondary)" font-size="12">í”„ë¡¬í”„íŠ¸ ì…ë ¥</text>

    <path d="M 510 190 L 280 190" stroke="var(--text-tertiary)" stroke-width="2" stroke-dasharray="5,5"/>
    <text x="395" y="180" text-anchor="middle" fill="var(--text-tertiary)" font-size="11">ë§Œì¡±ìŠ¤ëŸ½ì§€ ì•Šìœ¼ë©´</text>

    <rect x="280" y="160" width="180" height="60" rx="8" fill="var(--bg-tertiary)" stroke="var(--border-primary)" stroke-width="2"/>
    <text x="370" y="185" text-anchor="middle" fill="var(--text-secondary)" font-weight="bold" font-size="14">ë‹¤ë¥¸ ëª¨ë¸ ì‹œë„</text>
    <text x="370" y="205" text-anchor="middle" fill="var(--text-tertiary)" font-size="12">ollama pull</text>

    <path d="M 370 220 L 370 260" stroke="var(--accent-primary)" stroke-width="2" marker-end="url(#ollama-usage-1-arrow)"/>

    <rect x="280" y="260" width="180" height="60" rx="8" fill="var(--accent-warning)" stroke="var(--diagram-stroke)" stroke-width="2"/>
    <text x="370" y="285" text-anchor="middle" fill="var(--text-primary)" font-weight="bold" font-size="14">5. ìµœì í™”</text>
    <text x="370" y="305" text-anchor="middle" fill="var(--text-secondary)" font-size="12">íŒŒë¼ë¯¸í„° ì¡°ì •</text>

    <path d="M 370 320 L 370 360" stroke="var(--accent-primary)" stroke-width="2" marker-end="url(#ollama-usage-1-arrow)"/>

    <rect x="280" y="360" width="180" height="60" rx="8" fill="var(--accent-primary)" stroke="var(--diagram-stroke)" stroke-width="2"/>
    <text x="370" y="385" text-anchor="middle" fill="var(--bg-primary)" font-weight="bold" font-size="14">6. í†µí•©</text>
    <text x="370" y="405" text-anchor="middle" fill="var(--bg-primary)" font-size="12">APIë¡œ ì•± ì—°ë™</text>

    <!-- Optional: cleanup -->
    <rect x="50" y="260" width="180" height="60" rx="8" fill="var(--bg-tertiary)" stroke="var(--border-primary)" stroke-width="1" stroke-dasharray="5,5"/>
    <text x="140" y="285" text-anchor="middle" fill="var(--text-tertiary)" font-weight="bold" font-size="14">ollama rm</text>
    <text x="140" y="305" text-anchor="middle" fill="var(--text-tertiary)" font-size="12">ë¶ˆí•„ìš” ëª¨ë¸ ì‚­ì œ</text>
  </svg>
</section>

<section class="content-section">
  <h2 id="api-usage">REST API ì‚¬ìš©ë²•</h2>

  <p>OllamaëŠ” HTTP REST APIë¥¼ ì œê³µí•˜ì—¬ í”„ë¡œê·¸ë˜ë° ë°©ì‹ìœ¼ë¡œ ëª¨ë¸ê³¼ ìƒí˜¸ì‘ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>

  <h3 id="api-endpoints">ì£¼ìš” API ì—”ë“œí¬ì¸íŠ¸</h3>

  <table>
    <thead>
      <tr>
        <th>ì—”ë“œí¬ì¸íŠ¸</th>
        <th>ë©”ì„œë“œ</th>
        <th>ì„¤ëª…</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><code>/api/generate</code></td>
        <td>POST</td>
        <td>í…ìŠ¤íŠ¸ ìƒì„± (ë‹¨ì¼ í”„ë¡¬í”„íŠ¸)</td>
      </tr>
      <tr>
        <td><code>/api/chat</code></td>
        <td>POST</td>
        <td>ì±„íŒ… ëŒ€í™” (ë©€í‹°í„´)</td>
      </tr>
      <tr>
        <td><code>/api/embeddings</code></td>
        <td>POST</td>
        <td>í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±</td>
      </tr>
      <tr>
        <td><code>/api/tags</code></td>
        <td>GET</td>
        <td>ë¡œì»¬ ëª¨ë¸ ëª©ë¡</td>
      </tr>
      <tr>
        <td><code>/api/show</code></td>
        <td>POST</td>
        <td>ëª¨ë¸ ì •ë³´ ì¡°íšŒ</td>
      </tr>
      <tr>
        <td><code>/api/pull</code></td>
        <td>POST</td>
        <td>ëª¨ë¸ ë‹¤ìš´ë¡œë“œ</td>
      </tr>
      <tr>
        <td><code>/api/push</code></td>
        <td>POST</td>
        <td>ëª¨ë¸ ì—…ë¡œë“œ</td>
      </tr>
      <tr>
        <td><code>/api/delete</code></td>
        <td>DELETE</td>
        <td>ëª¨ë¸ ì‚­ì œ</td>
      </tr>
    </tbody>
  </table>

  <h3 id="generate-api">í…ìŠ¤íŠ¸ ìƒì„± API (/api/generate)</h3>

  <h4>ê¸°ë³¸ ì‚¬ìš©</h4>

  <pre><code><span class="cmt"># curl ì‚¬ìš©</span>
curl http://localhost:<span class="num">11434</span>/api/generate -d <span class="str">'{
  "model": "llama3.2",
  "prompt": "Why is the sky blue?"
}'</span>

<span class="cmt"># ì‘ë‹µ (ìŠ¤íŠ¸ë¦¬ë°)</span>
{<span class="str">"model"</span>:<span class="str">"llama3.2"</span>,<span class="str">"created_at"</span>:<span class="str">"2024-01-15T10:30:45Z"</span>,<span class="str">"response"</span>:<span class="str">"The"</span>,<span class="str">"done"</span>:<span class="kw">false</span>}
{<span class="str">"model"</span>:<span class="str">"llama3.2"</span>,<span class="str">"created_at"</span>:<span class="str">"2024-01-15T10:30:45Z"</span>,<span class="str">"response"</span>:<span class="str">" sky"</span>,<span class="str">"done"</span>:<span class="kw">false</span>}
{<span class="str">"model"</span>:<span class="str">"llama3.2"</span>,<span class="str">"created_at"</span>:<span class="str">"2024-01-15T10:30:45Z"</span>,<span class="str">"response"</span>:<span class="str">" appears"</span>,<span class="str">"done"</span>:<span class="kw">false</span>}
...
{<span class="str">"model"</span>:<span class="str">"llama3.2"</span>,<span class="str">"done"</span>:<span class="kw">true</span>,<span class="str">"total_duration"</span>:<span class="num">2500000000</span>,<span class="str">"prompt_eval_count"</span>:<span class="num">10</span>}</code></pre>

  <h4>ìŠ¤íŠ¸ë¦¬ë° ë¹„í™œì„±í™”</h4>

  <pre><code>curl http://localhost:<span class="num">11434</span>/api/generate -d <span class="str">'{
  "model": "llama3.2",
  "prompt": "Why is the sky blue?",
  "stream": false
}'</span>

<span class="cmt"># ì‘ë‹µ (ì „ì²´ í…ìŠ¤íŠ¸ í•œ ë²ˆì—)</span>
{
  <span class="str">"model"</span>: <span class="str">"llama3.2"</span>,
  <span class="str">"created_at"</span>: <span class="str">"2024-01-15T10:30:45Z"</span>,
  <span class="str">"response"</span>: <span class="str">"The sky appears blue because of a phenomenon..."</span>,
  <span class="str">"done"</span>: <span class="kw">true</span>,
  <span class="str">"total_duration"</span>: <span class="num">2500000000</span>,
  <span class="str">"load_duration"</span>: <span class="num">500000000</span>,
  <span class="str">"prompt_eval_count"</span>: <span class="num">10</span>,
  <span class="str">"prompt_eval_duration"</span>: <span class="num">800000000</span>,
  <span class="str">"eval_count"</span>: <span class="num">50</span>,
  <span class="str">"eval_duration"</span>: <span class="num">1200000000</span>
}</code></pre>

  <h3 id="chat-api">ì±„íŒ… API (/api/chat)</h3>

  <p>ë©€í‹°í„´ ëŒ€í™”ë¥¼ ìœ„í•œ APIì…ë‹ˆë‹¤. ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ìœ ì§€í•˜ë ¤ë©´ ì´ì „ ë©”ì‹œì§€ë¥¼ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.</p>

  <pre><code>curl http://localhost:<span class="num">11434</span>/api/chat -d <span class="str">'{
  "model": "llama3.2",
  "messages": [
    {
      "role": "system",
      "content": "You are a helpful assistant specializing in Python."
    },
    {
      "role": "user",
      "content": "How do I read a CSV file in Python?"
    }
  ]
}'</span>

<span class="cmt"># ë‘ ë²ˆì§¸ ë©”ì‹œì§€ (ì»¨í…ìŠ¤íŠ¸ ìœ ì§€)</span>
curl http://localhost:<span class="num">11434</span>/api/chat -d <span class="str">'{
  "model": "llama3.2",
  "messages": [
    {
      "role": "system",
      "content": "You are a helpful assistant specializing in Python."
    },
    {
      "role": "user",
      "content": "How do I read a CSV file in Python?"
    },
    {
      "role": "assistant",
      "content": "You can use the csv module or pandas..."
    },
    {
      "role": "user",
      "content": "Can you show me a pandas example?"
    }
  ]
}'</span></code></pre>

  <h3 id="python-api">Python API ì˜ˆì œ</h3>

  <h4>requests ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©</h4>

  <pre><code><span class="kw">import</span> requests
<span class="kw">import</span> json

<span class="kw">def</span> <span class="fn">generate_text</span>(prompt, model=<span class="str">"llama3.2"</span>, stream=<span class="kw">True</span>):
    url = <span class="str">"http://localhost:11434/api/generate"</span>
    payload = {
        <span class="str">"model"</span>: model,
        <span class="str">"prompt"</span>: prompt,
        <span class="str">"stream"</span>: stream
    }

    response = requests.post(url, json=payload, stream=stream)

    <span class="kw">if</span> stream:
        <span class="cmt"># ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬</span>
        full_response = <span class="str">""</span>
        <span class="kw">for</span> line <span class="kw">in</span> response.iter_lines():
            <span class="kw">if</span> line:
                data = json.loads(line)
                <span class="kw">if</span> <span class="kw">not</span> data.get(<span class="str">"done"</span>):
                    chunk = data.get(<span class="str">"response"</span>, <span class="str">""</span>)
                    full_response += chunk
                    <span class="fn">print</span>(chunk, end=<span class="str">""</span>, flush=<span class="kw">True</span>)
        <span class="fn">print</span>()  <span class="cmt"># ì¤„ë°”ê¿ˆ</span>
        <span class="kw">return</span> full_response
    <span class="kw">else</span>:
        <span class="cmt"># ë¹„ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ</span>
        data = response.json()
        <span class="kw">return</span> data.get(<span class="str">"response"</span>, <span class="str">""</span>)

<span class="cmt"># ì‚¬ìš© ì˜ˆì‹œ</span>
result = generate_text(<span class="str">"Explain machine learning in simple terms"</span>)
<span class="fn">print</span>(result)</code></pre>

  <h4>ì±„íŒ… ê¸°ëŠ¥ êµ¬í˜„</h4>

  <pre><code><span class="kw">class</span> <span class="type">OllamaChat</span>:
    <span class="kw">def</span> <span class="fn">__init__</span>(<span class="kw">self</span>, model=<span class="str">"llama3.2"</span>, system_prompt=<span class="kw">None</span>):
        <span class="kw">self</span>.model = model
        <span class="kw">self</span>.messages = []
        <span class="kw">self</span>.url = <span class="str">"http://localhost:11434/api/chat"</span>

        <span class="kw">if</span> system_prompt:
            <span class="kw">self</span>.messages.append({
                <span class="str">"role"</span>: <span class="str">"system"</span>,
                <span class="str">"content"</span>: system_prompt
            })

    <span class="kw">def</span> <span class="fn">send</span>(<span class="kw">self</span>, user_message):
        <span class="cmt"># ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€</span>
        <span class="kw">self</span>.messages.append({
            <span class="str">"role"</span>: <span class="str">"user"</span>,
            <span class="str">"content"</span>: user_message
        })

        <span class="cmt"># API í˜¸ì¶œ</span>
        payload = {
            <span class="str">"model"</span>: <span class="kw">self</span>.model,
            <span class="str">"messages"</span>: <span class="kw">self</span>.messages,
            <span class="str">"stream"</span>: <span class="kw">False</span>
        }

        response = requests.post(<span class="kw">self</span>.url, json=payload)
        data = response.json()

        <span class="cmt"># ì–´ì‹œìŠ¤í„´íŠ¸ ì‘ë‹µ ì €ì¥</span>
        assistant_message = data[<span class="str">"message"</span>][<span class="str">"content"</span>]
        <span class="kw">self</span>.messages.append({
            <span class="str">"role"</span>: <span class="str">"assistant"</span>,
            <span class="str">"content"</span>: assistant_message
        })

        <span class="kw">return</span> assistant_message

    <span class="kw">def</span> <span class="fn">reset</span>(<span class="kw">self</span>):
        <span class="cmt"># ëŒ€í™” íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”</span>
        system_msg = [m <span class="kw">for</span> m <span class="kw">in</span> <span class="kw">self</span>.messages <span class="kw">if</span> m[<span class="str">"role"</span>] == <span class="str">"system"</span>]
        <span class="kw">self</span>.messages = system_msg

<span class="cmt"># ì‚¬ìš© ì˜ˆì‹œ</span>
chat = OllamaChat(
    model=<span class="str">"llama3.2"</span>,
    system_prompt=<span class="str">"You are a helpful coding assistant."</span>
)

<span class="fn">print</span>(chat.send(<span class="str">"How do I sort a list in Python?"</span>))
<span class="fn">print</span>(chat.send(<span class="str">"What about in reverse order?"</span>))
<span class="fn">print</span>(chat.send(<span class="str">"Can I sort by a custom key?"</span>))</code></pre>

  <h3 id="javascript-api">JavaScript/Node.js API ì˜ˆì œ</h3>

  <pre><code><span class="cmt">// fetch API ì‚¬ìš©</span>
<span class="kw">async function</span> <span class="fn">generateText</span>(prompt, model = <span class="str">'llama3.2'</span>) {
  <span class="kw">const</span> response = <span class="kw">await</span> <span class="fn">fetch</span>(<span class="str">'http://localhost:11434/api/generate'</span>, {
    method: <span class="str">'POST'</span>,
    headers: { <span class="str">'Content-Type'</span>: <span class="str">'application/json'</span> },
    body: JSON.<span class="fn">stringify</span>({
      model,
      prompt,
      stream: <span class="kw">false</span>
    })
  });

  <span class="kw">const</span> data = <span class="kw">await</span> response.<span class="fn">json</span>();
  <span class="kw">return</span> data.response;
}

<span class="cmt">// ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬</span>
<span class="kw">async function</span> <span class="fn">streamText</span>(prompt, model = <span class="str">'llama3.2'</span>) {
  <span class="kw">const</span> response = <span class="kw">await</span> <span class="fn">fetch</span>(<span class="str">'http://localhost:11434/api/generate'</span>, {
    method: <span class="str">'POST'</span>,
    headers: { <span class="str">'Content-Type'</span>: <span class="str">'application/json'</span> },
    body: JSON.<span class="fn">stringify</span>({ model, prompt, stream: <span class="kw">true</span> })
  });

  <span class="kw">const</span> reader = response.body.<span class="fn">getReader</span>();
  <span class="kw">const</span> decoder = <span class="kw">new</span> <span class="type">TextDecoder</span>();

  <span class="kw">while</span> (<span class="kw">true</span>) {
    <span class="kw">const</span> { done, value } = <span class="kw">await</span> reader.<span class="fn">read</span>();
    <span class="kw">if</span> (done) <span class="kw">break</span>;

    <span class="kw">const</span> text = decoder.<span class="fn">decode</span>(value);
    <span class="kw">const</span> lines = text.<span class="fn">split</span>(<span class="str">'\n'</span>).<span class="fn">filter</span>(line =&gt; line.<span class="fn">trim</span>());

    <span class="kw">for</span> (<span class="kw">const</span> line <span class="kw">of</span> lines) {
      <span class="kw">const</span> data = JSON.<span class="fn">parse</span>(line);
      <span class="kw">if</span> (!data.done) {
        process.stdout.<span class="fn">write</span>(data.response);
      }
    }
  }
  console.<span class="fn">log</span>();
}

<span class="cmt">// ì‚¬ìš©</span>
<span class="kw">await</span> <span class="fn">streamText</span>(<span class="str">'Write a haiku about programming'</span>);</code></pre>

  <h3 id="embeddings-api">ì„ë² ë”© API</h3>

  <pre><code><span class="cmt"># ì„ë² ë”© ìƒì„±</span>
curl http://localhost:<span class="num">11434</span>/api/embeddings -d <span class="str">'{
  "model": "nomic-embed-text",
  "prompt": "The quick brown fox jumps over the lazy dog"
}'</span>

<span class="cmt"># ì‘ë‹µ</span>
{
  <span class="str">"embedding"</span>: [<span class="num">0.123</span>, <span class="num">-0.456</span>, <span class="num">0.789</span>, ...]  <span class="cmt">// 768ì°¨ì›</span>
}

<span class="cmt"># Python ì˜ˆì œ</span>
<span class="kw">def</span> <span class="fn">get_embedding</span>(text, model=<span class="str">"nomic-embed-text"</span>):
    response = requests.post(
        <span class="str">"http://localhost:11434/api/embeddings"</span>,
        json={<span class="str">"model"</span>: model, <span class="str">"prompt"</span>: text}
    )
    <span class="kw">return</span> response.json()[<span class="str">"embedding"</span>]

<span class="cmt"># ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°</span>
<span class="kw">from</span> numpy <span class="kw">import</span> dot
<span class="kw">from</span> numpy.linalg <span class="kw">import</span> norm

<span class="kw">def</span> <span class="fn">cosine_similarity</span>(a, b):
    <span class="kw">return</span> dot(a, b) / (norm(a) * norm(b))

emb1 = get_embedding(<span class="str">"I love programming"</span>)
emb2 = get_embedding(<span class="str">"I enjoy coding"</span>)
emb3 = get_embedding(<span class="str">"The weather is nice"</span>)

<span class="fn">print</span>(cosine_similarity(emb1, emb2))  <span class="cmt"># ë†’ì€ ìœ ì‚¬ë„</span>
<span class="fn">print</span>(cosine_similarity(emb1, emb3))  <span class="cmt"># ë‚®ì€ ìœ ì‚¬ë„</span></code></pre>
</section>

<section class="content-section">
  <h2 id="parameters">íŒŒë¼ë¯¸í„° ì„¤ì •</h2>

  <p>OllamaëŠ” ë‹¤ì–‘í•œ íŒŒë¼ë¯¸í„°ë¥¼ í†µí•´ ëª¨ë¸ì˜ ì¶œë ¥ì„ ì„¸ë°€í•˜ê²Œ ì¡°ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>

  <h3 id="parameter-list">ì£¼ìš” íŒŒë¼ë¯¸í„°</h3>

  <table>
    <thead>
      <tr>
        <th>íŒŒë¼ë¯¸í„°</th>
        <th>ê¸°ë³¸ê°’</th>
        <th>ë²”ìœ„</th>
        <th>ì„¤ëª…</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><code>temperature</code></td>
        <td>0.8</td>
        <td>0.0 - 2.0</td>
        <td>ì°½ì˜ì„± vs ì¼ê´€ì„±</td>
      </tr>
      <tr>
        <td><code>top_p</code></td>
        <td>0.9</td>
        <td>0.0 - 1.0</td>
        <td>ëˆ„ì  í™•ë¥  ìƒ˜í”Œë§</td>
      </tr>
      <tr>
        <td><code>top_k</code></td>
        <td>40</td>
        <td>1 - 100</td>
        <td>ìƒìœ„ kê°œ í† í°ë§Œ ê³ ë ¤</td>
      </tr>
      <tr>
        <td><code>num_ctx</code></td>
        <td>2048</td>
        <td>128 - 128000</td>
        <td>ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° í¬ê¸°</td>
      </tr>
      <tr>
        <td><code>num_predict</code></td>
        <td>-1</td>
        <td>-1 or 1+</td>
        <td>ìµœëŒ€ ìƒì„± í† í° ìˆ˜</td>
      </tr>
      <tr>
        <td><code>repeat_penalty</code></td>
        <td>1.1</td>
        <td>0.0 - 2.0</td>
        <td>ë°˜ë³µ ì–µì œ ê°•ë„</td>
      </tr>
      <tr>
        <td><code>seed</code></td>
        <td>0</td>
        <td>-</td>
        <td>ì¬í˜„ ê°€ëŠ¥í•œ ì¶œë ¥</td>
      </tr>
      <tr>
        <td><code>stop</code></td>
        <td>[]</td>
        <td>-</td>
        <td>ìƒì„± ì¤‘ë‹¨ ë¬¸ìì—´</td>
      </tr>
    </tbody>
  </table>

  <h3 id="temperature">Temperature (ì˜¨ë„)</h3>

  <p>ê°€ì¥ ì¤‘ìš”í•œ íŒŒë¼ë¯¸í„°ë¡œ, ì¶œë ¥ì˜ ë¬´ì‘ìœ„ì„±ì„ ì¡°ì ˆí•©ë‹ˆë‹¤.</p>

  <pre><code><span class="cmt"># ë‚®ì€ temperature (0.1-0.3): ê²°ì •ë¡ ì , ì¼ê´€ì </span>
curl http://localhost:<span class="num">11434</span>/api/generate -d <span class="str">'{
  "model": "llama3.2",
  "prompt": "Complete the phrase: To be or not to be",
  "options": {
    "temperature": 0.1
  }
}'</span>
<span class="cmt"># ì¶œë ¥: "that is the question" (ë§¤ìš° ì¼ê´€ì )</span>

<span class="cmt"># ë†’ì€ temperature (1.5-2.0): ì°½ì˜ì , ë‹¤ì–‘í•¨</span>
curl http://localhost:<span class="num">11434</span>/api/generate -d <span class="str">'{
  "model": "llama3.2",
  "prompt": "Complete the phrase: To be or not to be",
  "options": {
    "temperature": 1.8
  }
}'</span>
<span class="cmt"># ì¶œë ¥: "a butterfly in the moonlight" (ì°½ì˜ì ì´ì§€ë§Œ ì˜ˆì¸¡ ë¶ˆê°€)</span></code></pre>

  <svg viewBox="0 0 800 300" style="max-width: 100%; height: auto; background: var(--diagram-fill); border-radius: 8px; padding: 20px;">
    <!-- Temperature spectrum -->
    <defs>
      <linearGradient id="tempGradient" x1="0%" y1="0%" x2="100%" y2="0%">
        <stop offset="0%" style="stop-color:var(--accent-tertiary);stop-opacity:1" />
        <stop offset="50%" style="stop-color:var(--accent-primary);stop-opacity:1" />
        <stop offset="100%" style="stop-color:var(--accent-warning);stop-opacity:1" />
      </linearGradient>
    </defs>

    <rect x="100" y="100" width="600" height="50" rx="5" fill="url(#tempGradient)" stroke="var(--border-primary)" stroke-width="2"/>

    <!-- Labels -->
    <text x="100" y="90" fill="var(--text-primary)" font-size="14" font-weight="bold">0.0</text>
    <text x="395" y="90" text-anchor="middle" fill="var(--text-primary)" font-size="14" font-weight="bold">0.8 (ê¸°ë³¸)</text>
    <text x="690" y="90" text-anchor="end" fill="var(--text-primary)" font-size="14" font-weight="bold">2.0</text>

    <!-- Use cases -->
    <text x="150" y="180" fill="var(--accent-tertiary)" font-size="13" font-weight="bold">ê²°ì •ë¡ ì </text>
    <text x="150" y="200" fill="var(--text-secondary)" font-size="11">â€¢ ì½”ë“œ ìƒì„±</text>
    <text x="150" y="215" fill="var(--text-secondary)" font-size="11">â€¢ ì •í™•í•œ ë‹µë³€</text>
    <text x="150" y="230" fill="var(--text-secondary)" font-size="11">â€¢ ë²ˆì—­</text>

    <text x="400" y="180" text-anchor="middle" fill="var(--accent-primary)" font-weight="bold" font-size="13">ê· í˜•</text>
    <text x="400" y="200" text-anchor="middle" fill="var(--text-secondary)" font-size="11">â€¢ ì¼ë°˜ ëŒ€í™”</text>
    <text x="400" y="215" text-anchor="middle" fill="var(--text-secondary)" font-size="11">â€¢ ì§ˆì˜ì‘ë‹µ</text>

    <text x="650" y="180" text-anchor="end" fill="var(--accent-warning)" font-weight="bold" font-size="13">ì°½ì˜ì </text>
    <text x="650" y="200" text-anchor="end" fill="var(--text-secondary)" font-size="11">â€¢ ë¸Œë ˆì¸ìŠ¤í† ë°</text>
    <text x="650" y="215" text-anchor="end" fill="var(--text-secondary)" font-size="11">â€¢ ìŠ¤í† ë¦¬ ìƒì„±</text>
    <text x="650" y="230" text-anchor="end" fill="var(--text-secondary)" font-size="11">â€¢ ì‹œ/ìŒì•…</text>

    <!-- Markers -->
    <circle cx="100" cy="125" r="5" fill="var(--accent-tertiary)"/>
    <circle cx="400" cy="125" r="5" fill="var(--accent-primary)"/>
    <circle cx="700" cy="125" r="5" fill="var(--accent-warning)"/>
  </svg>

  <h3 id="top-p-k">Top-pì™€ Top-k</h3>

  <p>ë‘ íŒŒë¼ë¯¸í„° ëª¨ë‘ ìƒ˜í”Œë§ì„ ì œí•œí•˜ì—¬ ì¶œë ¥ í’ˆì§ˆì„ ì¡°ì ˆí•©ë‹ˆë‹¤.</p>

  <pre><code><span class="cmt"># Top-p (nucleus sampling)</span>
<span class="cmt"># ëˆ„ì  í™•ë¥ ì´ pì— ë„ë‹¬í•  ë•Œê¹Œì§€ì˜ í† í°ë§Œ ê³ ë ¤</span>
{
  <span class="str">"options"</span>: {
    <span class="str">"top_p"</span>: <span class="num">0.9</span>  <span class="cmt">// ìƒìœ„ 90% í™•ë¥  í† í°ë§Œ</span>
  }
}

<span class="cmt"># Top-k</span>
<span class="cmt"># í™•ë¥  ìƒìœ„ kê°œ í† í°ë§Œ ê³ ë ¤</span>
{
  <span class="str">"options"</span>: {
    <span class="str">"top_k"</span>: <span class="num">40</span>  <span class="cmt">// ìƒìœ„ 40ê°œ í† í°ë§Œ</span>
  }
}

<span class="cmt"># ì¡°í•© ì‚¬ìš© (ê¶Œì¥)</span>
{
  <span class="str">"options"</span>: {
    <span class="str">"temperature"</span>: <span class="num">0.7</span>,
    <span class="str">"top_p"</span>: <span class="num">0.9</span>,
    <span class="str">"top_k"</span>: <span class="num">40</span>
  }
}</code></pre>

  <h3 id="context">ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ (num_ctx)</h3>

  <p>ëª¨ë¸ì´ í•œ ë²ˆì— ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” í† í° ìˆ˜ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.</p>

  <pre><code><span class="cmt"># ê¸´ ë¬¸ì„œ ì²˜ë¦¬</span>
{
  <span class="str">"model"</span>: <span class="str">"llama3.2"</span>,
  <span class="str">"prompt"</span>: <span class="str">"Summarize this long document: ..."</span>,
  <span class="str">"options"</span>: {
    <span class="str">"num_ctx"</span>: <span class="num">8192</span>  <span class="cmt">// ê¸°ë³¸ 2048ë³´ë‹¤ í¼</span>
  }
}

<span class="cmt"># ì£¼ì˜: ë” í° ì»¨í…ìŠ¤íŠ¸ëŠ” ë” ë§ì€ ë©”ëª¨ë¦¬ì™€ ì‹œê°„ í•„ìš”</span>
<span class="cmt"># Llama 3.2ëŠ” ìµœëŒ€ 128K í† í° ì§€ì›</span></code></pre>

  <h3 id="seed">ì‹œë“œ (ì¬í˜„ì„±)</h3>

  <pre><code><span class="cmt"># ê°™ì€ ì‹œë“œ = ê°™ì€ ì¶œë ¥</span>
{
  <span class="str">"model"</span>: <span class="str">"llama3.2"</span>,
  <span class="str">"prompt"</span>: <span class="str">"Generate a random story"</span>,
  <span class="str">"options"</span>: {
    <span class="str">"seed"</span>: <span class="num">42</span>,
    <span class="str">"temperature"</span>: <span class="num">0.8</span>
  }
}

<span class="cmt"># í…ŒìŠ¤íŠ¸, ë””ë²„ê¹…, ë²¤ì¹˜ë§ˆí¬ì— ìœ ìš©</span></code></pre>

  <h3 id="parameter-presets">ì‚¬ìš© ì‚¬ë¡€ë³„ íŒŒë¼ë¯¸í„° í”„ë¦¬ì…‹</h3>

  <pre><code><span class="cmt"># 1. ì½”ë“œ ìƒì„± (ì •í™•ì„± ìš°ì„ )</span>
CODE_GENERATION = {
    <span class="str">"temperature"</span>: <span class="num">0.2</span>,
    <span class="str">"top_p"</span>: <span class="num">0.95</span>,
    <span class="str">"top_k"</span>: <span class="num">40</span>,
    <span class="str">"repeat_penalty"</span>: <span class="num">1.1</span>
}

<span class="cmt"># 2. ì°½ì˜ì  ê¸€ì“°ê¸°</span>
CREATIVE_WRITING = {
    <span class="str">"temperature"</span>: <span class="num">1.2</span>,
    <span class="str">"top_p"</span>: <span class="num">0.95</span>,
    <span class="str">"top_k"</span>: <span class="num">50</span>,
    <span class="str">"repeat_penalty"</span>: <span class="num">1.2</span>
}

<span class="cmt"># 3. ì •í™•í•œ ë‹µë³€ (QA)</span>
FACTUAL_QA = {
    <span class="str">"temperature"</span>: <span class="num">0.1</span>,
    <span class="str">"top_p"</span>: <span class="num">0.9</span>,
    <span class="str">"top_k"</span>: <span class="num">30</span>,
    <span class="str">"num_predict"</span>: <span class="num">256</span>
}

<span class="cmt"># 4. ì¼ë°˜ ëŒ€í™”</span>
CONVERSATIONAL = {
    <span class="str">"temperature"</span>: <span class="num">0.8</span>,
    <span class="str">"top_p"</span>: <span class="num">0.9</span>,
    <span class="str">"top_k"</span>: <span class="num">40</span>
}

<span class="cmt"># 5. ë²ˆì—­</span>
TRANSLATION = {
    <span class="str">"temperature"</span>: <span class="num">0.3</span>,
    <span class="str">"top_p"</span>: <span class="num">0.9</span>,
    <span class="str">"repeat_penalty"</span>: <span class="num">1.0</span>
}</code></pre>

  <div class="info-box warning">
    <div class="info-box-title">âš ï¸ íŒŒë¼ë¯¸í„° ì£¼ì˜ì‚¬í•­</div>
    <ul>
      <li><code>temperature=0</code>ì€ ì™„ì „ ê²°ì •ë¡ ì ì´ì§€ë§Œ ë°˜ë³µì ì¼ ìˆ˜ ìˆìŒ</li>
      <li><code>top_p</code>ì™€ <code>top_k</code>ë¥¼ í•¨ê»˜ ì‚¬ìš©í•˜ë©´ ë” ì œí•œì </li>
      <li><code>num_ctx</code> ì¦ê°€ëŠ” ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ í¬ê²Œ ëŠ˜ë¦¼</li>
      <li><code>repeat_penalty</code>ê°€ ë„ˆë¬´ ë†’ìœ¼ë©´ ë¹„ë¬¸ë²•ì  ì¶œë ¥ ê°€ëŠ¥</li>
      <li>ê° ëª¨ë¸ë§ˆë‹¤ ìµœì  íŒŒë¼ë¯¸í„°ê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ</li>
    </ul>
  </div>
</section>

<section class="content-section">
  <h2 id="streaming">ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬</h2>

  <p>ìŠ¤íŠ¸ë¦¬ë°ì€ í† í°ì´ ìƒì„±ë˜ëŠ” ì¦‰ì‹œ ë°›ì•„ë³¼ ìˆ˜ ìˆì–´ ì‚¬ìš©ì ê²½í—˜ì´ í–¥ìƒë©ë‹ˆë‹¤.</p>

  <h3 id="streaming-python">Python ìŠ¤íŠ¸ë¦¬ë°</h3>

  <pre><code><span class="kw">import</span> requests
<span class="kw">import</span> json

<span class="kw">def</span> <span class="fn">stream_response</span>(prompt, model=<span class="str">"llama3.2"</span>):
    url = <span class="str">"http://localhost:11434/api/generate"</span>
    payload = {
        <span class="str">"model"</span>: model,
        <span class="str">"prompt"</span>: prompt,
        <span class="str">"stream"</span>: <span class="kw">True</span>
    }

    <span class="kw">with</span> requests.post(url, json=payload, stream=<span class="kw">True</span>) <span class="kw">as</span> response:
        <span class="kw">for</span> line <span class="kw">in</span> response.iter_lines():
            <span class="kw">if</span> line:
                data = json.loads(line)

                <span class="cmt"># ìƒì„±ëœ í…ìŠ¤íŠ¸ ì¶œë ¥</span>
                <span class="kw">if</span> <span class="kw">not</span> data.get(<span class="str">"done"</span>, <span class="kw">False</span>):
                    <span class="fn">print</span>(data.get(<span class="str">"response"</span>, <span class="str">""</span>), end=<span class="str">""</span>, flush=<span class="kw">True</span>)
                <span class="kw">else</span>:
                    <span class="cmt"># ì™„ë£Œ ì •ë³´</span>
                    <span class="fn">print</span>(<span class="str">"\n\n--- Stats ---"</span>)
                    <span class="fn">print</span>(<span class="str">f"Total duration: <span class="num">{data.get('total_duration', 0) / 1e9:.2f}</span>s"</span>)
                    <span class="fn">print</span>(<span class="str">f"Tokens generated: <span class="num">{data.get('eval_count', 0)}</span>"</span>)
                    <span class="fn">print</span>(<span class="str">f"Speed: <span class="num">{data.get('eval_count', 0) / (data.get('eval_duration', 1) / 1e9):.1f}</span> tokens/s"</span>)

<span class="cmt"># ì‚¬ìš©</span>
stream_response(<span class="str">"Write a short story about a robot"</span>)</code></pre>

  <h3 id="streaming-js">JavaScript ìŠ¤íŠ¸ë¦¬ë° (ë¸Œë¼ìš°ì €)</h3>

  <pre><code><span class="kw">async function</span> <span class="fn">streamToDOM</span>(prompt, elementId) {
  <span class="kw">const</span> element = document.<span class="fn">getElementById</span>(elementId);
  element.textContent = <span class="str">''</span>;

  <span class="kw">const</span> response = <span class="kw">await</span> <span class="fn">fetch</span>(<span class="str">'http://localhost:11434/api/generate'</span>, {
    method: <span class="str">'POST'</span>,
    headers: { <span class="str">'Content-Type'</span>: <span class="str">'application/json'</span> },
    body: JSON.<span class="fn">stringify</span>({
      model: <span class="str">'llama3.2'</span>,
      prompt,
      stream: <span class="kw">true</span>
    })
  });

  <span class="kw">const</span> reader = response.body.<span class="fn">getReader</span>();
  <span class="kw">const</span> decoder = <span class="kw">new</span> <span class="type">TextDecoder</span>();

  <span class="kw">while</span> (<span class="kw">true</span>) {
    <span class="kw">const</span> { done, value } = <span class="kw">await</span> reader.<span class="fn">read</span>();
    <span class="kw">if</span> (done) <span class="kw">break</span>;

    <span class="kw">const</span> text = decoder.<span class="fn">decode</span>(value);
    <span class="kw">const</span> lines = text.<span class="fn">split</span>(<span class="str">'\n'</span>).<span class="fn">filter</span>(l =&gt; l.<span class="fn">trim</span>());

    <span class="kw">for</span> (<span class="kw">const</span> line <span class="kw">of</span> lines) {
      <span class="kw">const</span> data = JSON.<span class="fn">parse</span>(line);
      <span class="kw">if</span> (!data.done) {
        element.textContent += data.response;
        <span class="cmt">// ìë™ ìŠ¤í¬ë¡¤</span>
        element.<span class="fn">scrollIntoView</span>({ behavior: <span class="str">'smooth'</span>, block: <span class="str">'end'</span> });
      }
    }
  }
}

<span class="cmt">// ì‚¬ìš©</span>
<span class="fn">streamToDOM</span>(<span class="str">'Explain async/await'</span>, <span class="str">'output'</span>);</code></pre>

  <h3 id="streaming-sse">Server-Sent Events (SSE) íŒ¨í„´</h3>

  <pre><code><span class="cmt"># Express.js ì„œë²„ ì˜ˆì œ</span>
<span class="kw">const</span> express = <span class="fn">require</span>(<span class="str">'express'</span>);
<span class="kw">const</span> fetch = <span class="fn">require</span>(<span class="str">'node-fetch'</span>);
<span class="kw">const</span> app = <span class="fn">express</span>();

app.<span class="fn">get</span>(<span class="str">'/stream'</span>, <span class="kw">async</span> (req, res) =&gt; {
  <span class="kw">const</span> prompt = req.query.prompt;

  res.<span class="fn">setHeader</span>(<span class="str">'Content-Type'</span>, <span class="str">'text/event-stream'</span>);
  res.<span class="fn">setHeader</span>(<span class="str">'Cache-Control'</span>, <span class="str">'no-cache'</span>);
  res.<span class="fn">setHeader</span>(<span class="str">'Connection'</span>, <span class="str">'keep-alive'</span>);

  <span class="kw">const</span> ollamaRes = <span class="kw">await</span> <span class="fn">fetch</span>(<span class="str">'http://localhost:11434/api/generate'</span>, {
    method: <span class="str">'POST'</span>,
    headers: { <span class="str">'Content-Type'</span>: <span class="str">'application/json'</span> },
    body: JSON.<span class="fn">stringify</span>({
      model: <span class="str">'llama3.2'</span>,
      prompt,
      stream: <span class="kw">true</span>
    })
  });

  <span class="kw">const</span> reader = ollamaRes.body.<span class="fn">getReader</span>();
  <span class="kw">const</span> decoder = <span class="kw">new</span> <span class="type">TextDecoder</span>();

  <span class="kw">while</span> (<span class="kw">true</span>) {
    <span class="kw">const</span> { done, value } = <span class="kw">await</span> reader.<span class="fn">read</span>();
    <span class="kw">if</span> (done) {
      res.<span class="fn">write</span>(<span class="str">'data: [DONE]\n\n'</span>);
      res.<span class="fn">end</span>();
      <span class="kw">break</span>;
    }

    <span class="kw">const</span> text = decoder.<span class="fn">decode</span>(value);
    <span class="kw">const</span> lines = text.<span class="fn">split</span>(<span class="str">'\n'</span>).<span class="fn">filter</span>(l =&gt; l.<span class="fn">trim</span>());

    <span class="kw">for</span> (<span class="kw">const</span> line <span class="kw">of</span> lines) {
      res.<span class="fn">write</span>(<span class="str">`data: <span class="num">${line}</span>\n\n`</span>);
    }
  }
});

app.<span class="fn">listen</span>(<span class="num">3000</span>);</code></pre>
</section>

<section class="content-section">
  <h2 id="multimodal">ë©€í‹°ëª¨ë‹¬ (ì´ë¯¸ì§€) ì²˜ë¦¬</h2>

  <p>OllamaëŠ” LLaVA, Bakllava, Llama 3.2 Vision ë“± ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ì„ ì§€ì›í•©ë‹ˆë‹¤.</p>

  <h3 id="vision-cli">CLIì—ì„œ ì´ë¯¸ì§€ ì‚¬ìš©</h3>

  <pre><code><span class="cmt"># ì´ë¯¸ì§€ íŒŒì¼ê³¼ í•¨ê»˜ í”„ë¡¬í”„íŠ¸</span>
ollama run llava <span class="str">"What's in this image?"</span> /path/to/image.jpg

<span class="cmt"># ì—¬ëŸ¬ ì´ë¯¸ì§€</span>
ollama run llava <span class="str">"Compare these images"</span> image1.jpg image2.jpg

<span class="cmt"># ëŒ€í™”í˜• ëª¨ë“œì—ì„œ</span>
ollama run llava
<span class="str">&gt;&gt;&gt; Describe this image /path/to/photo.png</span>
This image shows a beautiful sunset over a mountain landscape...

<span class="str">&gt;&gt;&gt; What colors do you see? /path/to/photo.png</span>
The dominant colors are orange, pink, and purple in the sky...</code></pre>

  <h3 id="vision-api">APIë¡œ ì´ë¯¸ì§€ ì „ì†¡</h3>

  <pre><code><span class="cmt"># Base64 ì¸ì½”ë”© í•„ìš”</span>
<span class="kw">import</span> base64
<span class="kw">import</span> requests

<span class="kw">def</span> <span class="fn">analyze_image</span>(image_path, prompt):
    <span class="cmt"># ì´ë¯¸ì§€ë¥¼ base64ë¡œ ì¸ì½”ë”©</span>
    <span class="kw">with</span> <span class="fn">open</span>(image_path, <span class="str">"rb"</span>) <span class="kw">as</span> img_file:
        image_data = base64.b64encode(img_file.read()).decode(<span class="str">'utf-8'</span>)

    <span class="cmt"># API í˜¸ì¶œ</span>
    response = requests.post(
        <span class="str">"http://localhost:11434/api/generate"</span>,
        json={
            <span class="str">"model"</span>: <span class="str">"llava"</span>,
            <span class="str">"prompt"</span>: prompt,
            <span class="str">"images"</span>: [image_data],
            <span class="str">"stream"</span>: <span class="kw">False</span>
        }
    )

    <span class="kw">return</span> response.json()[<span class="str">"response"</span>]

<span class="cmt"># ì‚¬ìš© ì˜ˆì‹œ</span>
result = analyze_image(
    <span class="str">"screenshot.png"</span>,
    <span class="str">"What UI elements are visible in this screenshot?"</span>
)
<span class="fn">print</span>(result)</code></pre>

  <h3 id="vision-usecases">ë¹„ì „ ëª¨ë¸ í™œìš© ì‚¬ë¡€</h3>

  <pre><code><span class="cmt"># 1. OCR (í…ìŠ¤íŠ¸ ì¶”ì¶œ)</span>
analyze_image(<span class="str">"document.jpg"</span>, <span class="str">"Extract all text from this image"</span>)

<span class="cmt"># 2. ì´ë¯¸ì§€ ì„¤ëª…</span>
analyze_image(<span class="str">"photo.jpg"</span>, <span class="str">"Describe this image in detail"</span>)

<span class="cmt"># 3. ê°ì²´ ê°ì§€</span>
analyze_image(<span class="str">"scene.jpg"</span>, <span class="str">"List all objects you can see"</span>)

<span class="cmt"># 4. ë‹¤ì´ì–´ê·¸ë¨ ë¶„ì„</span>
analyze_image(<span class="str">"flowchart.png"</span>, <span class="str">"Explain this flowchart step by step"</span>)

<span class="cmt"># 5. ì½”ë“œ ìŠ¤í¬ë¦°ìƒ·</span>
analyze_image(<span class="str">"code.png"</span>, <span class="str">"What does this code do? Find any bugs."</span>)

<span class="cmt"># 6. UI/UX ë¶„ì„</span>
analyze_image(<span class="str">"website.png"</span>, <span class="str">"Critique this web design"</span>)

<span class="cmt"># 7. ì°¨íŠ¸/ê·¸ë˜í”„ í•´ì„</span>
analyze_image(<span class="str">"chart.png"</span>, <span class="str">"Summarize the trends in this chart"</span>)</code></pre>

  <div class="info-box tip">
    <div class="info-box-title">ğŸ’¡ ë¹„ì „ ëª¨ë¸ íŒ</div>
    <ul>
      <li>ê³ í•´ìƒë„ ì´ë¯¸ì§€ëŠ” ìë™ìœ¼ë¡œ ë¦¬ì‚¬ì´ì¦ˆë¨</li>
      <li>ë³µì¡í•œ ì´ë¯¸ì§€ëŠ” ë” í° ëª¨ë¸(13B+) ì‚¬ìš© ê¶Œì¥</li>
      <li>ëª…í™•í•˜ê³  êµ¬ì²´ì ì¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©</li>
      <li>OCRì€ ì¸ì‡„ëœ í…ìŠ¤íŠ¸ì—ì„œ ê°€ì¥ ì˜ ì‘ë™</li>
      <li>ì—¬ëŸ¬ ê°ë„/ë²„ì „ì˜ ì´ë¯¸ì§€ë¡œ í…ŒìŠ¤íŠ¸</li>
    </ul>
  </div>
</section>

<section class="content-section">
  <h2 id="conversation">ë©€í‹°í„´ ëŒ€í™” êµ¬í˜„</h2>

  <p>ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ìœ ì§€í•˜ë©´ì„œ ì—°ì†ì ì¸ ëŒ€í™”ë¥¼ êµ¬í˜„í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.</p>

  <h3 id="conversation-class">ëŒ€í™” ê´€ë¦¬ í´ë˜ìŠ¤</h3>

  <pre><code><span class="kw">import</span> requests
<span class="kw">from</span> typing <span class="kw">import</span> List, Dict

<span class="kw">class</span> <span class="type">Conversation</span>:
    <span class="kw">def</span> <span class="fn">__init__</span>(
        <span class="kw">self</span>,
        model: <span class="type">str</span> = <span class="str">"llama3.2"</span>,
        system_prompt: <span class="type">str</span> = <span class="kw">None</span>,
        api_url: <span class="type">str</span> = <span class="str">"http://localhost:11434/api/chat"</span>
    ):
        <span class="kw">self</span>.model = model
        <span class="kw">self</span>.api_url = api_url
        <span class="kw">self</span>.messages: List[Dict] = []

        <span class="kw">if</span> system_prompt:
            <span class="kw">self</span>.messages.append({
                <span class="str">"role"</span>: <span class="str">"system"</span>,
                <span class="str">"content"</span>: system_prompt
            })

    <span class="kw">def</span> <span class="fn">send</span>(<span class="kw">self</span>, message: <span class="type">str</span>, stream: <span class="type">bool</span> = <span class="kw">False</span>) -&gt; <span class="type">str</span>:
        <span class="cmt">"""ë©”ì‹œì§€ ì „ì†¡ ë° ì‘ë‹µ ë°›ê¸°"""</span>
        <span class="kw">self</span>.messages.append({
            <span class="str">"role"</span>: <span class="str">"user"</span>,
            <span class="str">"content"</span>: message
        })

        response = requests.post(
            <span class="kw">self</span>.api_url,
            json={
                <span class="str">"model"</span>: <span class="kw">self</span>.model,
                <span class="str">"messages"</span>: <span class="kw">self</span>.messages,
                <span class="str">"stream"</span>: stream
            },
            stream=stream
        )

        <span class="kw">if</span> stream:
            <span class="kw">return</span> <span class="kw">self</span>._handle_stream(response)
        <span class="kw">else</span>:
            data = response.json()
            assistant_msg = data[<span class="str">"message"</span>][<span class="str">"content"</span>]
            <span class="kw">self</span>.messages.append({
                <span class="str">"role"</span>: <span class="str">"assistant"</span>,
                <span class="str">"content"</span>: assistant_msg
            })
            <span class="kw">return</span> assistant_msg

    <span class="kw">def</span> <span class="fn">_handle_stream</span>(<span class="kw">self</span>, response):
        <span class="cmt">"""ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬"""</span>
        full_response = <span class="str">""</span>
        <span class="kw">for</span> line <span class="kw">in</span> response.iter_lines():
            <span class="kw">if</span> line:
                <span class="kw">import</span> json
                data = json.loads(line)
                <span class="kw">if</span> <span class="kw">not</span> data.get(<span class="str">"done"</span>):
                    chunk = data.get(<span class="str">"message"</span>, {}).get(<span class="str">"content"</span>, <span class="str">""</span>)
                    full_response += chunk
                    <span class="fn">print</span>(chunk, end=<span class="str">""</span>, flush=<span class="kw">True</span>)
        <span class="fn">print</span>()
        <span class="kw">self</span>.messages.append({
            <span class="str">"role"</span>: <span class="str">"assistant"</span>,
            <span class="str">"content"</span>: full_response
        })
        <span class="kw">return</span> full_response

    <span class="kw">def</span> <span class="fn">get_history</span>(<span class="kw">self</span>) -&gt; List[Dict]:
        <span class="cmt">"""ëŒ€í™” íˆìŠ¤í† ë¦¬ ë°˜í™˜"""</span>
        <span class="kw">return</span> <span class="kw">self</span>.messages.copy()

    <span class="kw">def</span> <span class="fn">clear</span>(<span class="kw">self</span>):
        <span class="cmt">"""ëŒ€í™” ì´ˆê¸°í™” (ì‹œìŠ¤í…œ ë©”ì‹œì§€ ìœ ì§€)"""</span>
        system_msgs = [m <span class="kw">for</span> m <span class="kw">in</span> <span class="kw">self</span>.messages <span class="kw">if</span> m[<span class="str">"role"</span>] == <span class="str">"system"</span>]
        <span class="kw">self</span>.messages = system_msgs

    <span class="kw">def</span> <span class="fn">save</span>(<span class="kw">self</span>, filepath: <span class="type">str</span>):
        <span class="cmt">"""ëŒ€í™” ì €ì¥"""</span>
        <span class="kw">import</span> json
        <span class="kw">with</span> <span class="fn">open</span>(filepath, <span class="str">"w"</span>) <span class="kw">as</span> f:
            json.dump({
                <span class="str">"model"</span>: <span class="kw">self</span>.model,
                <span class="str">"messages"</span>: <span class="kw">self</span>.messages
            }, f, indent=<span class="num">2</span>)

    <span class="str">@classmethod</span>
    <span class="kw">def</span> <span class="fn">load</span>(<span class="type">cls</span>, filepath: <span class="type">str</span>):
        <span class="cmt">"""ì €ì¥ëœ ëŒ€í™” ë¶ˆëŸ¬ì˜¤ê¸°"""</span>
        <span class="kw">import</span> json
        <span class="kw">with</span> <span class="fn">open</span>(filepath) <span class="kw">as</span> f:
            data = json.load(f)
        conv = <span class="type">cls</span>(model=data[<span class="str">"model"</span>])
        conv.messages = data[<span class="str">"messages"</span>]
        <span class="kw">return</span> conv

<span class="cmt"># ì‚¬ìš© ì˜ˆì‹œ</span>
conv = Conversation(
    model=<span class="str">"llama3.2"</span>,
    system_prompt=<span class="str">"You are a helpful Python programming tutor."</span>
)

<span class="fn">print</span>(conv.send(<span class="str">"How do I create a class in Python?"</span>))
<span class="fn">print</span>(conv.send(<span class="str">"Can you show me an example?"</span>))
<span class="fn">print</span>(conv.send(<span class="str">"What about inheritance?"</span>))

<span class="cmt"># ëŒ€í™” ì €ì¥</span>
conv.save(<span class="str">"chat_history.json"</span>)

<span class="cmt"># ë‚˜ì¤‘ì— ë¶ˆëŸ¬ì˜¤ê¸°</span>
conv2 = Conversation.load(<span class="str">"chat_history.json"</span>)
<span class="fn">print</span>(conv2.send(<span class="str">"Can you explain polymorphism?"</span>))</code></pre>

  <h3 id="conversation-repl">ê°„ë‹¨í•œ REPL êµ¬í˜„</h3>

  <pre><code><span class="kw">def</span> <span class="fn">chat_repl</span>(model=<span class="str">"llama3.2"</span>):
    <span class="str">"""ê°„ë‹¨í•œ ëŒ€í™”í˜• ì¸í„°í˜ì´ìŠ¤"""</span>
    conv = Conversation(model)

    <span class="fn">print</span>(<span class="str">f"Chatting with <span class="num">{model}</span>. Type 'exit' to quit, 'clear' to reset."</span>)
    <span class="fn">print</span>(<span class="str">"="</span> * <span class="num">60</span>)

    <span class="kw">while</span> <span class="kw">True</span>:
        <span class="kw">try</span>:
            user_input = <span class="fn">input</span>(<span class="str">"\nğŸ‘¤ You: "</span>).<span class="fn">strip</span>()

            <span class="kw">if</span> <span class="kw">not</span> user_input:
                <span class="kw">continue</span>

            <span class="kw">if</span> user_input.lower() == <span class="str">"exit"</span>:
                <span class="fn">print</span>(<span class="str">"Goodbye!"</span>)
                <span class="kw">break</span>

            <span class="kw">if</span> user_input.lower() == <span class="str">"clear"</span>:
                conv.clear()
                <span class="fn">print</span>(<span class="str">"âœ“ Conversation cleared"</span>)
                <span class="kw">continue</span>

            <span class="fn">print</span>(<span class="str">"\nğŸ¤– Assistant: "</span>, end=<span class="str">""</span>)
            conv.send(user_input, stream=<span class="kw">True</span>)

        <span class="kw">except</span> <span class="type">KeyboardInterrupt</span>:
            <span class="fn">print</span>(<span class="str">"\n\nInterrupted. Goodbye!"</span>)
            <span class="kw">break</span>
        <span class="kw">except</span> <span class="type">Exception</span> <span class="kw">as</span> e:
            <span class="fn">print</span>(<span class="str">f"\nâŒ Error: <span class="num">{e}</span>"</span>)

<span class="kw">if</span> __name__ == <span class="str">"__main__"</span>:
    chat_repl()</code></pre>
</section>

<section class="content-section">
  <h2 id="advanced-cli">ê³ ê¸‰ CLI ê¸°ëŠ¥</h2>

  <h3 id="keep-alive">ëª¨ë¸ ì–¸ë¡œë“œ ì œì–´ (keep_alive)</h3>

  <pre><code><span class="cmt"># ê¸°ë³¸ì ìœ¼ë¡œ 5ë¶„ í›„ ìë™ ì–¸ë¡œë“œ</span>
<span class="cmt"># ìœ ì§€ ì‹œê°„ ë³€ê²½</span>
curl http://localhost:<span class="num">11434</span>/api/generate -d <span class="str">'{
  "model": "llama3.2",
  "prompt": "Hello",
  "keep_alive": "10m"
}'</span>

<span class="cmt"># ì˜êµ¬ ìœ ì§€ (ì„œë²„ ì¬ì‹œì‘ ì „ê¹Œì§€)</span>
curl http://localhost:<span class="num">11434</span>/api/generate -d <span class="str">'{
  "model": "llama3.2",
  "prompt": "Hello",
  "keep_alive": -1
}'</span>

<span class="cmt"># ì¦‰ì‹œ ì–¸ë¡œë“œ</span>
curl http://localhost:<span class="num">11434</span>/api/generate -d <span class="str">'{
  "model": "llama3.2",
  "prompt": "Hello",
  "keep_alive": 0
}'</span></code></pre>

  <h3 id="format">ì¶œë ¥ í˜•ì‹ ì§€ì • (format)</h3>

  <pre><code><span class="cmt"># JSON í˜•ì‹ ê°•ì œ</span>
curl http://localhost:<span class="num">11434</span>/api/generate -d <span class="str">'{
  "model": "llama3.2",
  "prompt": "List 3 programming languages with their year of creation",
  "format": "json"
}'</span>

<span class="cmt"># ì¶œë ¥ ì˜ˆì‹œ:</span>
{
  <span class="str">"languages"</span>: [
    {<span class="str">"name"</span>: <span class="str">"Python"</span>, <span class="str">"year"</span>: <span class="num">1991</span>},
    {<span class="str">"name"</span>: <span class="str">"JavaScript"</span>, <span class="str">"year"</span>: <span class="num">1995</span>},
    {<span class="str">"name"</span>: <span class="str">"Rust"</span>, <span class="str">"year"</span>: <span class="num">2010</span>}
  ]
}</code></pre>

  <h3 id="raw-mode">Raw ëª¨ë“œ</h3>

  <pre><code><span class="cmt"># í…œí”Œë¦¿ ì²˜ë¦¬ ì—†ì´ ì§ì ‘ ëª¨ë¸ì— ì „ë‹¬</span>
curl http://localhost:<span class="num">11434</span>/api/generate -d <span class="str">'{
  "model": "llama3.2",
  "prompt": "&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\n\nHello&lt;|eot_id|&gt;",
  "raw": true
}'</span></code></pre>

  <div class="info-box info">
    <div class="info-box-title">ğŸ’¡ ê³ ê¸‰ ê¸°ëŠ¥ í™œìš©</div>
    <ul>
      <li><code>keep_alive</code>: ë¹ˆë²ˆí•œ ìš”ì²­ ì‹œ ë©”ëª¨ë¦¬ì— ìœ ì§€í•˜ì—¬ ì†ë„ í–¥ìƒ</li>
      <li><code>format: "json"</code>: êµ¬ì¡°í™”ëœ ë°ì´í„° ì¶”ì¶œ ì‹œ ìœ ìš©</li>
      <li><code>raw: true</code>: íŠ¹ìˆ˜í•œ í”„ë¡¬í”„íŠ¸ í˜•ì‹ í…ŒìŠ¤íŠ¸ ì‹œ ì‚¬ìš©</li>
    </ul>
  </div>
</section>

<section class="content-section">
  <h2 id="best-practices">ëª¨ë²” ì‚¬ë¡€</h2>

  <h3 id="error-handling">ì—ëŸ¬ ì²˜ë¦¬</h3>

  <pre><code><span class="kw">import</span> requests
<span class="kw">from</span> requests.exceptions <span class="kw">import</span> RequestException, Timeout

<span class="kw">def</span> <span class="fn">safe_generate</span>(prompt, model=<span class="str">"llama3.2"</span>, max_retries=<span class="num">3</span>):
    <span class="kw">for</span> attempt <span class="kw">in</span> <span class="fn">range</span>(max_retries):
        <span class="kw">try</span>:
            response = requests.post(
                <span class="str">"http://localhost:11434/api/generate"</span>,
                json={<span class="str">"model"</span>: model, <span class="str">"prompt"</span>: prompt, <span class="str">"stream"</span>: <span class="kw">False</span>},
                timeout=<span class="num">60</span>  <span class="cmt"># 60ì´ˆ íƒ€ì„ì•„ì›ƒ</span>
            )
            response.raise_for_status()
            <span class="kw">return</span> response.json()[<span class="str">"response"</span>]

        <span class="kw">except</span> <span class="type">Timeout</span>:
            <span class="fn">print</span>(<span class="str">f"Timeout on attempt <span class="num">{attempt + 1}</span>"</span>)
            <span class="kw">if</span> attempt == max_retries - <span class="num">1</span>:
                <span class="kw">raise</span>

        <span class="kw">except</span> <span class="type">RequestException</span> <span class="kw">as</span> e:
            <span class="fn">print</span>(<span class="str">f"Error: <span class="num">{e}</span>"</span>)
            <span class="kw">if</span> attempt == max_retries - <span class="num">1</span>:
                <span class="kw">raise</span>

        <span class="kw">except</span> <span class="type">KeyError</span>:
            <span class="fn">print</span>(<span class="str">"Invalid response format"</span>)
            <span class="kw">raise</span></code></pre>

  <h3 id="performance">ì„±ëŠ¥ ìµœì í™”</h3>

  <ul>
    <li><strong>ë°°ì¹˜ ì²˜ë¦¬</strong>: ì—¬ëŸ¬ ìš”ì²­ì„ ë³‘ë ¬ë¡œ ì²˜ë¦¬</li>
    <li><strong>ìºì‹±</strong>: ë™ì¼ í”„ë¡¬í”„íŠ¸ ê²°ê³¼ ìºì‹œ</li>
    <li><strong>ì—°ê²° ì¬ì‚¬ìš©</strong>: <code>requests.Session()</code> ì‚¬ìš©</li>
    <li><strong>ì ì ˆí•œ íƒ€ì„ì•„ì›ƒ</strong>: ê¸´ ì‘ë‹µ ì˜ˆìƒ ì‹œ íƒ€ì„ì•„ì›ƒ ì¦ê°€</li>
  </ul>

  <pre><code><span class="kw">from</span> concurrent.futures <span class="kw">import</span> ThreadPoolExecutor
<span class="kw">import</span> requests

<span class="cmt"># ì—°ê²° ì¬ì‚¬ìš©</span>
session = requests.Session()

<span class="kw">def</span> <span class="fn">generate_parallel</span>(prompts, model=<span class="str">"llama3.2"</span>):
    <span class="kw">def</span> <span class="fn">_generate</span>(prompt):
        response = session.post(
            <span class="str">"http://localhost:11434/api/generate"</span>,
            json={<span class="str">"model"</span>: model, <span class="str">"prompt"</span>: prompt, <span class="str">"stream"</span>: <span class="kw">False</span>}
        )
        <span class="kw">return</span> response.json()[<span class="str">"response"</span>]

    <span class="kw">with</span> ThreadPoolExecutor(max_workers=<span class="num">4</span>) <span class="kw">as</span> executor:
        results = <span class="fn">list</span>(executor.map(_generate, prompts))

    <span class="kw">return</span> results

<span class="cmt"># ì‚¬ìš©</span>
prompts = [
    <span class="str">"Summarize quantum computing"</span>,
    <span class="str">"Explain blockchain"</span>,
    <span class="str">"What is AI?"</span>
]
results = generate_parallel(prompts)</code></pre>
</section>

<section class="content-section">
  <h2 id="summary">í•µì‹¬ ì •ë¦¬</h2>
  <ul>
    <li>Ollama ì‚¬ìš©ë²•ì˜ í•µì‹¬ ê°œë…ê³¼ íë¦„ì„ ì •ë¦¬í•©ë‹ˆë‹¤.</li>
    <li>CLI ê¸°ë³¸ ëª…ë ¹ì–´ë¥¼ ë‹¨ê³„ë³„ë¡œ ì´í•´í•©ë‹ˆë‹¤.</li>
    <li>ì‹¤ì „ ì ìš© ì‹œ ê¸°ì¤€ê³¼ ì£¼ì˜ì ì„ í™•ì¸í•©ë‹ˆë‹¤.</li>
  </ul>
</section>

<section class="content-section">
  <h2 id="practice-tips">ì‹¤ë¬´ íŒ</h2>
  <ul>
    <li>ì…ë ¥/ì¶œë ¥ ì˜ˆì‹œë¥¼ ê³ ì •í•´ ì¬í˜„ì„±ì„ í™•ë³´í•˜ì„¸ìš”.</li>
    <li>Ollama ì‚¬ìš©ë²• ë²”ìœ„ë¥¼ ì‘ê²Œ ì¡ê³  ë‹¨ê³„ì ìœ¼ë¡œ í™•ì¥í•˜ì„¸ìš”.</li>
    <li>CLI ê¸°ë³¸ ëª…ë ¹ì–´ ì¡°ê±´ì„ ë¬¸ì„œí™”í•´ ëŒ€ì‘ ì‹œê°„ì„ ì¤„ì´ì„¸ìš”.</li>
  </ul>
</section>
<nav class="page-nav"></nav>
</main>
<aside class="inline-toc">
  <div class="toc-title">ëª©ì°¨</div>
  <div class="toc-nav"></div>
</aside>
<footer class="site-footer"></footer>
</div>
<script src="../js/main.js"></script>
</body>
</html>