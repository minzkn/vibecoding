<!DOCTYPE html>
<html lang="ko" data-theme="dark-kernel">
<head>
<!-- BEGIN: Google adsense -->
<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-2110881342960271" crossorigin="anonymous"></script>
<!-- END: Google adsense -->
<!-- BEGIN: Google adsense repair -->
<script async src="https://fundingchoicesmessages.google.com/i/pub-2110881342960271?ers=1" nonce="laI6FT8gpRxugDJv5AGJRA"></script><script nonce="laI6FT8gpRxugDJv5AGJRA">(function() {function signalGooglefcPresent() {if (!window.frames['googlefcPresent']) {if (document.body) {const iframe = document.createElement('iframe'); iframe.style = 'width: 0; height: 0; border: none; z-index: -1000; left: -1000px; top: -1000px;'; iframe.style.display = 'none'; iframe.name = 'googlefcPresent'; document.body.appendChild(iframe);} else {setTimeout(signalGooglefcPresent, 0);}}}signalGooglefcPresent();})();</script>
<!-- END: Google adsense repair -->
<!-- BEGIN: Google analytics -->
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-1VWQF060SX"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-1VWQF060SX');
</script>
<!-- END: Google analytics -->

<!-- Flash 방지: 쿠키에서 테마 즉시 적용 -->
<script>
(function(){var m=document.cookie.match(/claude_theme=([^;]+)/);if(m)document.documentElement.setAttribute('data-theme',m[1]);})();
</script>
<meta charset="UTF-8">
<meta property="og:type" content="article">
<meta property="og:site_name" content="AI Vibe Coding 가이드 /with MINZKN">
<meta property="og:title" content="LLM 이론 종합: 확률·트랜스포머·스케일링">
<meta property="og:description" content="LLM 확률적 생성, 정보이론, Transformer, 스케일링 법칙, ICL 이론을 종합 정리합니다.">
<meta property="og:url" content="https://minzkn.com/claude/pages/llm-theory.html">
<meta property="og:image" content="https://minzkn.com/claude/images/og-image.png">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="LLM 확률적 생성, 정보이론, Transformer, 스케일링 법칙, ICL 이론을 종합 정리합니다.">
<meta name="keywords" content="llm 이론 확률 정보이론 transformer attention scaling icl 일반화 환각">
<meta name="author" content="MINZKN">
<title>LLM 이론 종합: 확률·트랜스포머·스케일링 - AI Vibe Coding 가이드 /with MINZKN</title>
<link rel="icon" type="image/svg+xml" href="../images/favicon.svg">
<link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/joungkyun/font-d2coding/d2coding.css">
<link rel="stylesheet" href="../css/themes.css">
<link rel="stylesheet" href="../css/style.css">
<link rel="stylesheet" href="../css/responsive.css">
</head>
<body>
<div class="page-wrapper">

<!-- ===== Header ===== -->
<header class="site-header">
</header>

<!-- ===== Side Navigation ===== -->
<nav class="side-nav" aria-label="사이트 내비게이션">
</nav>

<!-- ===== Main Content ===== -->
<main class="main-content">
<div class="breadcrumb"></div>

<h1 id="top">LLM 이론 종합: 확률·트랜스포머·스케일링</h1>
<p class="page-description">LLM의 확률적 생성, 정보이론 지표, Transformer 아키텍처, 스케일링 법칙, In-Context Learning까지 핵심 이론을 하나로 정리합니다.</p>

<!-- ============================================================ -->
<!--  기초 파트                                                     -->
<!-- ============================================================ -->

<section class="content-section">
  <h2 id="overview">개요</h2>
  <p>LLM은 "다음 토큰의 확률"을 예측하는 모델입니다. 이 단순한 가정이 정보이론, 일반화, 환각 같은 현상으로 연결되고, Transformer 아키텍처와 스케일링 법칙이 실질적인 성능을 결정합니다. 이 페이지는 기초부터 심화까지 핵심 이론을 실무 관점에서 풀어 설명합니다.</p>
  <div class="info-box info">
    <strong>구성:</strong> 전반부(확률·정보이론·일반화·환각)는 기초, 후반부(Transformer·스케일링·ICL·추론)는 심화입니다. 순서대로 읽으면 자연스럽게 이해가 깊어집니다.
  </div>
</section>

<section class="content-section">
  <h2 id="probability">확률적 생성의 의미</h2>
  <p>모델은 정답을 "하나"로 내지 않고, 가능한 답변의 분포를 학습합니다. 따라서 응답은 항상 확률적으로 결정됩니다.</p>
  <ul>
    <li><strong>온도(temperature)</strong>: 분포를 넓히거나 좁힘 — 높을수록 다양한 토큰이 선택될 확률 증가</li>
    <li><strong>Top-k/Top-p</strong>: 후보 토큰의 범위를 제한하여 불필요한 노이즈 차단</li>
    <li><strong>샘플링 vs 결정론</strong>: 창의성(높은 온도)과 일관성(낮은 온도)의 균형</li>
  </ul>

  <h3 id="temperature-math">온도 파라미터의 수학적 효과</h3>
  <p>온도(T)는 softmax 함수의 분포를 조절합니다. 각 토큰 i의 선택 확률은 다음과 같이 계산됩니다:</p>
  <pre><code><span class="cmt"># 온도가 적용된 Softmax</span>
P(token_i) = <span class="fn">exp</span>(logit_i / T) / ∑ <span class="fn">exp</span>(logit_j / T)

<span class="cmt"># T → 0 : 가장 높은 logit의 토큰만 선택 (결정론적)</span>
<span class="cmt"># T = 1 : 원래 분포 그대로</span>
<span class="cmt"># T → ∞ : 균등 분포 (완전 랜덤)</span></code></pre>

  <p>예를 들어 3개 토큰의 logit이 [2.0, 1.0, 0.5]일 때, 온도에 따른 확률 변화는 다음과 같습니다:</p>
  <pre><code><span class="cmt"># logits = [2.0, 1.0, 0.5]</span>

<span class="cmt"># T=0.1 (매우 낮음) → 거의 결정론적</span>
P = [<span class="num">1.000</span>, <span class="num">0.000</span>, <span class="num">0.000</span>]  <span class="cmt"># 최고 확률 토큰만 선택</span>

<span class="cmt"># T=0.7 (낮음) → 안정적이되 약간의 변동</span>
P = [<span class="num">0.757</span>, <span class="num">0.166</span>, <span class="num">0.077</span>]  <span class="cmt"># 1등 토큰에 집중</span>

<span class="cmt"># T=1.0 (기본) → 원래 분포</span>
P = [<span class="num">0.594</span>, <span class="num">0.219</span>, <span class="num">0.133</span>]  <span class="cmt"># 자연스러운 분포</span>

<span class="cmt"># T=1.5 (높음) → 평탄한 분포</span>
P = [<span class="num">0.463</span>, <span class="num">0.281</span>, <span class="num">0.218</span>]  <span class="cmt"># 2·3등도 자주 선택됨</span></code></pre>

  <h3 id="top-p-sampling">Top-p (Nucleus) 샘플링</h3>
  <p>Top-p 샘플링은 확률을 내림차순으로 정렬한 후, 누적 확률이 p에 도달할 때까지의 토큰만 후보로 남깁니다. 이렇게 하면 확률이 극히 낮은 "노이즈" 토큰을 자연스럽게 제외합니다.</p>
  <pre><code><span class="cmt"># Top-p=0.9 예시</span>
<span class="cmt"># 전체 토큰 확률 분포 (내림차순 정렬):</span>
"the"    : <span class="num">0.35</span>  → 누적 <span class="num">0.35</span> ✓
"a"      : <span class="num">0.25</span>  → 누적 <span class="num">0.60</span> ✓
"this"   : <span class="num">0.15</span>  → 누적 <span class="num">0.75</span> ✓
"that"   : <span class="num">0.10</span>  → 누적 <span class="num">0.85</span> ✓
"an"     : <span class="num">0.08</span>  → 누적 <span class="num">0.93</span> ✓ <span class="cmt">← p=0.9 도달, 여기까지 포함</span>
"my"     : <span class="num">0.04</span>  → 제외
"some"   : <span class="num">0.02</span>  → 제외
"..."    : <span class="num">0.01</span>  → 제외

<span class="cmt"># 결과: 상위 5개 토큰 중에서만 재샘플링</span>
<span class="cmt"># Top-k와 달리 동적으로 후보 수가 조절되는 것이 장점</span></code></pre>

  <h3 id="param-guide">상황별 추천 파라미터</h3>
  <table>
    <thead>
      <tr>
        <th>사용 사례</th>
        <th>Temperature</th>
        <th>Top-p</th>
        <th>이유</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>코드 생성</td>
        <td>0.1~0.3</td>
        <td>0.9</td>
        <td>문법 정확성이 최우선, 변동 최소화</td>
      </tr>
      <tr>
        <td>사실 기반 Q&amp;A</td>
        <td>0.2~0.4</td>
        <td>0.9</td>
        <td>정확한 정보 전달, 일관성 중시</td>
      </tr>
      <tr>
        <td>비즈니스 문서 작성</td>
        <td>0.5~0.7</td>
        <td>0.95</td>
        <td>자연스러움과 정확성의 균형</td>
      </tr>
      <tr>
        <td>창작 글쓰기</td>
        <td>0.8~1.0</td>
        <td>0.95</td>
        <td>다양한 표현과 창의적 전개</td>
      </tr>
      <tr>
        <td>브레인스토밍</td>
        <td>1.0~1.5</td>
        <td>1.0</td>
        <td>의외성과 참신한 아이디어 탐색</td>
      </tr>
    </tbody>
  </table>

  <div class="diagram-container">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 700 240"
         style="width:100%;max-width:700px;display:block;margin:1.5em auto;background:var(--diagram-fill);border-radius:8px;padding:20px;">
      <defs>
        <marker id="llm-theory-1-arrow" viewBox="0 0 10 7" refX="10" refY="3.5"
                markerWidth="10" markerHeight="7" orient="auto">
          <polygon points="0 0, 10 3.5, 0 7" fill="var(--diagram-arrow)"/>
        </marker>
      </defs>

      <rect x="40" y="50" width="170" height="60" rx="6"
            fill="var(--bg-secondary)" stroke="var(--border-color)" stroke-width="1.5"/>
      <text x="125" y="85" text-anchor="middle" fill="var(--diagram-text)" font-size="13">입력 토큰</text>

      <rect x="270" y="50" width="170" height="60" rx="6"
            fill="var(--bg-secondary)" stroke="var(--border-color)" stroke-width="1.5"/>
      <text x="355" y="85" text-anchor="middle" fill="var(--diagram-text)" font-size="13">확률 분포</text>

      <rect x="500" y="50" width="160" height="60" rx="6"
            fill="var(--bg-secondary)" stroke="var(--border-color)" stroke-width="1.5"/>
      <text x="580" y="85" text-anchor="middle" fill="var(--diagram-text)" font-size="13">출력 토큰</text>

      <line x1="210" y1="80" x2="270" y2="80" stroke="var(--diagram-arrow)" stroke-width="2"
            marker-end="url(#llm-theory-1-arrow)"/>
      <line x1="440" y1="80" x2="500" y2="80" stroke="var(--diagram-arrow)" stroke-width="2"
            marker-end="url(#llm-theory-1-arrow)"/>

      <text x="240" y="135" text-anchor="middle" fill="var(--diagram-text)" font-size="11" opacity="0.7">LLM 모델</text>
      <text x="470" y="135" text-anchor="middle" fill="var(--diagram-text)" font-size="11" opacity="0.7">온도/Top-p</text>
    </svg>
    <p class="diagram-caption">입력 → 확률 분포 → 출력의 기본 생성 흐름</p>
  </div>
</section>

<section class="content-section">
  <h2 id="math-basics">수학 기초: 선형대수</h2>
  <p>LLM의 내부 동작을 이해하려면 최소한의 선형대수와 미적분 직관이 필요합니다. 이 섹션에서는 AI/ML에서 가장 빈번하게 사용되는 수학적 개념들을 다룹니다.</p>

  <h3 id="vector-basics">벡터: 단어를 숫자로</h3>
  <p><strong>벡터</strong>는 숫자를 일렬로並べ한 것입니다. LLM에서는 모든 단어나 토큰을 벡터로 표현합니다. 이를 <strong>임베딩(embedding)</strong>이라고 합니다.</p>
  <pre><code><span class="cmt"># 토큰 임베딩 예시 (단순화한 4차원)</span>
<span class="str">"고양이"</span>  →  [<span class="num">0.2</span>, <span class="num">-0.5</span>, <span class="num">1.3</span>, <span class="num">0.8</span>]
<span class="str">"강아지"</span>  →  [<span class="num">0.3</span>, <span class="num">-0.4</span>, <span class="num">1.1</span>, <span class="num">0.7</span>]
<span class="str">"자동차"</span>  →  [<span class="num">-0.9</span>, <span class="num">0.7</span>, <span class="num">-0.2</span>, <span class="num">1.5</span>]

<span class="cmt"># "고양이"와 "강아지"는 숫자가 비슷 → 의미도 유사</span>
<span class="cmt"># "고양이"와 "자동차"는 숫자가 다름 → 의미도 다름</span></code></pre>
  <p>실제 LLM에서는 차원이 훨씬 높습니다(예: 4096차원). 높은 차원 공간에서 유사한 의미를 가진 단어들이 서로 가까이 위치하게 됩니다.</p>

  <h3 id="dot-product">내적: 유사도를 측정하는 핵심 연산</h3>
  <p><strong>내적(dot product)</strong>은 두 벡터의 유사도를 측정하는 기본 연산입니다.Attention 메커니즘의 핵심이 바로 이 내적 연산입니다.</p>
  <pre><code><span class="cmt"># 내적 정의</span>
v · w = ∑ v_i × w_i = v₁w₁ + v₂w₂ + ... + vₙwₙ

<span class="cmt"># 구체적인 계산 예시</span>
[<span class="num">1</span>, <span class="num">2</span>, <span class="num">3</span>] · [<span class="num">4</span>, <span class="num">5</span>, <span class="num">6</span>] = <span class="num">1×4</span> + <span class="num">2×5</span> + <span class="num">3×6</span> = <span class="num">4</span> + <span class="num">10</span> + <span class="num">18</span> = <span class="num">32</span>

<span class="cmt"># 기하학적 해석:</span>
<span class="cmt"># - 두 벡터가 같은 방향 → 내적 > 0 (양수)</span>
<span class="cmt"># - 두 벡터가 수직(90°) → 내적 = 0</span>
<span class="cmt"># - 두 벡터가 반대 방향 → 내적 < 0 (음수)</span></code></pre>

  <h3 id="matrix-basics">행렬: 벡터의 모음</h3>
  <p><strong>행렬</strong>은 숫자를 2차원 형태로並べ한 것입니다. LLM에서 행렬은 여러 가지 의미를 가집니다:</p>
  <ul>
    <li><strong>가중치 행렬</strong>: 행과 열이 있는 학습 가능한 파라미터</li>
    <li><strong>임베딩 행렬</strong>: 모든 토큰의 임베딩을 행으로 저장</li>
    <li><strong>어텐션 스코어 행렬</strong>: 토큰 쌍 간 유사도를 저장</li>
  </ul>
  <pre><code><span class="cmt"># 행렬 예시: 3개의 토큰 임베딩 (각 4차원)</span>
<span class="cmt"># 각 행 = 하나의 토큰</span>
Embeddings = [
  [<span class="num">0.2</span>, <span class="num">-0.5</span>, <span class="num">1.3</span>, <span class="num">0.8</span>],   <span class="cmt"># 토큰 1: "The"</span>
  [<span class="num">0.1</span>, <span class="num">-0.3</span>, <span class="num">0.9</span>, <span class="num">0.5</span>],   <span class="cmt"># 토큰 2: "cat"</span>
  [<span class="num">-0.1</span>, <span class="num">0.2</span>, <span class="num">0.7</span>, <span class="num">1.2</span>]    <span class="cmt"># 토큰 3: "sat"</span>
]

<span class="cmt"># 행렬의 크기: 3행 × 4열 = 3×4 행렬</span></code></pre>

  <h3 id="matrix-multiply">행렬 곱셈과 Attention의 연결</h3>
  <p>Attention에서 Q(Query)와 K(Key)의 유사도를 한꺼번에 구할 때 행렬 곱셈을 사용합니다. 내적을 모든 토큰 쌍에 대해 동시에 계산하는 것입니다.</p>
  <pre><code><span class="cmt"># 행렬 곱셈: Q(2×3) × K^T(3×2) = Score(2×2)</span>
<span class="cmt"># Q: 2개 토큰의 Query (각 3차원)</span>
Q = [[<span class="num">1</span>, <span class="num">0</span>, <span class="num">2</span>],
     [<span class="num">0</span>, <span class="num">1</span>, <span class="num">1</span>]]

<span class="cmt"># K: 2개 토큰의 Key (각 3차원) → 전치하여 3×2</span>
K = [[<span class="num">1</span>, <span class="num">2</span>, <span class="num">0</span>],
     [<span class="num">0</span>, <span class="num">1</span>, <span class="num">3</span>]]
K^T = [[<span class="num">1</span>, <span class="num">0</span>],
        [<span class="num">2</span>, <span class="num">1</span>],
        [<span class="num">0</span>, <span class="num">3</span>]]

<span class="cmt"># 행렬 곱셈 수행</span>
QK^T = [[<span class="num">1×1+0×2+2×0</span>, <span class="num">1×0+0×1+2×3</span>],
        [<span class="num">0×1+1×2+1×0</span>, <span class="num">0×0+1×1+1×3</span>]]
     = [[<span class="num">1</span>, <span class="num">6</span>],
        [<span class="num">2</span>, <span class="num">4</span>]]

<span class="cmt"># 결과 해석:</span>
<span class="cmt"># QK^T[0][1] = 6 → 첫 번째 Query와 두 번째 Key가 매우 유사</span>
<span class="cmt"># QK^T[1][0] = 2 → 두 번째 Query와 첫 번째 Key의 유사도</span></code></pre>

  <div class="diagram-container">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 700 300"
         style="width:100%;max-width:700px;display:block;margin:1.5em auto;background:var(--diagram-fill);border-radius:8px;padding:20px;">
      <defs>
        <marker id="math-1-arrow" viewBox="0 0 10 7" refX="10" refY="3.5"
                markerWidth="10" markerHeight="7" orient="auto">
          <polygon points="0 0, 10 3.5, 0 7" fill="var(--diagram-arrow)"/>
        </marker>
      </defs>

      <rect x="30" y="40" width="120" height="80" rx="6"
            fill="var(--bg-secondary)" stroke="var(--border-color)" stroke-width="1.5"/>
      <text x="90" y="75" text-anchor="middle" fill="var(--diagram-text)" font-size="12">Query</text>
      <text x="90" y="95" text-anchor="middle" fill="var(--diagram-text)" font-size="11" opacity="0.7">토큰 A</text>
      <text x="90" y="110" text-anchor="middle" fill="var(--text-secondary)" font-size="10">[1, 0, 2]</text>

      <rect x="30" y="170" width="120" height="80" rx="6"
            fill="var(--bg-secondary)" stroke="var(--border-color)" stroke-width="1.5"/>
      <text x="90" y="205" text-anchor="middle" fill="var(--diagram-text)" font-size="12">Query</text>
      <text x="90" y="225" text-anchor="middle" fill="var(--diagram-text)" font-size="11" opacity="0.7">토큰 B</text>
      <text x="90" y="240" text-anchor="middle" fill="var(--text-secondary)" font-size="10">[0, 1, 1]</text>

      <rect x="250" y="40" width="120" height="80" rx="6"
            fill="var(--bg-secondary)" stroke="var(--border-color)" stroke-width="1.5"/>
      <text x="310" y="75" text-anchor="middle" fill="var(--diagram-text)" font-size="12">Key</text>
      <text x="310" y="95" text-anchor="middle" fill="var(--diagram-text)" font-size="11" opacity="0.7">토큰 A</text>
      <text x="310" y="110" text-anchor="middle" fill="var(--text-secondary)" font-size="10">[1, 2, 0]</text>

      <rect x="250" y="170" width="120" height="80" rx="6"
            fill="var(--bg-secondary)" stroke="var(--border-color)" stroke-width="1.5"/>
      <text x="310" y="205" text-anchor="middle" fill="var(--diagram-text)" font-size="12">Key</text>
      <text x="310" y="225" text-anchor="middle" fill="var(--diagram-text)" font-size="11" opacity="0.7">토큰 B</text>
      <text x="310" y="240" text-anchor="middle" fill="var(--text-secondary)" font-size="10">[0, 1, 3]</text>

      <line x1="150" y1="80" x2="250" y2="80" stroke="var(--diagram-arrow)" stroke-width="2"
            marker-end="url(#math-1-arrow)"/>
      <line x1="150" y1="210" x2="250" y2="210" stroke="var(--diagram-arrow)" stroke-width="2"
            marker-end="url(#math-1-arrow)"/>

      <rect x="450" y="100" width="220" height="100" rx="6"
            fill="var(--bg-secondary)" stroke="var(--accent-primary)" stroke-width="2"/>
      <text x="560" y="135" text-anchor="middle" fill="var(--diagram-text)" font-size="13" font-weight="bold">어텐션 스코어 행렬</text>
      <text x="560" y="160" text-anchor="middle" fill="var(--diagram-text)" font-size="12">Q × K^T</text>
      <text x="560" y="185" text-anchor="middle" fill="var(--text-secondary)" font-size="11">[[ 1,  6],</text>
      <text x="560" y="200" text-anchor="middle" fill="var(--text-secondary)" font-size="11">[ 2,  4]]</text>
    </svg>
    <p class="diagram-caption">행렬 곱셈으로 토큰 간 유사도(어텐션 스코어) 계산</p>
  </div>

  <h3 id="matrix-transform">행렬의 기하학적 해석: 공간 변환</h3>
  <p>행렬은 벡터 공간을 변환하는 도구로 이해할 수 있습니다. LLM에서 가중치 행렬은 입력 벡터를 새로운 공간으로 변환하여 "의미 있는 특성"을 추출합니다.</p>
  <pre><code><span class="cmt"># 2D 회전 행렬 (반시계 방향 45°)</span>
<span class="fn">Rotation</span>(45°) = [
  [ cos(45°), -sin(45°) ],
  [ sin(45°),  cos(45°) ]
] ≈ [
  [ <span class="num">0.707</span>, <span class="num">-0.707</span> ],
  [ <span class="num">0.707</span>,  <span class="num">0.707</span> ]
]

<span class="cmt"># 벡터 [1, 0] (x축 단위 벡터)에 적용</span>
[<span class="num">0.707</span>, <span class="num">-0.707</span>] × [<span class="num">1</span>] + [<span class="num">0.707</span>, <span class="num">0.707</span>] × [<span class="num">0</span>]
= [<span class="num">0.707</span>, <span class="num">0.707</span>]

<span class="cmt"># 결과: 45° 방향으로 회전됨</span>

<span class="cmt"># 신호망에서의 변환 의미:</span>
<span class="cmt"># - 회전: 특성 간 관계 재배열</span>
<span class="cmt"># - 스케일링: 특성 강조/약화</span>
<span class="cmt"># - 전치: 특성 축 변환</span></code></pre>

  <div class="diagram-container">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 700 320"
         style="width:100%;max-width:700px;display:block;margin:1.5em auto;background:var(--diagram-fill);border-radius:8px;padding:20px;">
      <defs>
        <marker id="math-7-arrow" viewBox="0 0 10 7" refX="10" refY="3.5"
                markerWidth="10" markerHeight="7" orient="auto">
          <polygon points="0 0, 10 3.5, 0 7" fill="var(--diagram-arrow)"/>
        </marker>
      </defs>

      <text x="350" y="30" text-anchor="middle" fill="var(--diagram-text)" font-size="13" font-weight="bold">행렬 곱셈의 기하학적 의미: 공간 변환</text>

      <line x1="80" y1="200" x2="280" y2="200" stroke="var(--border-color)" stroke-width="1"/>
      <line x1="180" y1="100" x2="180" y2="250" stroke="var(--border-color)" stroke-width="1"/>

      <line x1="180" y1="200" x2="250" y2="150" stroke="var(--accent-primary)" stroke-width="3"
            marker-end="url(#math-7-arrow)"/>
      <text x="265" y="145" text-anchor="middle" fill="var(--accent-primary)" font-size="11">v₁ = [1,0]</text>

      <line x1="180" y1="200" x2="220" y2="220" stroke="var(--diagram-accent)" stroke-width="3"
            marker-end="url(#math-7-arrow)"/>
      <text x="235" y="235" text-anchor="middle" fill="var(--diagram-accent)" font-size="11">v₂ = [0,1]</text>

      <rect x="340" y="160" width="80" height="40" rx="4"
            fill="var(--bg-secondary)" stroke="var(--accent-primary)" stroke-width="1.5"/>
      <text x="380" y="185" text-anchor="middle" fill="var(--diagram-text)" font-size="11">W</text>

      <line x1="420" y1="180" x2="480" y2="180" stroke="var(--diagram-arrow)" stroke-width="2"
            marker-end="url(#math-7-arrow)"/>

      <line x1="520" y1="200" x2="620" y2="200" stroke="var(--border-color)" stroke-width="1"/>
      <line x1="570" y1="120" x2="570" y2="250" stroke="var(--border-color)" stroke-width="1"/>

      <line x1="570" y1="200" x2="620" y2="150" stroke="var(--accent-primary)" stroke-width="3"
            marker-end="url(#math-7-arrow)"/>
      <text x="635" y="145" text-anchor="middle" fill="var(--accent-primary)" font-size="11">W·v₁</text>

      <line x1="570" y1="200" x2="620" y2="220" stroke="var(--diagram-accent)" stroke-width="3"
            marker-end="url(#math-7-arrow)"/>
      <text x="635" y="235" text-anchor="middle" fill="var(--diagram-accent)" font-size="11">W·v₂</text>

      <text x="350" y="280" text-anchor="middle" fill="var(--text-secondary)" font-size="10">
        입력 벡터 공간 (왼쪽) → 변환된 특성 공간 (오른쪽)
      </text>
      <text x="350" y="295" text-anchor="middle" fill="var(--text-secondary)" font-size="10">
        W(가중치 행렬)가 벡터의 "의미"를 재해석합니다
      </text>
    </svg>
    <p class="diagram-caption">행렬 곱셈은 벡터 공간의 변환을 의미 (회전, 스케일링, 전단 등)</p>
  </div>

  <h3 id="cosine-similarity">코사인 유사도</h3>
  <p>임베딩 벡터 간의 의미적 유사도를 측정할 때 <strong>코사인 유사도</strong>를 사용합니다. 벡터의 크기에 무관하게 방향만으로 비교하므로, 단어·문장 임베딩 비교의 표준 지표입니다.</p>
  <pre><code><span class="cmt"># 코사인 유사도 공식</span>
cos(θ) = (A · B) / (|A| × |B|)

<span class="cmt"># |A|: 벡터 A의 크기 (노름)</span>
<span class="cmt"># |A| = √(A₁² + A₂² + ... + Aₙ²)</span>

<span class="cmt"># 결과 범위:</span>
<span class="cmt"># - 1.0: 같은 방향 (완전히 유사)</span>
<span class="cmt"># - 0.0: 수직 (무관)</span>
<span class="cmt"># - -1.0: 반대 방향 (완전히 다름)</span>

<span class="cmt"># 실제 예시:</span>
<span class="cmt"># "고양이"와 "강아지" → cos ≈ 0.85 (높은 유사도)</span>
<span class="cmt"># "고양이"와 "자동차" → cos ≈ 0.12 (낮은 유사도)</span>
<span class="cmt"># "좋다"와 "나쁘다" → cos ≈ -0.3 (반대 의미)</span></code></pre>

  <div class="diagram-container">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 700 220"
         style="width:100%;max-width:700px;display:block;margin:1.5em auto;background:var(--diagram-fill);border-radius:8px;padding:20px;">
      <defs>
        <marker id="math-2-arrow" viewBox="0 0 10 7" refX="10" refY="3.5"
                markerWidth="10" markerHeight="7" orient="auto">
          <polygon points="0 0, 10 3.5, 0 7" fill="var(--diagram-arrow)"/>
        </marker>
      </defs>

      <line x1="100" y1="150" x2="500" y2="150" stroke="var(--border-color)" stroke-width="1"/>

      <line x1="150" y1="150" x2="250" y2="50" stroke="var(--diagram-arrow)" stroke-width="3"
            marker-end="url(#math-2-arrow)"/>
      <text x="200" y="100" text-anchor="middle" fill="var(--diagram-text)" font-size="12">A:고양이</text>
      <text x="200" y="120" text-anchor="middle" fill="var(--text-secondary)" font-size="10">cos=0.85</text>

      <line x1="200" y1="150" x2="320" y2="80" stroke="var(--diagram-accent)" stroke-width="3"
            marker-end="url(#math-2-arrow)"/>
      <text x="260" y="115" text-anchor="middle" fill="var(--diagram-text)" font-size="12">B:강아지</text>
      <text x="260" y="135" text-anchor="middle" fill="var(--text-secondary)" font-size="10">높은 유사도</text>

      <line x1="350" y1="150" x2="450" y2="120" stroke="var(--diagram-arrow)" stroke-width="3"
            marker-end="url(#math-2-arrow)" stroke-dasharray="5,5"/>
      <text x="400" y="140" text-anchor="middle" fill="var(--diagram-text)" font-size="12">C:자동차</text>
      <text x="400" y="160" text-anchor="middle" fill="var(--text-secondary)" font-size="10">낮은 유사도</text>

      <circle cx="550" cy="110" r="40" fill="var(--bg-secondary)" stroke="var(--border-color)" stroke-width="1.5"/>
      <text x="550" y="105" text-anchor="middle" fill="var(--diagram-text)" font-size="11">cos > 0.7</text>
      <text x="550" y="125" text-anchor="middle" fill="var(--accent-primary)" font-size="10">유사함</text>

      <circle cx="550" cy="110" r="40" fill="none" stroke="var(--border-color)" stroke-width="1.5" opacity="0.5" transform="translate(120, 20)"/>
      <text x="670" y="105" text-anchor="middle" fill="var(--diagram-text)" font-size="11">cos < 0.3</text>
      <text x="670" y="125" text-anchor="middle" fill="var(--text-secondary)" font-size="10">무관함</text>
    </svg>
    <p class="diagram-caption">코사인 유사도로 의미적 유사도 측정 (높은 cos = 비슷한 방향)</p>
  </div>

  <h3 id="activation-functions">활성화 함수: 비선형성의 핵심</h3>
  <p>활성화 함수는 신경망에 <strong>비선형성(non-linearity)</strong>을 도입합니다. 선형 변환만으로는 복잡한 패턴을 학습할 수 없고, 활성화 함수를 통해 복잡한 결정 경계를 만들 수 있습니다.</p>

  <h4 id="relu">ReLU (Rectified Linear Unit)</h4>
  <p>현대 딥러닝에서 가장 널리 사용되는 활성화 함수입니다. 간단하면서도 효과적이며, gradient vanishing 문제를 완화합니다.</p>
  <pre><code><span class="cmt"># ReLU 함수 정의</span>
<span class="fn">ReLU</span>(x) = <span class="fn">max</span>(0, x)

<span class="cmt"># 미분 (Gradient)</span>
d/dx <span class="fn">ReLU</span>(x) = 
  <span class="num">1</span>  <span class="cmt">if x > 0</span>
  <span class="num">0</span>  <span class="cmt">if x ≤ 0</span>

<span class="cmt"># 계산 예시</span>
<span class="fn">ReLU</span>(<span class="num">3.5</span>) = <span class="num">3.5</span>    <span class="cmt"># 양수 → 그대로 통과</span>
<span class="fn">ReLU</span>(<span class="num">-1.2</span>) = <span class="num">0</span>      <span class="cmt"># 음수 → 0</span>
<span class="fn">ReLU</span>(<span class="num">0</span>)   = <span class="num">0</span>      <span class="cmt"># 영 → 0</span></code></pre>

  <h4 id="sigmoid">Sigmoid: 확률로의 변환</h4>
  <p>Sigmoid는 임의의 실수를 0~1 사이의 값으로 압축합니다. 이特性으로 인해 확률 해석이 필요할 때 유용합니다.</p>
  <pre><code><span class="cmt"># Sigmoid 함수</span>
σ(x) = <span class="num">1</span> / (<span class="num">1</span> + <span class="fn">exp</span>(-x))

<span class="cmt"># 미분</span>
d/dx σ(x) = σ(x) × (<span class="num">1</span> - σ(x))

<span class="cmt"># 계산 예시</span>
σ(<span class="num">0</span>)   = <span class="num">0.5</span>         <span class="cmt"># 중앙에서 0.5</span>
σ(<span class="num">2</span>)   ≈ <span class="num">0.88</span>        <span class="cmt"># 양수 → 1에 가까움</span>
σ(<span class="num">-2</span>)  ≈ <span class="num">0.12</span>        <span class="cmt"># 음수 → 0에 가까움</span>

<span class="cmt"># 특성:</span>
<span class="cmt"># - 출력 ∈ (0, 1) → 확률로 해석 가능</span>
<span class="cmt"># - 미분값이 0~0.25 사이 (최대 0.25)</span>
<span class="cmt"># - 깊은 네트워크에서 gradient vanishing 발생</span></code></pre>

  <h4 id="tanh">Tanh: -1에서 1 사이</h4>
  <p>Tanh는 Sigmoid와 비슷하지만 출력 범위가 -1에서 1 사이입니다. 평균이 0에 가까워 학습이 더 효율적입니다.</p>
  <pre><code><span class="cmt"># Tanh 함수</span>
<span class="fn">tanh</span>(x) = (<span class="fn">exp</span>(x) - <span class="fn">exp</span>(-x)) / (<span class="fn">exp</span>(x) + <span class="fn">exp</span>(-x))

<span class="cmt"># 미분</span>
d/dx <span class="fn">tanh</span>(x) = <span class="num">1</span> - <span class="fn">tanh</span>(x)²

<span class="cmt"># 계산 예시</span>
<span class="fn">tanh</span>(<span class="num">0</span>)   = <span class="num">0</span>
<span class="fn">tanh</span>(<span class="num">1</span>)   ≈ <span class="num">0.76</span>
<span class="fn">tanh</span>(<span class="num">-1</span>)  ≈ <span class="num">-0.76</span>

<span class="cmt"># Sigmoid vs Tanh 비교:</span>
<span class="cmt"># Sigmoid: 출력 (0, 1) - 양수만 출력</span>
<span class="cmt"># Tanh: 출력 (-1, 1) - 양수/음수 모두 출력 (평균 0)</span></code></pre>

  <div class="diagram-container">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 700 300"
         style="width:100%;max-width:700px;display:block;margin:1.5em auto;background:var(--diagram-fill);border-radius:8px;padding:20px;">
      <defs>
        <marker id="math-8-arrow" viewBox="0 0 10 7" refX="10" refY="3.5"
                markerWidth="10" markerHeight="7" orient="auto">
          <polygon points="0 0, 10 3.5, 0 7" fill="var(--diagram-arrow)"/>
        </marker>
      </defs>

      <text x="230" y="25" text-anchor="middle" fill="var(--diagram-text)" font-size="13" font-weight="bold">활성화 함수 비교</text>
      <text x="230" y="45" text-anchor="middle" fill="var(--text-secondary)" font-size="10">x축: 입력값, y축: 출력값</text>

      <line x1="30" y1="150" x2="430" y2="150" stroke="var(--border-color)" stroke-width="1"/>
      <line x1="230" y1="50" x2="230" y2="250" stroke="var(--border-color)" stroke-width="1"/>

      <path d="M 30,150 L 230,150 L 430,50" fill="none" stroke="var(--accent-primary)" stroke-width="2.5"/>
      <text x="330" y="90" text-anchor="middle" fill="var(--accent-primary)" font-size="11">ReLU</text>

      <path d="M 30,230 Q 150,230 230,150 Q 310,70 430,70" fill="none" stroke="var(--diagram-arrow)" stroke-width="2.5"/>
      <text x="380" y="60" text-anchor="middle" fill="var(--diagram-arrow)" font-size="11">Sigmoid</text>

      <path d="M 30,70 Q 130,70 230,150 Q 330,230 430,230" fill="none" stroke="var(--diagram-accent)" stroke-width="2.5"/>
      <text x="350" y="260" text-anchor="middle" fill="var(--diagram-accent)" font-size="11">Tanh</text>

      <text x="230" y="270" text-anchor="middle" fill="var(--text-secondary)" font-size="10">0</text>
      <text x="30" y="160" text-anchor="middle" fill="var(--text-secondary)" font-size="10">-3</text>
      <text x="430" y="160" text-anchor="middle" fill="var(--text-secondary)" font-size="10">+3</text>

      <rect x="470" y="60" width="200" height="180" rx="6"
            fill="var(--bg-secondary)" stroke="var(--border-color)" stroke-width="1.5"/>
      <text x="570" y="85" text-anchor="middle" fill="var(--diagram-text)" font-size="12" font-weight="bold">특성 비교</text>

      <text x="480" y="110" text-anchor="start" fill="var(--accent-primary)" font-size="10">• ReLU: max(0,x)</text>
      <text x="480" y="125" text-anchor="start" fill="var(--text-secondary)" font-size="9">  출력: [0, ∞)</text>
      <text x="480" y="138" text-anchor="start" fill="var(--text-secondary)" font-size="9">  미분: 0 또는 1</text>

      <text x="480" y="160" text-anchor="start" fill="var(--diagram-arrow)" font-size="10">• Sigmoid: 1/(1+e^-x)</text>
      <text x="480" y="175" text-anchor="start" fill="var(--text-secondary)" font-size="9">  출력: (0, 1)</text>
      <text x="480" y="188" text-anchor="start" fill="var(--text-secondary)" font-size="9">  미분: 0~0.25</text>

      <text x="480" y="210" text-anchor="start" fill="var(--diagram-accent)" font-size="10">• Tanh: (e^x-e^-x)/(e^x+e^-x)</text>
      <text x="480" y="225" text-anchor="start" fill="var(--text-secondary)" font-size="9">  출력: (-1, 1)</text>
      <text x="480" y="238" text-anchor="start" fill="var(--text-secondary)" font-size="9">  미분: 0~1</text>
    </svg>
    <p class="diagram-caption">주요 활성화 함수의 그래프와 특성 비교</p>
  </div>

  <div class="info-box info">
    <strong>LLM에서의 활성화 함수:</strong>
    <ul>
      <li><strong>ReLU 또는 GELU</strong>: 은닉층에서 주로 사용 (GELU는 BERT, GPT 등 최신 모델에서 표준)</li>
      <li><strong>Softmax</strong>: 마지막 층에서 다음 토큰 확률 계산</li>
      <li><strong>Sigmoid</strong>: 이진 분류나 Gates (LSTM 등)에서 사용</li>
    </ul>
  </div>
</section>

<section class="content-section">
  <h2 id="calculus-basics">미분과 기울기: 학습의 핵심</h2>
  <p>LLM은 <strong>경사 하강법(Gradient Descent)</strong>으로 학습합니다. 이 섹션에서는 미분과 기울기의 개념을 다룹니다.</p>

  <h3 id="derivative-intuition">미분: 변화율의 측정</h3>
  <p><strong>미분(differentiation)</strong>은 "변수가 조금 변할 때 함수의 출력은 얼마나 변하는가"를 측정합니다. ML에서 이것을用来 학습 방향을 결정합니다.</p>
  <pre><code><span class="cmt"># 미분의 정의</span>
f'(x) = lim[h→0] (f(x+h) - f(x)) / h

<span class="cmt"># 직관적 의미: "x가 1 증가하면 f(x)는 얼마나 증가하는가?"</span>

<span class="cmt"># 간단한 예: f(x) = x²</span>
<span class="cmt"># x = 3에서:</span>
<span class="cmt"># f'(3) = 2 × 3 = 6</span>
<span class="cmt"># → x가 1 증가하면 f(x)가 약 6 증가</span>

<span class="cmt"># ML에서 자주 등장하는 미분:</span>
<span class="cmt"># f(x) = xⁿ     → f'(x) = n × xⁿ⁻¹</span>
<span class="cmt"># f(x) = eˣ    → f'(x) = eˣ</span>
<span class="cmt"># f(x) = log(x) → f'(x) = 1/x</span></code></pre>

  <h3 id="gradient-descent">경사 하강법: 손실을 줄이는 법</h3>
  <p><strong>경사 하강법(Gradient Descent)</strong>은 함수의 최소값을 찾는 알고리즘입니다. ML에서는 "손실(loss)"을 최소화하는 파라미터를 찾습니다.</p>
  <pre><code><span class="cmt"># 경사 하강법 공식</span>
θ_new = θ_old - η × ∇L(θ)

<span class="cmt"># θ: 학습할 파라미터 (가중치)</span>
<span class="cmt"># η (eta): 학습률 (learning rate)</span>
<span class="cmt"># ∇L(θ): 손실 함수의 기울기</span>

<span class="cmt"># 구체적인 예: L(θ) = (θ - 3)²</span>
<span class="cmt"># 이 손실을 최소화하는 θ = 3</span>

<span class="cmt"># 1단계: θ = 0에서 시작</span>
<span class="cmt"># L(θ) = (0 - 3)² = 9</span>
<span class="cmt"># ∇L(θ) = 2 × (θ - 3) = 2 × (-3) = -6</span>
<span class="cmt"># θ_new = 0 - 0.1 × (-6) = 0.6</span>

<span class="cmt"># 2단계: θ = 0.6에서</span>
<span class="cmt"># L(θ) = (0.6 - 3)² = 5.76</span>
<span class="cmt"># ∇L(θ) = 2 × (0.6 - 3) = -4.8</span>
<span class="cmt"># θ_new = 0.6 - 0.1 × (-4.8) = 1.08</span>

<span class="cmt"># 3단계: θ = 1.08에서</span>
<span class="cmt"># θ_new = 1.08 - 0.1 × (-3.84) = 1.46</span>

<span class="cmt"># ... 반복하면 θ가 3에 수렴</span></code></pre>

  <div class="diagram-container">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 700 280"
         style="width:100%;max-width:700px;display:block;margin:1.5em auto;background:var(--diagram-fill);border-radius:8px;padding:20px;">
      <defs>
        <marker id="math-3-arrow" viewBox="0 0 10 7" refX="10" refY="3.5"
                markerWidth="10" markerHeight="7" orient="auto">
          <polygon points="0 0, 10 3.5, 0 7" fill="var(--diagram-arrow)"/>
        </marker>
      </defs>

      <path d="M 50,220 Q 150,20 350,220" fill="none" stroke="var(--diagram-arrow)" stroke-width="3"/>

      <circle cx="100" cy="100" r="8" fill="var(--accent-primary)"/>
      <text x="100" y="85" text-anchor="middle" fill="var(--diagram-text)" font-size="10">시작 θ=0</text>
      <line x1="100" y1="100" x2="150" y2="170" stroke="var(--diagram-accent)" stroke-width="2" stroke-dasharray="5,5"
            marker-end="url(#math-3-arrow)"/>

      <circle cx="150" cy="170" r="8" fill="var(--accent-primary)"/>
      <text x="150" y="195" text-anchor="middle" fill="var(--diagram-text)" font-size="10">θ=0.6</text>
      <line x1="150" y1="170" x2="195" y2="195" stroke="var(--diagram-accent)" stroke-width="2" stroke-dasharray="5,5"
            marker-end="url(#math-3-arrow)"/>

      <circle cx="195" cy="195" r="8" fill="var(--accent-primary)"/>
      <text x="195" y="220" text-anchor="middle" fill="var(--diagram-text)" font-size="10">θ=1.08</text>
      <line x1="195" y1="195" x2="235" y2="205" stroke="var(--diagram-accent)" stroke-width="2" stroke-dasharray="5,5"
            marker-end="url(#math-3-arrow)"/>

      <circle cx="280" cy="205" r="6" fill="var(--diagram-accent)"/>
      <text x="280" y="230" text-anchor="middle" fill="var(--diagram-text)" font-size="10">→ 수렴</text>

      <circle cx="350" cy="220" r="8" fill="var(--accent-primary)"/>
      <text x="350" y="250" text-anchor="middle" fill="var(--diagram-text)" font-size="11" font-weight="bold">최소값 θ=3</text>

      <text x="500" y="80" text-anchor="middle" fill="var(--diagram-text)" font-size="12" font-weight="bold">경사 하강법</text>
      <text x="500" y="110" text-anchor="middle" fill="var(--text-secondary)" font-size="11">θ_new = θ - η × ∇L(θ)</text>
      <text x="500" y="140" text-anchor="middle" fill="var(--text-secondary)" font-size="10">η(학습률)이 크면:</text>
      <text x="500" y="155" text-anchor="middle" fill="var(--text-secondary)" font-size="10">→ 수렴太快 (발산 위험)</text>
      <text x="500" y="175" text-anchor="middle" fill="var(--text-secondary)" font-size="10">η(학습률)이 작으면:</text>
      <text x="500" y="190" text-anchor="middle" fill="var(--text-secondary)" font-size="10">→ 수렴慢 (시간 오래 걸림)</text>
    </svg>
    <p class="diagram-caption">경사 하강법으로 손실 함수의 최소값 탐색</p>
  </div>

  <h3 id="chain-rule">연쇄 법칙: 복잡한 함수의 미분</h3>
  <p>LLM은 매우 깊은 신경망으로 구성됩니다. <strong>연쇄 법칙(chain rule)</strong>을 사용하면 여러 층을 통과한 미분을 계산할 수 있습니다. 이것이 <strong>역전파(backpropagation)</strong>의 수학적 기초입니다.</p>
  <pre><code><span class="cmt"># 연쇄 법칙</span>
<span class="cmt"># y = f(g(x))일 때,</span>
<span class="cmt"># dy/dx = (df/dg) × (dg/dx)</span>

<span class="cmt"># 구체적인 예:</span>
<span class="cmt"># x → g(x) = 2x + 1 → f(g) = g²</span>
<span class="cmt"># y = f(g(x)) = (2x + 1)²</span>

<span class="cmt"># 연쇄 법칙으로 미분:</span>
<span class="cmt"># dg/dx = 2</span>
<span class="cmt"># df/dg = 2g = 2(2x + 1) = 4x + 2</span>
<span class="cmt"># dy/dx = 2 × (4x + 2) = 8x + 4</span>

<span class="cmt"># 신경망에서 역전파:</span>
<span class="cmt"># 출력층 ← 은닉층 ← 입력층</span>
<span class="cmt"># 각 층의 기울기를 차례로 곱해감</span>
<span class="cmt"># ∇L/∂W = (∂L/∂a) × (∂a/∂z) × (∂z/∂W)</span></code></pre>

  <h3 id="backprop-visual">역전파 개념도</h3>
  <div class="diagram-container">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 700 250"
         style="width:100%;max-width:700px;display:block;margin:1.5em auto;background:var(--diagram-fill);border-radius:8px;padding:20px;">
      <defs>
        <marker id="math-4-arrow" viewBox="0 0 10 7" refX="10" refY="3.5"
                markerWidth="10" markerHeight="7" orient="auto">
          <polygon points="0 0, 10 3.5, 0 7" fill="var(--diagram-arrow)"/>
        </marker>
      </defs>

      <rect x="30" y="100" width="80" height="50" rx="6"
            fill="var(--bg-secondary)" stroke="var(--border-color)" stroke-width="1.5"/>
      <text x="70" y="130" text-anchor="middle" fill="var(--diagram-text)" font-size="12">입력 x</text>

      <rect x="170" y="100" width="80" height="50" rx="6"
            fill="var(--bg-secondary)" stroke="var(--border-color)" stroke-width="1.5"/>
      <text x="210" y="130" text-anchor="middle" fill="var(--diagram-text)" font-size="12">은닉 h</text>

      <rect x="310" y="100" width="80" height="50" rx="6"
            fill="var(--bg-secondary)" stroke="var(--border-color)" stroke-width="1.5"/>
      <text x="350" y="130" text-anchor="middle" fill="var(--diagram-text)" font-size="12">출력 y</text>

      <rect x="430" y="90" width="120" height="70" rx="6"
            fill="var(--bg-secondary)" stroke="var(--accent-primary)" stroke-width="2"/>
      <text x="490" y="115" text-anchor="middle" fill="var(--diagram-text)" font-size="12" font-weight="bold">손실 계산</text>
      <text x="490" y="140" text-anchor="middle" fill="var(--text-secondary)" font-size="10">L = (y - ŷ)²</text>

      <line x1="110" y1="125" x2="170" y2="125" stroke="var(--diagram-arrow)" stroke-width="2"
            marker-end="url(#math-4-arrow)"/>
      <line x1="250" y1="125" x2="310" y2="125" stroke="var(--diagram-arrow)" stroke-width="2"
            marker-end="url(#math-4-arrow)"/>

      <path d="M 430,125 Q 390,160 350,125" fill="none" stroke="var(--diagram-accent)" stroke-width="2" stroke-dasharray="5,5"/>
      <path d="M 310,125 Q 250,180 210,125" fill="none" stroke="var(--diagram-accent)" stroke-width="2" stroke-dasharray="5,5"/>
      <path d="M 210,125 Q 150,160 110,125" fill="none" stroke="var(--diagram-accent)" stroke-width="2" stroke-dasharray="5,5"/>

      <text x="350" y="210" text-anchor="middle" fill="var(--diagram-text)" font-size="11" font-weight="bold">역전파 (Backpropagation)</text>
      <text x="350" y="230" text-anchor="middle" fill="var(--text-secondary)" font-size="10">← 손실 → 은닉 → 입력 (기울기 순전파)</text>
    </svg>
    <p class="diagram-caption">역전파: 손실로부터 입력 방향으로 기울기 계산</p>
  </div>

  <h3 id="gradient-problems">Gradient Vanishing & Exploding: 학습의 병목</h3>
  <p>매우 깊은 신경망(LLM은 수십~수백 층)에서 학습할 때 발생하는 핵심 문제를 살펴봅니다.</p>

  <h4 id="vanishing-gradient">Gradient Vanishing (기울기 소실)</h4>
  <p>역전파 시 층을 거遡しながら gradient가 지수적으로 감소합니다. 앞쪽 층의 가중치가 거의 업데이트되지 않아 학습이 진행되지 않습니다.</p>
  <pre><code><span class="cmt"># 문제 원인: Sigmoid/Tanh의 미분값이 1보다 작음</span>

<span class="cmt"># Sigmoid 미분의 최대값</span>
d/dx σ(x) = σ(x)(1 - σ(x)) ≤ <span class="num">0.25</span>

<span class="cmt"># 100층의神经网络에서 역전파 시:</span>
<span class="cmt"># gradient_100 = 0.25 × 0.25 × ... × 0.25 (100번)</span>
<span class="cmt">#           ≈ 0.25^100 ≈ 10^-60</span>
<span class="cmt"># → 앞쪽 층의 gradient가 거의 0!</span>

<span class="cmt"># 해결 방법:</span>
<span class="cmt"># 1) ReLU 사용 (gradient = 0 또는 1)</span>
<span class="cmt"># 2) Residual Connection (덜 거침)</span>
<span class="cmt"># 3) 적절한 가중치 초기화</span>
<span class="cmt"># 4) Batch/Layer Normalization</span></code></pre>

  <h4 id="exploding-gradient">Gradient Exploding (기울기 폭발)</h4>
  <p>반대로 gradient가 지수적으로 증가하여 학습이 불안정해집니다. 가중치가 크게振荡하면서 수렴하지 않습니다.</p>
  <pre><code><span class="cmt"># 문제 원인: 가중치 > 1인 경우 gradient가 증폭</span>

<span class="cmt"># 예시: 50층에서 gradient 계산</span>
<span class="cmt"># gradient_50 = 1.5 × 1.5 × ... × 1.5 (50번)</span>
<span class="cmt">#           ≈ 1.5^50 ≈ 10^9</span>
<span class="cmt"># → 가중치 업데이트가 폭발적!</span>

<span class="cmt"># 증상:</span>
<span class="cmt"># - 손실이 NaN으로 변함</span>
<span class="cmt"># - 가중치가 inf 또는 NaN</span>
<span class="cmt"># - 출력 값이极端적으로 커짐</span>

<span class="cmt"># 해결 방법:</span>
<span class="cmt"># 1) Gradient Clipping (임계값으로 자르기)</span>
<span class="cmt"># 2) 적절한 학습률 설정</span>
<span class="cmt"># 3) 가중치 정규화 (Weight Decay)</span></code></pre>

  <div class="diagram-container">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 700 280"
         style="width:100%;max-width:700px;display:block;margin:1.5em auto;background:var(--diagram-fill);border-radius:8px;padding:20px;">
      <defs>
        <marker id="math-9-arrow" viewBox="0 0 10 7" refX="10" refY="3.5"
                markerWidth="10" markerHeight="7" orient="auto">
          <polygon points="0 0, 10 3.5, 0 7" fill="var(--diagram-arrow)"/>
        </marker>
      </defs>

      <text x="350" y="25" text-anchor="middle" fill="var(--diagram-text)" font-size="13" font-weight="bold">역전파에서 Gradient 흐름</text>

      <rect x="30" y="80" width="60" height="40" rx="4" fill="var(--bg-secondary)" stroke="var(--border-color)" stroke-width="1.5"/>
      <text x="60" y="105" text-anchor="middle" fill="var(--diagram-text)" font-size="10">Layer 1</text>

      <rect x="120" y="80" width="60" height="40" rx="4" fill="var(--bg-secondary)" stroke="var(--border-color)" stroke-width="1.5"/>
      <text x="150" y="105" text-anchor="middle" fill="var(--diagram-text)" font-size="10">...</text>

      <rect x="200" y="80" width="60" height="40" rx="4" fill="var(--bg-secondary)" stroke="var(--border-color)" stroke-width="1.5"/>
      <text x="230" y="105" text-anchor="middle" fill="var(--diagram-text)" font-size="10">Layer N</text>

      <rect x="300" y="80" width="60" height="40" rx="4" fill="var(--bg-secondary)" stroke="var(--border-color)" stroke-width="1.5"/>
      <text x="330" y="105" text-anchor="middle" fill="var(--diagram-text)" font-size="10">출력</text>

      <text x="60" y="155" text-anchor="middle" fill="var(--diagram-text)" font-size="10">역전파:</text>
      <text x="60" y="175" text-anchor="middle" fill="var(--diagram-text)" font-size="9">←梯度 전파</text>

      <path d="M 330,100 Q 280,80 230,100" fill="none" stroke="var(--diagram-accent)" stroke-width="3"/>
      <path d="M 230,100 Q 180,80 150,100" fill="none" stroke="var(--diagram-accent)" stroke-width="2"/>
      <path d="M 150,100 Q 100,80 60,100" fill="none" stroke="var(--diagram-accent)" stroke-width="1"/>

      <text x="150" y="160" text-anchor="middle" fill="var(--accent-primary)" font-size="10" font-weight="bold">Gradient Vanishing</text>
      <text x="150" y="178" text-anchor="middle" fill="var(--text-secondary)" font-size="9">0.25^N → 0</text>

      <rect x="460" y="40" width="210" height="90" rx="6" fill="none" stroke="var(--accent-primary)" stroke-width="2"/>
      <text x="565" y="65" text-anchor="middle" fill="var(--accent-primary)" font-size="12" font-weight="bold">해결 방법</text>
      <text x="475" y="90" text-anchor="start" fill="var(--text-secondary)" font-size="10">• ReLU/GELU 활성화</text>
      <text x="475" y="108" text-anchor="start" fill="var(--text-secondary)" font-size="10">• Residual Connection</text>
      <text x="475" y="126" text-anchor="start" fill="var(--text-secondary)" font-size="10">• Layer/Batch Normalization</text>

      <rect x="460" y="150" width="210" height="90" rx="6" fill="none" stroke="var(--diagram-arrow)" stroke-width="2"/>
      <text x="565" y="175" text-anchor="middle" fill="var(--diagram-arrow)" font-size="12" font-weight="bold">해결 방법</text>
      <text x="475" y="200" text-anchor="start" fill="var(--text-secondary)" font-size="10">• Gradient Clipping</text>
      <text x="475" y="218" text-anchor="start" fill="var(--text-secondary)" font-size="10">• 적절한 학습률</text>
      <text x="475" y="236" text-anchor="start" fill="var(--text-secondary)" font-size="10">• Weight Decay</text>
    </svg>
    <p class="diagram-caption">깊은 신경망에서 Gradient Vanishing/Exploding 문제와 해결 방법</p>
  </div>
</section>

<section class="content-section">
  <h2 id="probability-basics">확률론 기초: 불확실성의 수학</h2>
  <p>LLM은 본질적으로 확률 모델입니다. 출력이 확률 분포에서 샘플링되고, 학습은 확률을 최대화하는 방향으로 진행됩니다.</p>

  <h3 id="probability-distribution">확률 분포</h3>
  <p><strong>확률 분포</strong>는 각 가능한 결과가 나타날 확률을 정의합니다. LLM에서는 다음 토큰이 될 수 있는 모든 단어에 확률을 할당합니다.</p>
  <pre><code><span class="cmt"># 확률 분포의 조건</span>
<span class="cmt"># 1) 모든 확률은 0 이상: P(x) ≥ 0</span>
<span class="cmt"># 2) 확률의 합은 1: Σ P(x) = 1</span>

<span class="cmt"># LLM의 다음 토큰 확률 분포 예시</span>
<span class="cmt"># 입력: "서울의"</span>
next_token_probs = {
  <span class="str">"수"</span>: <span class="num">0.35</span>,    <span class="cmt"># "서울의 수"</span>
  <span class="str">"首都"</span>: <span class="num">0.25</span>,    <span class="cmt"># "서울의 수도"</span>
  <span class="str">"날씨"</span>: <span class="num">0.15</span>,    <span class="cmt"># "서울의 날씨"</span>
  <span class="str">"맛"</span>: <span class="num">0.10</span>,     <span class="cmt"># "서울의 맛"</span>
  <span class="str">"...</span>   <span class="num">0.15</span>     <span class="cmt"># 다른 단어들</span>
}
<span class="cmt"># 합계: 1.0 ✓</span></code></pre>

  <h3 id="softmax-function">Softmax: 로짓을 확률로</h3>
  <p>LLM의 마지막 층은 <strong>로짓(logit)</strong>이라는 점수를 출력합니다. Softmax 함수가 이 점수를 확률로 변환합니다.</p>
  <pre><code><span class="cmt"># Softmax 함수</span>
<span class="fn">softmax</span>(z_i) = <span class="fn">exp</span>(z_i) / Σⱼ <span class="fn">exp</span>(z_j)

<span class="cmt"># z: 로짓 (LLM 출력 점수)</span>
<span class="cmt"># exp(): 지수 함수 (양수)</span>

<span class="cmt"># 구체적인 예</span>
z = [<span class="num">2.0</span>, <span class="num">1.0</span>, <span class="num">0.5</span>]  <span class="cmt"># 로짓</span>

<span class="cmt"># exp(z)</span>
exp_z = [<span class="num">7.39</span>, <span class="num">2.72</span>, <span class="num">1.65</span>]

<span class="cmt"># 합계</span>
sum_exp = <span class="num">7.39</span> + <span class="num">2.72</span> + <span class="num">1.65</span> = <span class="num">11.76</span>

<span class="cmt"># Softmax (확률)</span>
P = [<span class="num">7.39/11.76</span>, <span class="num">2.72/11.76</span>, <span class="num">1.65/11.76</span>]
  = [<span class="num">0.63</span>, <span class="num">0.23</span>, <span class="num">0.14</span>]

<span class="cmt"># 검증: 0.63 + 0.23 + 0.14 = 1.0 ✓</span></code></pre>

  <div class="diagram-container">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 700 280"
         style="width:100%;max-width:700px;display:block;margin:1.5em auto;background:var(--diagram-fill);border-radius:8px;padding:20px;">
      <defs>
        <marker id="math-5-arrow" viewBox="0 0 10 7" refX="10" refY="3.5"
                markerWidth="10" markerHeight="7" orient="auto">
          <polygon points="0 0, 10 3.5, 0 7" fill="var(--diagram-arrow)"/>
        </marker>
      </defs>

      <rect x="30" y="80" width="140" height="80" rx="6"
            fill="var(--bg-secondary)" stroke="var(--border-color)" stroke-width="1.5"/>
      <text x="100" y="105" text-anchor="middle" fill="var(--diagram-text)" font-size="12">로짓 (Logits)</text>
      <text x="100" y="130" text-anchor="middle" fill="var(--text-secondary)" font-size="11">[2.0, 1.0, 0.5]</text>
      <text x="100" y="150" text-anchor="middle" fill="var(--text-secondary)" font-size="10">LLM 최종 층 출력</text>

      <rect x="300" y="80" width="140" height="80" rx="6"
            fill="var(--bg-secondary)" stroke="var(--border-color)" stroke-width="1.5"/>
      <text x="370" y="105" text-anchor="middle" fill="var(--diagram-text)" font-size="12">exp() 적용</text>
      <text x="370" y="130" text-anchor="middle" fill="var(--text-secondary)" font-size="11">[7.4, 2.7, 1.6]</text>
      <text x="370" y="150" text-anchor="middle" fill="var(--text-secondary)" font-size="10">양수로 변환</text>

      <rect x="530" y="80" width="140" height="80" rx="6"
            fill="var(--bg-secondary)" stroke="var(--accent-primary)" stroke-width="2"/>
      <text x="600" y="105" text-anchor="middle" fill="var(--diagram-text)" font-size="12" font-weight="bold">확률 (Prob)</text>
      <text x="600" y="130" text-anchor="middle" fill="var(--diagram-text)" font-size="11">[0.63, 0.23, 0.14]</text>
      <text x="600" y="150" text-anchor="middle" fill="var(--text-secondary)" font-size="10">합계 = 1.0</text>

      <line x1="170" y1="120" x2="300" y2="120" stroke="var(--diagram-arrow)" stroke-width="2"
            marker-end="url(#math-5-arrow)"/>
      <line x1="440" y1="120" x2="530" y2="120" stroke="var(--diagram-arrow)" stroke-width="2"
            marker-end="url(#math-5-arrow)"/>

      <rect x="100" y="200" width="500" height="60" rx="6"
            fill="var(--bg-secondary)" stroke="var(--border-color)" stroke-width="1"/>
      <text x="350" y="225" text-anchor="middle" fill="var(--diagram-text)" font-size="11">softmax(zᵢ) = exp(zᵢ) / Σ exp(zⱼ)</text>
      <text x="350" y="245" text-anchor="middle" fill="var(--text-secondary)" font-size="10">높은 로짓 → 높은 확률, 합계 정규화</text>
    </svg>
    <p class="diagram-caption">Softmax로 로짓을 확률 분포로 변환</p>
  </div>

  <h3 id="cross-entropy">크로스엔트로피: 학습의 손실 함수</h3>
  <p>LLM의 학습은 <strong>크로스엔트로피(Cross-Entropy)</strong> 손실 함수를 사용합니다. 이것은 "예측 확률 분포"와 "진짜 확률 분포"의 차이를 측정합니다.</p>
  <pre><code><span class="cmt"># 크로스엔트로피</span>
H(p, q) = - Σ p(x) × log(q(x))

<span class="cmt"># p(x): 실제 분포 (원-핫: 정답만 1, 나머지는 0)</span>
<span class="cmt"># q(x): 모델 예측 분포 (softmax 출력)</span>

<span class="cmt"># 구체적인 예</span>
<span class="cmt"># 정답: "수" (원-핫 인코딩)</span>
p = [<span class="num">1</span>, <span class="num">0</span>, <span class="num">0</span>]  <span class="cmt"># "수"=1, others=0</span>

<span class="cmt"># 모델 예측</span>
q = [<span class="num">0.63</span>, <span class="num">0.23</span>, <span class="num">0.14</span>]

<span class="cmt"># 크로스엔트로피 계산</span>
<span class="cmt"># H = -[1×log(0.63) + 0×log(0.23) + 0×log(0.14)]</span>
<span class="cmt"># H = -log(0.63) ≈ 0.46</span>

<span class="cmt"># 해석:</span>
<span class="cmt"># - 정답 확률이 높으면 (0.99) → loss ≈ 0.01 (작음)</span>
<span class="cmt"># - 정답 확률이 낮으면 (0.01) → loss ≈ 4.61 ( 큼)</span></code></pre>

  <h3 id="negative-log-likelihood">음성 로그 우도: 크로스엔트로피의 다른 표현</h3>
  <p>실제 구현에서는 <strong>음성 로그 우도(Negative Log-Likelihood, NLL)</strong>를 더 자주 사용합니다. 원-핫 인코딩에서는 두 값이 같습니다.</p>
  <pre><code><span class="cmt"># 음성 로그 우도 (NLL)</span>
NLL = -log(q[target])

<span class="cmt"># 위 예시에서:</span>
<span class="cmt"># target = "수" (index 0)</span>
<span class="cmt"># q[0] = 0.63</span>
<span class="cmt"># NLL = -log(0.63) ≈ 0.46</span>

<span class="cmt"># 왜 로그를 사용할까?</span>
<span class="cmt"># - 확률의 곱 → 로그의 합 (계산 편리)</span>
<span class="cmt"># - 0~1 사이 값 → 로그 후 음수 (손실로 적합)</span>
<span class="cmt"># - 정답 확률 1.0 → log(1) = 0 (손실 0) ✓</span>
<span class="cmt"># - 정답 확률 0.1 → log(0.1) ≈ -2.3 (큰 손실)</span></code></pre>

  <div class="diagram-container">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 700 260"
         style="width:100%;max-width:700px;display:block;margin:1.5em auto;background:var(--diagram-fill);border-radius:8px;padding:20px;">
      <defs>
        <marker id="math-6-arrow" viewBox="0 0 10 7" refX="10" refY="3.5"
                markerWidth="10" markerHeight="7" orient="auto">
          <polygon points="0 0, 10 3.5, 0 7" fill="var(--diagram-arrow)"/>
        </marker>
      </defs>

      <text x="350" y="35" text-anchor="middle" fill="var(--diagram-text)" font-size="13" font-weight="bold">손실(Loss)과 정답 확률의 관계</text>

      <line x1="50" y1="200" x2="650" y2="200" stroke="var(--border-color)" stroke-width="1"/>
      <line x1="50" y1="200" x2="50" y2="50" stroke="var(--border-color)" stroke-width="1"/>

      <path d="M 50,200 Q 150,200 250,200" fill="none" stroke="var(--accent-primary)" stroke-width="3"/>
      <path d="M 250,200 Q 350,100 450,60" fill="none" stroke="var(--diagram-arrow)" stroke-width="3"/>
      <path d="M 450,60 Q 550,30 650,20" fill="none" stroke="var(--diagram-arrow)" stroke-width="3"/>

      <text x="80" y="220" text-anchor="middle" fill="var(--text-secondary)" font-size="10">0.0</text>
      <text x="350" y="220" text-anchor="middle" fill="var(--text-secondary)" font-size="10">0.5</text>
      <text x="620" y="220" text-anchor="middle" fill="var(--text-secondary)" font-size="10">1.0</text>

      <text x="30" y="70" text-anchor="middle" fill="var(--text-secondary)" font-size="10">손실</text>
      <text x="30" y="200" text-anchor="middle" fill="var(--text-secondary)" font-size="10">높음</text>
      <text x="30" y="100" text-anchor="middle" fill="var(--text-secondary)" font-size="10">낮음</text>

      <circle cx="100" cy="200" r="6" fill="var(--accent-primary)"/>
      <text x="100" y="185" text-anchor="middle" fill="var(--text-secondary)" font-size="9">정답 확률 0</text>

      <circle cx="350" cy="130" r="6" fill="var(--accent-primary)"/>
      <text x="350" y="115" text-anchor="middle" fill="var(--text-secondary)" font-size="9">정답 확률 0.5</text>

      <circle cx="600" cy="40" r="6" fill="var(--accent-primary)"/>
      <text x="600" y="25" text-anchor="middle" fill="var(--text-secondary)" font-size="9">정답 확률 1.0</text>

      <rect x="100" y="230" width="180" height="20" rx="4" fill="var(--bg-secondary)" stroke="var(--border-color)"/>
      <text x="190" y="244" text-anchor="middle" fill="var(--text-secondary)" font-size="9">-log(정답 확률)</text>

      <rect x="420" y="230" width="180" height="20" rx="4" fill="var(--bg-secondary)" stroke="var(--border-color)"/>
      <text x="510" y="244" text-anchor="middle" fill="var(--text-secondary)" font-size="9">정답 확률 ↑ = 손실 ↓</text>
    </svg>
    <p class="diagram-caption">정답 확률이 높을수록 손실이 낮아지는 관계</p>
  </div>

  <div class="info-box info">
    <strong>핵심 정리:</strong>
    <ul>
      <li><strong>Softmax</strong>: 로짓 점수를 확률로 변환 (항상 합=1)</li>
      <li><strong>크로스엔트로피/NLL</strong>: 예측과 정답의 차이를 측정하는 손실</li>
      <li><strong>경사 하강법</strong>: 손실을 최소화하는 방향으로 가중치 업데이트</li>
      <li><strong>역전파</strong>: 연쇄 법칙으로 각 가중치의 기여도 계산</li>
    </ul>
  </div>
</section>

<section class="content-section">
  <h2 id="information-theory">정보이론 관점</h2>
  <p>LLM의 학습과 평가는 정보이론 지표로 측정됩니다.</p>
  <ul>
    <li><strong>엔트로피(H)</strong>: 불확실성의 정도. 가능한 토큰이 많을수록 높아짐</li>
    <li><strong>크로스엔트로피(H(p,q))</strong>: 모델 예측(q)과 실제 분포(p)의 차이</li>
    <li><strong>퍼플렉서티(PPL)</strong>: 크로스엔트로피를 직관적으로 변환한 지표</li>
  </ul>

  <h3 id="entropy-intuition">엔트로피 직관: 불확실성의 척도</h3>
  <p>엔트로피는 "결과를 예측하기 얼마나 어려운가"를 수치화합니다. 일상적인 예시로 이해해 봅시다:</p>
  <pre><code><span class="cmt"># 공정한 동전 던지기: 2가지 결과, 각각 50%</span>
H = -(<span class="num">0.5</span> × log₂(<span class="num">0.5</span>) + <span class="num">0.5</span> × log₂(<span class="num">0.5</span>)) = <span class="num">1.0</span> bit

<span class="cmt"># 공정한 주사위: 6가지 결과, 각각 16.7%</span>
H = -6 × (<span class="num">0.167</span> × log₂(<span class="num">0.167</span>)) ≈ <span class="num">2.58</span> bits

<span class="cmt"># 편향된 동전 (앞면 90%): 거의 예측 가능</span>
H = -(<span class="num">0.9</span> × log₂(<span class="num">0.9</span>) + <span class="num">0.1</span> × log₂(<span class="num">0.1</span>)) ≈ <span class="num">0.47</span> bits

<span class="cmt"># LLM에서의 의미:</span>
<span class="cmt"># - 엔트로피가 높은 위치 = 다음 토큰이 불확실 (여러 후보 가능)</span>
<span class="cmt"># - 엔트로피가 낮은 위치 = 거의 확정적 ("서울의 수도는" → "한국")</span></code></pre>

  <pre><code><span class="cmt"># 크로스엔트로피</span>
H(p, q) = - ∑ p(x) log q(x)

<span class="cmt"># 퍼플렉서티</span>
PPL = exp(H(p, q))

<span class="cmt"># 직관적 관계</span>
<span class="kw">낮은</span> 크로스엔트로피 → <span class="kw">높은</span> 예측 정확도 → <span class="kw">낮은</span> 퍼플렉서티</code></pre>

  <h3 id="nll-loss">학습 손실: NLL (Negative Log-Likelihood)</h3>
  <p>LLM은 다음 토큰을 예측하는 방식으로 학습합니다. 학습 손실 함수는 크로스엔트로피와 직접 연결됩니다.</p>
  <pre><code><span class="cmt"># NLL Loss (다음 토큰 예측의 손실 함수)</span>
L = - ∑ <span class="kw">log</span> p(y_t | y_{&lt;t}, x)

<span class="cmt"># y_t : 실제 다음 토큰</span>
<span class="cmt"># y_{&lt;t} : 이전까지의 토큰 시퀀스</span>
<span class="cmt"># x : 입력 컨텍스트</span>
<span class="cmt"># 이 손실이 낮아질수록 모델의 예측이 정확해짐</span></code></pre>

  <h3 id="ppl-practice">퍼플렉서티 실무 해석</h3>
  <p>퍼플렉서티는 "모델이 매 토큰을 예측할 때 평균적으로 몇 개의 후보 사이에서 고민하는가"로 직관적으로 해석됩니다.</p>
  <table>
    <thead>
      <tr>
        <th>PPL 범위</th>
        <th>의미</th>
        <th>체감 품질</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>1~5</td>
        <td>거의 확정적 예측</td>
        <td>반복적이거나 공식적인 문장 (법률 문서, 코드 보일러플레이트)</td>
      </tr>
      <tr>
        <td>5~15</td>
        <td>양호한 예측</td>
        <td>자연스러운 문장 생성, 대부분의 실용적 용도에 충분</td>
      </tr>
      <tr>
        <td>15~50</td>
        <td>보통 수준의 불확실성</td>
        <td>다소 부자연스러운 표현이 간헐적으로 발생</td>
      </tr>
      <tr>
        <td>50+</td>
        <td>높은 불확실성</td>
        <td>모델이 해당 도메인/언어에 약하다는 신호</td>
      </tr>
    </tbody>
  </table>
  <div class="info-box info">
    <strong>비교 기준:</strong> 같은 데이터셋에서 모델 A(PPL=8)와 모델 B(PPL=25)를 비교하면, A는 평균 8개 후보 중 고르는 수준이고 B는 25개 후보 중 고르는 수준입니다. A가 해당 도메인에서 훨씬 강합니다.
  </div>
</section>

<section class="content-section">
  <h2 id="generalization">일반화와 과적합</h2>
  <p>LLM은 학습 데이터와 다른 입력에도 답해야 합니다. 일반화가 좋으면 새로운 입력에 안정적으로 대응합니다.</p>
  <ul>
    <li><strong>일반화</strong>: 보지 못한 입력에 대한 성능 — LLM의 실제 가치</li>
    <li><strong>과적합</strong>: 학습 데이터에만 강하고 새로운 입력에 취약</li>
    <li><strong>데이터 분포</strong>: 학습 데이터가 실제 사용자 입력과 일치해야 일반화 향상</li>
  </ul>

  <h3 id="distribution-mismatch">학습·검증·배포 분포 불일치</h3>
  <p>모델의 일반화 실패는 대부분 분포 불일치(distribution shift)에서 비롯됩니다. 학습 데이터, 검증 데이터, 실제 배포 환경의 입력이 각각 다른 특성을 가질 수 있습니다.</p>
  <ul>
    <li><strong>학습-검증 불일치</strong>: 검증 데이터가 학습 데이터와 너무 유사하면 과적합을 발견하지 못함</li>
    <li><strong>검증-배포 불일치</strong>: 벤치마크에서는 우수하지만 실제 사용자 입력에서는 성능 저하</li>
    <li><strong>시간적 불일치</strong>: 학습 이후 세계가 변화 — 최신 사건, 신기술에 대한 지식 부재</li>
  </ul>

  <h3 id="zero-shot-generalization">LLM의 제로샷 일반화</h3>
  <p>대규모 사전학습 모델은 학습 중 명시적으로 접하지 않은 태스크에서도 놀라운 성능을 보입니다. 이것이 <strong>제로샷(zero-shot) 일반화</strong>이며, LLM의 가장 독특한 특성 중 하나입니다.</p>
  <pre><code><span class="cmt"># 제로샷 일반화 예시</span>
<span class="cmt"># 학습 시: "다음 토큰 예측"만 수행</span>
<span class="cmt"># 배포 시: 아래와 같은 태스크를 별도 학습 없이 수행</span>

<span class="str">"이 리뷰의 감정을 판별하세요: '정말 맛있었어요!'"</span>
→ 긍정  <span class="cmt"># 감정 분류를 학습한 적 없지만 수행 가능</span>

<span class="str">"Translate to English: 오늘 날씨가 좋습니다"</span>
→ The weather is nice today  <span class="cmt"># 번역 태스크도 패턴으로 학습</span></code></pre>

  <div class="info-box warning">
    <strong>파인튜닝 시 데이터 다양성:</strong> 파인튜닝 데이터가 편향되면 특정 도메인에서만 강한 모델이 됩니다. 예를 들어 의료 텍스트만으로 파인튜닝하면 법률 질문에서 성능이 급락할 수 있습니다. 도메인, 스타일, 난이도, 언어를 다양하게 구성하는 것이 핵심입니다.
  </div>
</section>

<section class="content-section">
  <h2 id="hallucination">환각(허위 응답)의 원인</h2>
  <ul>
    <li><strong>확률적 생성</strong>: 낮은 확률의 토큰이 선택될 수 있음 — 그럴듯하지만 사실이 아닌 내용 생성</li>
    <li><strong>지식 공백</strong>: 학습 데이터에 없는 정보를 패턴으로 "채우려" 시도</li>
    <li><strong>프롬프트 불명확</strong>: 조건이 모호한 질문은 모델의 추측 범위를 넓힘</li>
  </ul>

  <h3 id="hallucination-mechanism">환각 발생 메커니즘</h3>
  <p>환각은 모델의 확률 분포 관점에서 이해할 수 있습니다. 모델이 다음 토큰을 예측할 때, 학습 데이터에서 본 패턴을 기반으로 확률을 할당합니다. 문제는 이 확률이 "사실 여부"가 아닌 "통계적 패턴"을 반영한다는 점입니다.</p>
  <pre><code><span class="cmt"># 환각 발생 과정 (개념적)</span>
입력: <span class="str">"알베르트 아인슈타인이 노벨상을 받은 연도는"</span>

<span class="cmt"># 모델 내부 확률 분포:</span>
<span class="str">"1921"</span>  → P = <span class="num">0.45</span>  <span class="cmt"># 정답 (물리학상)</span>
<span class="str">"1905"</span>  → P = <span class="num">0.25</span>  <span class="cmt"># 특수상대성이론 발표 연도 (오답이지만 관련)</span>
<span class="str">"1915"</span>  → P = <span class="num">0.15</span>  <span class="cmt"># 일반상대성이론 발표 연도 (오답)</span>
<span class="str">"1922"</span>  → P = <span class="num">0.10</span>  <span class="cmt"># 수상 발표/수령 연도 혼동</span>

<span class="cmt"># 온도가 높으면 오답이 선택될 확률 증가</span>
<span class="cmt"># "1905"가 선택되면 → 그럴듯하지만 사실이 아닌 환각</span></code></pre>

  <h3 id="hallucination-types">환각의 주요 유형</h3>
  <p><strong>사실적 환각(Factual Hallucination)</strong>: 존재하지 않는 사실을 만들어내는 경우입니다.</p>
  <pre><code><span class="cmt"># 사실적 환각 사례: 존재하지 않는 논문 인용</span>
<span class="kw">사용자</span>: <span class="str">"Transformer의 attention 효율성에 관한 논문을 추천해주세요"</span>

<span class="kw">모델 응답 (환각)</span>:
<span class="str">"Smith et al. (2023) 'Efficient Sparse Attention for</span>
<span class="str"> Long-Range Dependencies' - Journal of Machine Learning"</span>

<span class="cmt"># 문제: 저자, 제목, 저널 모두 그럴듯하지만</span>
<span class="cmt"># 실제로 존재하지 않는 논문</span>
<span class="cmt"># 학술 형식 패턴을 학습했기 때문에 형식은 완벽함</span></code></pre>

  <p><strong>논리적 환각(Logical Hallucination)</strong>: 각 단계는 그럴듯하지만 추론 체인이 잘못된 경우입니다.</p>
  <pre><code><span class="cmt"># 논리적 환각 사례: 잘못된 추론 체인</span>
<span class="kw">사용자</span>: <span class="str">"서울에서 부산까지 빛의 속도로 이동하면 몇 초?"</span>

<span class="kw">모델 응답 (환각)</span>:
<span class="str">"서울-부산 거리: 약 325km</span>
<span class="str"> 빛의 속도: 약 300,000km/s</span>
<span class="str"> 계산: 325 / 300,000 = 약 0.001초"</span>  ✓ <span class="cmt">정답</span>

<span class="kw">하지만 유사 질문에서:</span>
<span class="str">"서울-부산 거리: 약 325km</span>
<span class="str"> 빛의 속도: 약 30만km/s</span>
<span class="str"> 계산: 325 × 30 = 약 9,750초"</span>  ✗ <span class="cmt">곱셈/나눗셈 혼동</span>

<span class="cmt"># 원인: 토큰 단위 생성이므로 수식의 논리적 일관성을 보장하지 않음</span></code></pre>

  <h3 id="hallucination-mitigation">환각 완화 전략 비교</h3>
  <table>
    <thead>
      <tr>
        <th>전략</th>
        <th>효과</th>
        <th>구현 비용</th>
        <th>제약</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>RAG (외부 지식 검색)</td>
        <td>사실적 환각 크게 감소</td>
        <td>중간~높음</td>
        <td>검색 품질에 의존, 지연 시간 증가</td>
      </tr>
      <tr>
        <td>낮은 온도 (0.1~0.3)</td>
        <td>노이즈 토큰 억제</td>
        <td>낮음</td>
        <td>다양성 감소, 반복 위험</td>
      </tr>
      <tr>
        <td>자기검증 프롬프트</td>
        <td>논리적 환각 부분 완화</td>
        <td>낮음</td>
        <td>토큰 사용량 2배, 완전하지 않음</td>
      </tr>
      <tr>
        <td>도구 호출 (계산기, 검색)</td>
        <td>산술/사실 오류 제거</td>
        <td>중간</td>
        <td>도구 연동 필요, 호출 지연</td>
      </tr>
      <tr>
        <td>다중 샘플링 (Self-Consistency)</td>
        <td>일관된 답변 선별</td>
        <td>높음 (N배 비용)</td>
        <td>모든 샘플이 동일하게 틀릴 수 있음</td>
      </tr>
    </tbody>
  </table>

  <h3 id="hallucination-detection">환각 감지 기법</h3>
  <ul>
    <li><strong>자기일관성 검사</strong>: 같은 질문을 여러 번 질문하여 답변 간 일치도를 측정 — 답변이 자주 바뀌면 환각 가능성이 높음</li>
    <li><strong>외부 검증</strong>: 모델의 주장을 검색 엔진, 데이터베이스, API로 교차 확인</li>
    <li><strong>불확실성 표현 감지</strong>: 모델이 "아마도", "~일 수 있습니다" 같은 헤지 표현을 사용할 때 해당 부분을 집중 검증</li>
  </ul>
  <div class="info-box warning">
    <strong>주의:</strong> 환각은 "버그"라기보다 확률적 생성의 자연스러운 결과입니다. 완전히 제거할 수 없으며, 여러 완화 전략을 조합하는 것이 가장 효과적입니다.
  </div>
</section>

<section class="content-section">
  <h2 id="uncertainty">불확실성 다루기</h2>
  <ul>
    <li><strong>자기검증</strong>: 모델이 스스로 근거를 재점검하도록 유도</li>
    <li><strong>근거 제공</strong>: 출처를 명시하도록 프롬프트 설계</li>
    <li><strong>RAG (Retrieval-Augmented Generation)</strong>: 외부 지식으로 보강하여 사실성 향상</li>
    <li><strong>도구 호출</strong>: 계산·검색·API 호출로 정확한 정보 획득</li>
  </ul>

  <h3 id="self-verify-prompt">자기검증 프롬프트 패턴</h3>
  <p>모델이 자신의 답변을 스스로 검증하도록 유도하는 프롬프트 기법입니다:</p>
  <pre><code><span class="cmt"># 자기검증 프롬프트 예시</span>
<span class="kw">System</span>: <span class="str">"답변 후 다음 형식으로 자기검증을 수행하세요:</span>
<span class="str"> [검증] 위 답변에서 사실 확인이 필요한 부분:</span>
<span class="str"> - 확인 필요 1: ...</span>
<span class="str"> - 확인 필요 2: ...</span>
<span class="str"> [확신도] 상/중/하"</span>

<span class="kw">User</span>: <span class="str">"한국의 GDP 순위는?"</span>

<span class="kw">Assistant</span>: <span class="str">"한국은 2024년 기준 세계 GDP 약 13위입니다.</span>
<span class="str"> [검증]</span>
<span class="str"> - 확인 필요 1: 정확한 순위는 측정 기관(IMF/World Bank)마다 다름</span>
<span class="str"> - 확인 필요 2: 2024년 최신 데이터가 반영되지 않았을 수 있음</span>
<span class="str"> [확신도] 중"</span></code></pre>

  <h3 id="uncertainty-comparison">전략별 효과 비교</h3>
  <table>
    <thead>
      <tr>
        <th>방법</th>
        <th>효과</th>
        <th>구현 복잡도</th>
        <th>지연 영향</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>자기검증 프롬프트</td>
        <td>논리적 오류 감소 (10~30%)</td>
        <td>낮음 (프롬프트만 변경)</td>
        <td>출력 토큰 1.5~2배</td>
      </tr>
      <tr>
        <td>출처 명시 요구</td>
        <td>허위 인용 감소</td>
        <td>낮음</td>
        <td>소폭 증가</td>
      </tr>
      <tr>
        <td>RAG</td>
        <td>사실 오류 대폭 감소 (50~80%)</td>
        <td>높음 (벡터DB, 검색 파이프라인)</td>
        <td>검색 지연 0.5~2초</td>
      </tr>
      <tr>
        <td>도구 호출</td>
        <td>산술/실시간 정보 정확</td>
        <td>중간 (API 연동)</td>
        <td>도구별 지연 0.1~5초</td>
      </tr>
    </tbody>
  </table>

  <h3 id="combined-strategy">복합 전략 조합</h3>
  <p>실무에서는 단일 전략보다 여러 전략을 조합할 때 가장 효과적입니다:</p>
  <pre><code><span class="cmt"># 복합 전략 파이프라인 예시</span>

<span class="cmt"># 1단계: RAG로 관련 문서 검색</span>
context = <span class="fn">retrieve</span>(query, top_k=<span class="num">5</span>)

<span class="cmt"># 2단계: 검색 결과를 포함한 프롬프트 + 자기검증 요청</span>
prompt = <span class="str">f"""다음 참고 자료를 바탕으로 답변하세요:
{context}
질문: {query}
답변 후 [검증] 섹션에서 근거를 확인하세요."""</span>

<span class="cmt"># 3단계: 수치 계산이 필요하면 도구 호출</span>
<span class="kw">if</span> needs_calculation(response):
    result = <span class="fn">calculator</span>(expression)

<span class="cmt"># 효과: RAG(사실성) + 자기검증(논리성) + 도구(정확성)</span></code></pre>
</section>

<!-- ============================================================ -->
<!--  심화 파트                                                     -->
<!-- ============================================================ -->

<section class="content-section">
  <h2 id="transformer">Transformer 직관</h2>
  <p>현대 LLM의 기반이 되는 Transformer 아키텍처의 핵심 구성요소를 살펴봅니다.</p>
  <ul>
    <li><strong>Self-Attention</strong>: 모든 토큰의 관계를 동시에 고려 — 순환 구조 없이 병렬 처리 가능</li>
    <li><strong>Residual Connection + LayerNorm</strong>: 깊은 네트워크에서도 안정적인 학습을 보장</li>
    <li><strong>Position Encoding</strong>: 토큰의 순서 정보를 벡터로 주입 (Transformer 자체는 순서를 모름)</li>
    <li><strong>Feed-Forward Network</strong>: 각 토큰의 표현을 비선형 변환으로 풍부하게 함</li>
  </ul>

  <h3 id="multi-head">Multi-Head Attention: 여러 관점으로 보기</h3>
  <p>단일 Attention 헤드는 하나의 관계 유형만 포착합니다. Multi-Head Attention은 여러 개의 독립적인 Attention을 병렬로 수행하여, 동시에 여러 관계를 학습합니다.</p>
  <pre><code><span class="cmt"># Multi-Head Attention 직관</span>
<span class="cmt"># "The cat sat on the mat"에서 "sat"을 처리할 때:</span>

<span class="kw">Head 1</span> (주어 관계):  <span class="str">"sat"</span> → <span class="str">"cat"</span>에 강하게 주목   <span class="cmt"># 누가?</span>
<span class="kw">Head 2</span> (위치 관계):  <span class="str">"sat"</span> → <span class="str">"on"</span>에 강하게 주목    <span class="cmt"># 어디?</span>
<span class="kw">Head 3</span> (구문 관계):  <span class="str">"sat"</span> → <span class="str">"mat"</span>에 강하게 주목   <span class="cmt"># 무엇 위?</span>

<span class="cmt"># 각 헤드의 결과를 연결(concatenate)한 후 선형 변환</span>
MultiHead = <span class="fn">Concat</span>(Head_1, Head_2, ..., Head_h) × W_O

<span class="cmt"># GPT-3: 96개 헤드, Claude/GPT-4: 더 많은 헤드</span>
<span class="cmt"># 헤드가 많을수록 다양한 언어적 관계를 포착</span></code></pre>

  <h3 id="why-transformer">왜 Transformer인가: RNN/LSTM과의 비교</h3>
  <p>Transformer 이전에는 RNN(순환 신경망)과 LSTM이 주류였습니다. Transformer가 이들을 대체한 핵심 이유는 <strong>병렬 처리</strong>입니다.</p>
  <ul>
    <li><strong>RNN/LSTM</strong>: 토큰을 순차적으로 처리 — t번째 토큰의 결과가 나와야 t+1번째를 처리할 수 있음. 긴 시퀀스에서 학습 속도가 급격히 저하</li>
    <li><strong>Transformer</strong>: 모든 토큰 쌍의 관계를 동시에 계산 — GPU의 병렬 처리 능력을 최대한 활용. 수천 토큰도 한 번에 처리 가능</li>
    <li><strong>장거리 의존성</strong>: RNN은 긴 문장에서 앞부분 정보를 "잊는" 문제가 있지만, Attention은 거리에 관계없이 직접 참조 가능</li>
  </ul>

  <div class="diagram-container">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 700 260"
         style="width:100%;max-width:700px;display:block;margin:1.5em auto;background:var(--diagram-fill);border-radius:8px;padding:20px;">
      <defs>
        <marker id="llm-theory-2-arrow" viewBox="0 0 10 7" refX="10" refY="3.5"
                markerWidth="10" markerHeight="7" orient="auto">
          <polygon points="0 0, 10 3.5, 0 7" fill="var(--diagram-arrow)"/>
        </marker>
      </defs>

      <rect x="30" y="40" width="130" height="60" rx="6"
            fill="var(--bg-secondary)" stroke="var(--border-color)" stroke-width="1.5"/>
      <text x="95" y="75" text-anchor="middle" fill="var(--diagram-text)" font-size="12">입력 임베딩</text>

      <rect x="195" y="40" width="130" height="60" rx="6"
            fill="var(--bg-secondary)" stroke="var(--border-color)" stroke-width="1.5"/>
      <text x="260" y="68" text-anchor="middle" fill="var(--diagram-text)" font-size="12">Self-</text>
      <text x="260" y="84" text-anchor="middle" fill="var(--diagram-text)" font-size="12">Attention</text>

      <rect x="360" y="40" width="130" height="60" rx="6"
            fill="var(--bg-secondary)" stroke="var(--border-color)" stroke-width="1.5"/>
      <text x="425" y="68" text-anchor="middle" fill="var(--diagram-text)" font-size="12">Feed-</text>
      <text x="425" y="84" text-anchor="middle" fill="var(--diagram-text)" font-size="12">Forward</text>

      <rect x="525" y="40" width="130" height="60" rx="6"
            fill="var(--bg-secondary)" stroke="var(--border-color)" stroke-width="1.5"/>
      <text x="590" y="75" text-anchor="middle" fill="var(--diagram-text)" font-size="12">출력</text>

      <line x1="160" y1="70" x2="195" y2="70" stroke="var(--diagram-arrow)" stroke-width="2"
            marker-end="url(#llm-theory-2-arrow)"/>
      <line x1="325" y1="70" x2="360" y2="70" stroke="var(--diagram-arrow)" stroke-width="2"
            marker-end="url(#llm-theory-2-arrow)"/>
      <line x1="490" y1="70" x2="525" y2="70" stroke="var(--diagram-arrow)" stroke-width="2"
            marker-end="url(#llm-theory-2-arrow)"/>

      <text x="95" y="125" text-anchor="middle" fill="var(--diagram-text)" font-size="10" opacity="0.7">+ Position</text>
      <text x="260" y="125" text-anchor="middle" fill="var(--diagram-text)" font-size="10" opacity="0.7">+ Residual</text>
      <text x="425" y="125" text-anchor="middle" fill="var(--diagram-text)" font-size="10" opacity="0.7">+ LayerNorm</text>

      <text x="350" y="175" text-anchor="middle" fill="var(--diagram-text)" font-size="11" opacity="0.6">× N layers (반복)</text>
    </svg>
    <p class="diagram-caption">Transformer 블록의 기본 흐름 (임베딩 → Attention → FFN → 출력, N층 반복)</p>
  </div>

  <h3 id="layer-normalization">Layer Normalization: 학습 안정화의 핵심</h3>
  <p><strong>Layer Normalization</strong>은 각 샘플 내에서 활성화 값들을 정규화합니다. Batch Normalization과 달리 배치 크기에 의존하지 않아 Online 학습이나 순환 신경망에서 유리합니다.</p>
  <pre><code><span class="cmt"># Layer Normalization 공식</span>
<span class="fn">LayerNorm</span>(x) = γ × (x - μ) / σ + β

<span class="cmt"># μ (mu): 평균</span>
μ = (<span class="num">1</span>/d) × Σ x_i

<span class="cmt"># σ (sigma): 표준편차</span>
σ = √[(<span class="num">1</span>/d) × Σ (x_i - μ)²]

<span class="cmt"># γ (gamma): 학습 가능한 스케일 파라미터</span>
<span class="cmt"># β (beta): 학습 가능한 이동 파라미터</span>

<span class="cmt"># 구체적인 계산 예시 (단순화)</span>
x = [<span class="num">2.0</span>, <span class="num">4.0</span>, <span class="num">6.0</span>]

<span class="cmt"># 평균</span>
μ = (<span class="num">2+4+6</span>) / <span class="num">3</span> = <span class="num">4.0</span>

<span class="cmt"># 표준편차</span>
σ = √[((<span class="num">2-4</span>)² + (<span class="num">4-4</span>)² + (<span class="num">6-4</span>)²) / <span class="num">3</span>]
  = √[<span class="num">8</span>/<span class="num">3</span>] ≈ <span class="num">1.63</span>

<span class="cmt"># 정규화 (γ=1, β=0 가정)</span>
(x - μ) / σ = [<span class="num">-1.23</span>, <span class="num">0</span>, <span class="num">1.23</span>]</code></pre>

  <div class="diagram-container">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 700 220"
         style="width:100%;max-width:700px;display:block;margin:1.5em auto;background:var(--diagram-fill);border-radius:8px;padding:20px;">
      <defs>
        <marker id="math-10-arrow" viewBox="0 0 10 7" refX="10" refY="3.5"
                markerWidth="10" markerHeight="7" orient="auto">
          <polygon points="0 0, 10 3.5, 0 7" fill="var(--diagram-arrow)"/>
        </marker>
      </defs>

      <text x="350" y="25" text-anchor="middle" fill="var(--diagram-text)" font-size="13" font-weight="bold">Layer Normalization 연산</text>

      <rect x="40" y="60" width="150" height="80" rx="6"
            fill="var(--bg-secondary)" stroke="var(--border-color)" stroke-width="1.5"/>
      <text x="115" y="90" text-anchor="middle" fill="var(--diagram-text)" font-size="12">입력 벡터</text>
      <text x="115" y="115" text-anchor="middle" fill="var(--text-secondary)" font-size="10">[2.0, 4.0, 6.0]</text>

      <rect x="260" y="60" width="150" height="80" rx="6"
            fill="var(--bg-secondary)" stroke="var(--border-color)" stroke-width="1.5"/>
      <text x="335" y="90" text-anchor="middle" fill="var(--diagram-text)" font-size="12">평균/표준편차 계산</text>
      <text x="335" y="115" text-anchor="middle" fill="var(--text-secondary)" font-size="10">μ=4.0, σ=1.63</text>

      <rect x="480" y="60" width="150" height="80" rx="6"
            fill="var(--bg-secondary)" stroke="var(--accent-primary)" stroke-width="2"/>
      <text x="555" y="90" text-anchor="middle" fill="var(--diagram-text)" font-size="12" font-weight="bold">출력</text>
      <text x="555" y="115" text-anchor="middle" fill="var(--text-secondary)" font-size="10">정규화된 벡터</text>

      <line x1="190" y1="100" x2="260" y2="100" stroke="var(--diagram-arrow)" stroke-width="2"
            marker-end="url(#math-10-arrow)"/>
      <line x1="410" y1="100" x2="480" y2="100" stroke="var(--diagram-arrow)" stroke-width="2"
            marker-end="url(#math-10-arrow)"/>

      <text x="350" y="170" text-anchor="middle" fill="var(--text-secondary)" font-size="10">
        LayerNorm(x) = γ × (x - μ) / σ + β
      </text>
      <text x="350" y="188" text-anchor="middle" fill="var(--text-secondary)" font-size="9">
        γ, β는 학습 가능한 파라미터 (입력 분포를 다시 학습적으로 맞춤)
      </text>
    </svg>
    <p class="diagram-caption">Layer Normalization: 평균 0, 분산 1로 정규화 후 스케일/이동</p>
  </div>

  <div class="info-box info">
    <strong>LayerNorm vs BatchNorm:</strong>
    <ul>
      <li><strong>BatchNorm</strong>: 배치 차원으로 정규화 (배치 크기에 의존) - 추론 시 배치 크기가 달라면 다른 결과</li>
      <li><strong>LayerNorm</strong>: 샘플 내부에서 정규화 (배치 무관) - Transformer의 표준</li>
      <li>LLM에서는 입력 시퀀스 길이가 가변적이고 배치 크기도 다양하므로 LayerNorm이 적합</li>
    </ul>
  </div>

  <h3 id="position-encoding">Position Encoding: 순서의 수학적 표현</h3>
  <p>Transformer는 본질적으로 순열 불변(permutation invariant)합니다. "고양이가 강아지를 물었다"와 "강아지가 고양이를 물었다"는 완전히 다른 의미인데, 위치 정보를 주입하지 않으면 이를 구분할 수 없습니다.</p>
  <pre><span class="cmt"># Sinusoidal Position Encoding 공식</span>
<span class="cmt"># 짝수 차원 (sin):</span>
PE(pos, 2i)   = <span class="fn">sin</span>(pos / 10000^(2i/d_model))

<span class="cmt"># 홀수 차원 (cos):</span>
PE(pos, 2i+1) = <span class="fn">cos</span>(pos / 10000^(2i/d_model))

<span class="cmt"># 매개변수:</span>
<span class="cmt"># pos: 토큰 위치 (0, 1, 2, ...)</span>
<span class="cmt"># i: 차원 인덱스 (0, 1, 2, ...)</span>
<span class="cmt"># d_model: 임베딩 차원</span>

<span class="cmt"># 계산 예시 (d_model=4, pos=0,1)</span>
<span class="cmt"># i=0: 10000^(0/4) = 1</span>
<span class="cmt"># i=1: 10000^(2/4) = 10000^0.5 = 10</span>

PE(<span class="num">0</span>) = [<span class="num">sin(0/1)</span>=0,  <span class="num">cos(0/1)</span>=1,  <span class="num">sin(0/10)</span>=0, <span class="num">cos(0/10)</span>=1]
        = [<span class="num">0</span>, <span class="num">1</span>, <span class="num">0</span>, <span class="num">1</span>]

PE(<span class="num">1</span>) = [<span class="num">sin(1/1)</span>=0.84, <span class="num">cos(1/1)</span>=0.54, <span class="num">sin(1/10)</span>=0.10, <span class="num">cos(1/10)</span>=0.99]
        = [<span class="num">0.84</span>, <span class="num">0.54</span>, <span class="num">0.10</span>, <span class="num">0.99</span>]</pre>

  <div class="diagram-container">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 700 260"
         style="width:100%;max-width:700px;display:block;margin:1.5em auto;background:var(--diagram-fill);border-radius:8px;padding:20px;">
      <defs>
        <marker id="math-11-arrow" viewBox="0 0 10 7" refX="10" refY="3.5"
                markerWidth="10" markerHeight="7" orient="auto">
          <polygon points="0 0, 10 3.5, 0 7" fill="var(--diagram-arrow)"/>
        </marker>
      </defs>

      <text x="350" y="25" text-anchor="middle" fill="var(--diagram-text)" font-size="13" font-weight="bold">Sinusoidal Position Encoding</text>

      <line x1="50" y1="200" x2="450" y2="200" stroke="var(--border-color)" stroke-width="1"/>
      <line x1="50" y1="50" x2="50" y2="200" stroke="var(--border-color)" stroke-width="1"/>

      <path d="M 50,200 Q 150,50 250,120 Q 350,190 450,100" fill="none" stroke="var(--accent-primary)" stroke-width="2"/>
      <text x="250" y="230" text-anchor="middle" fill="var(--accent-primary)" font-size="10">sin(pos / 10000^(2i/d))</text>

      <line x1="480" y1="50" x2="650" y2="50" stroke="var(--border-color)" stroke-width="1"/>
      <line x1="480" y1="50" x2="480" y2="200" stroke="var(--border-color)" stroke-width="1"/>

      <path d="M 480,50 Q 530,100 580,150 Q 630,190 650,180" fill="none" stroke="var(--diagram-arrow)" stroke-width="2"/>
      <text x="565" y="230" text-anchor="middle" fill="var(--diagram-arrow)" font-size="10">cos(pos / 10000^(2i/d))</text>

      <rect x="80" y="60" width="100" height="30" rx="4" fill="var(--bg-secondary)" stroke="var(--border-color)"/>
      <text x="130" y="80" text-anchor="middle" fill="var(--diagram-text)" font-size="9">토큰 0</text>

      <rect x="200" y="80" width="100" height="30" rx="4" fill="var(--bg-secondary)" stroke="var(--border-color)"/>
      <text x="250" y="100" text-anchor="middle" fill="var(--diagram-text)" font-size="9">토큰 1</text>

      <rect x="320" y="120" width="100" height="30" rx="4" fill="var(--bg-secondary)" stroke="var(--border-color)"/>
      <text x="370" y="140" text-anchor="middle" fill="var(--diagram-text)" font-size="9">토큰 2</text>

      <rect x="510" y="60" width="120" height="30" rx="4" fill="var(--bg-secondary)" stroke="var(--accent-primary)" stroke-width="1.5"/>
      <text x="570" y="80" text-anchor="middle" fill="var(--diagram-text)" font-size="9">위치 인코딩 추가</text>

      <line x1="130" y1="90" x2="510" y2="75" stroke="var(--diagram-arrow)" stroke-width="1" stroke-dasharray="4,4"/>
      <line x1="250" y1="110" x2="510" y2="75" stroke="var(--diagram-arrow)" stroke-width="1" stroke-dasharray="4,4"/>

      <text x="570" y="110" text-anchor="middle" fill="var(--text-secondary)" font-size="9">임베딩 + PE</text>
      <text x="570" y="125" text-anchor="middle" fill="var(--text-secondary)" font-size="8">= 입력으로</text>
    </svg>
    <p class="diagram-caption">각 위치마다 고유한 파장을 가진 Sinusoidal 인코딩을 임베딩에 더함</p>
  </div>

  <div class="info-box tip">
    <strong>Sinusoidal 인코딩의 장점:</strong>
    <ul>
      <li><strong>상대 위치 정보 포착</strong>: sin/cos의周期性으로 인해 인접한 위치가 유사한 인코딩</li>
      <li><strong>외삽 가능</strong>: 학습하지 않은 위치도 Sinusoidal은 어느 정도 일반화</li>
      <li><strong>학습 불필요</strong>: 고정된 함수로 계산되므로 학습 파라미터가 없음</li>
    </ul>
  </div>
</section>

<section class="content-section">
  <h2 id="attention-math">Attention 수학적 직관</h2>
  <pre><code><span class="cmt"># Scaled Dot-Product Attention</span>
<span class="fn">Attention</span>(Q, K, V) = <span class="fn">softmax</span>(QK^T / <span class="fn">sqrt</span>(d)) × V

<span class="cmt"># Q (Query)  : "내가 찾고 싶은 정보"</span>
<span class="cmt"># K (Key)    : "내가 가진 정보의 라벨"</span>
<span class="cmt"># V (Value)  : "실제 정보 내용"</span>
<span class="cmt"># d          : 임베딩 차원 (스케일링용)</span>

<span class="cmt"># Softmax: 점수를 0~1 사이 확률로 변환</span>
<span class="fn">softmax</span>(z_i) = <span class="fn">exp</span>(z_i) / ∑ <span class="fn">exp</span>(z_j)</code></pre>
  <p>Q, K, V는 각각 질의/키/값이며, 유사도가 높은 키에 더 큰 가중치를 부여합니다. 마치 도서관에서 질문(Q)과 가장 관련된 책 제목(K)을 찾고, 그 책의 내용(V)을 가져오는 것과 같습니다.</p>

  <h3 id="attention-example">Attention 계산 단계별 예시</h3>
  <div class="info-box tip">
    <strong>실제 계산 예시:</strong> 3개 토큰("I", "love", "AI")의 Self-Attention을 5단계로 따라가 봅니다.
  </div>

  <pre><code><span class="cmt"># 1단계: 입력 임베딩 (단순화된 2차원 예시)</span>
Q = [[<span class="num">1.0</span>, <span class="num">0.5</span>],   <span class="cmt"># "I"</span>
     [<span class="num">0.8</span>, <span class="num">1.2</span>],   <span class="cmt"># "love"</span>
     [<span class="num">1.5</span>, <span class="num">0.3</span>]]   <span class="cmt"># "AI"</span>

K = Q  <span class="cmt"># Self-Attention에서는 Q=K=V (단순화)</span>
V = Q

<span class="cmt"># 2단계: 유사도 계산 (QK^T = 내적으로 유사도 측정)</span>
QK^T = [[<span class="num">1.25</span>, <span class="num">1.60</span>, <span class="num">1.65</span>],   <span class="cmt"># "I"와 각 토큰의 유사도</span>
        [<span class="num">1.60</span>, <span class="num">2.08</span>, <span class="num">1.56</span>],   <span class="cmt"># "love"와 각 토큰의 유사도</span>
        [<span class="num">1.65</span>, <span class="num">1.56</span>, <span class="num">2.34</span>]]   <span class="cmt"># "AI"와 각 토큰의 유사도</span>

<span class="cmt"># 3단계: 스케일링 (sqrt(d) = sqrt(2) ≈ 1.41)</span>
<span class="cmt"># → 내적 값이 너무 커지면 softmax가 극단적이 됨, 이를 방지</span>
Scaled = QK^T / <span class="num">1.41</span>
       = [[<span class="num">0.89</span>, <span class="num">1.13</span>, <span class="num">1.17</span>],
          [<span class="num">1.13</span>, <span class="num">1.47</span>, <span class="num">1.11</span>],
          [<span class="num">1.17</span>, <span class="num">1.11</span>, <span class="num">1.66</span>]]

<span class="cmt"># 4단계: Softmax (각 행의 합 = 1, 확률 분포로 변환)</span>
Attention_Weights = [[<span class="num">0.31</span>, <span class="num">0.35</span>, <span class="num">0.34</span>],   <span class="cmt"># "I"가 각 토큰에 주목하는 비율</span>
                     [<span class="num">0.30</span>, <span class="num">0.41</span>, <span class="num">0.29</span>],   <span class="cmt"># "love"가 각 토큰에 주목</span>
                     [<span class="num">0.30</span>, <span class="num">0.28</span>, <span class="num">0.42</span>]]   <span class="cmt"># "AI"가 각 토큰에 주목</span>

<span class="cmt"># 5단계: 가중합 (Attention_Weights × V)</span>
Output = [[<span class="num">1.13</span>, <span class="num">0.63</span>],   <span class="cmt"># "I"의 새로운 표현 (주변 문맥 반영)</span>
          [<span class="num">1.08</span>, <span class="num">0.68</span>],   <span class="cmt"># "love"의 새로운 표현</span>
          [<span class="num">1.21</span>, <span class="num">0.57</span>]]   <span class="cmt"># "AI"의 새로운 표현</span>

<span class="cmt"># 결과 해석:</span>
<span class="cmt"># - "love"는 자기 자신(0.41)과 "I"(0.30)에 많이 주목</span>
<span class="cmt"># - "AI"는 자기 자신(0.42)에 가장 많이 주목</span>
<span class="cmt"># - 각 토큰의 출력은 문맥을 반영하여 풍부해짐</span></code></pre>
</section>

<section class="content-section">
  <h2 id="scaling">스케일링 법칙</h2>
  <p>LLM의 성능은 세 가지 축의 상호작용으로 결정됩니다.</p>
  <ul>
    <li><strong>모델 크기(N)</strong>: 파라미터 증가 시 성능 개선 (멱법칙적 관계)</li>
    <li><strong>데이터(D)</strong>: 학습 데이터가 충분하지 않으면 큰 모델도 한계</li>
    <li><strong>컴퓨트(C)</strong>: 일정 수준까지는 스케일이 성능을 주도</li>
  </ul>

  <h3 id="scaling-comparison">모델 크기별 성능 비교</h3>
  <table>
    <thead>
      <tr>
        <th>모델 크기</th>
        <th>파라미터 수</th>
        <th>MMLU 정확도</th>
        <th>학습 비용</th>
        <th>추론 속도</th>
        <th>적합한 용도</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>Small (3B)</td>
        <td>~3B</td>
        <td>60-70%</td>
        <td>$10K-50K</td>
        <td>매우 빠름</td>
        <td>간단한 분류, 요약</td>
      </tr>
      <tr>
        <td>Medium (7B)</td>
        <td>~7B</td>
        <td>70-80%</td>
        <td>$50K-200K</td>
        <td>빠름</td>
        <td>일반 대화, 코딩 보조</td>
      </tr>
      <tr>
        <td>Large (13B)</td>
        <td>~13B</td>
        <td>80-85%</td>
        <td>$200K-500K</td>
        <td>보통</td>
        <td>복잡한 추론, 전문 작업</td>
      </tr>
      <tr>
        <td>Very Large (70B)</td>
        <td>~70B</td>
        <td>85-90%</td>
        <td>$1M-5M</td>
        <td>느림</td>
        <td>고급 추론, 연구</td>
      </tr>
      <tr>
        <td>Frontier (175B+)</td>
        <td>175B+</td>
        <td>90-95%</td>
        <td>$10M+</td>
        <td>매우 느림</td>
        <td>최첨단 연구, 벤치마크</td>
      </tr>
    </tbody>
  </table>

  <div class="info-box info">
    <strong>스케일링 법칙 핵심:</strong>
    <ul>
      <li>모델 크기를 10배 늘리면 성능이 약 5-10% 향상</li>
      <li>데이터가 부족하면 큰 모델도 과적합 발생</li>
      <li>컴퓨트 예산 = 모델 크기 × 데이터 크기 × 학습 시간</li>
      <li>최적 균형점: Chinchilla 법칙 (파라미터:토큰 ≈ 1:20)</li>
    </ul>
  </div>
</section>

<section class="content-section">
  <h2 id="icl">In-Context Learning</h2>
  <p>모델이 추가 학습(가중치 업데이트) 없이도 프롬프트 내 예시를 통해 작업을 수행하는 현상입니다.</p>
  <ul>
    <li><strong>Few-shot</strong>: 예시 2~5개로 학습 효과 — 별도 파인튜닝 불필요</li>
    <li><strong>패턴 추론</strong>: 예시의 규칙을 컨텍스트에서 즉석 추론</li>
    <li><strong>컨텍스트 길이</strong>: 더 긴 컨텍스트는 더 복잡한 패턴 학습 가능</li>
  </ul>

  <h3 id="icl-example">Few-shot 프롬프트 실전 예시</h3>
  <p>감정 분류 태스크에서 Few-shot ICL이 어떻게 동작하는지 살펴봅시다:</p>
  <pre><code><span class="cmt"># Few-shot 감정 분류 프롬프트</span>
<span class="kw">System</span>: <span class="str">"리뷰의 감정을 '긍정', '부정', '중립'으로 분류하세요."</span>

<span class="cmt"># 예시 1 (긍정)</span>
<span class="kw">리뷰</span>: <span class="str">"배송도 빠르고 품질이 정말 좋아요!"</span>
<span class="kw">감정</span>: 긍정

<span class="cmt"># 예시 2 (부정)</span>
<span class="kw">리뷰</span>: <span class="str">"일주일 만에 고장났습니다. 환불 요청합니다."</span>
<span class="kw">감정</span>: 부정

<span class="cmt"># 예시 3 (중립)</span>
<span class="kw">리뷰</span>: <span class="str">"보통이에요. 가격 대비 적당합니다."</span>
<span class="kw">감정</span>: 중립

<span class="cmt"># 실제 질문</span>
<span class="kw">리뷰</span>: <span class="str">"디자인은 예쁜데 기능이 좀 아쉬워요."</span>
<span class="kw">감정</span>:
<span class="cmt"># → 모델이 패턴을 파악하여 "중립" 또는 "부정"으로 분류</span></code></pre>

  <h3 id="shot-comparison">Zero-shot vs Few-shot vs Many-shot</h3>
  <p>제공하는 예시의 수에 따라 ICL의 성격이 달라집니다:</p>
  <ul>
    <li><strong>Zero-shot</strong>: 예시 없이 지시만 제공. 모델의 사전학습 지식에만 의존하며, 간단한 태스크에 적합</li>
    <li><strong>Few-shot (2~5개)</strong>: 소수의 예시로 출력 형식과 기준을 전달. 대부분의 실무 시나리오에서 최적</li>
    <li><strong>Many-shot (10개+)</strong>: 많은 예시로 복잡한 패턴 전달. 정확도는 높아지지만 컨텍스트를 많이 소비</li>
  </ul>
  <div class="info-box tip">
    <strong>실무 팁:</strong> 예시 수를 늘릴 때 수확체감(diminishing returns)이 발생합니다. 보통 3~5개의 다양한 예시가 비용 대비 최적이며, 예시의 <em>품질과 다양성</em>이 수량보다 중요합니다.
  </div>

  <div class="diagram-container">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 700 260"
         style="width:100%;max-width:700px;display:block;margin:1.5em auto;background:var(--diagram-fill);border-radius:8px;padding:20px;">
      <defs>
        <marker id="llm-theory-3-arrow" viewBox="0 0 10 7" refX="10" refY="3.5"
                markerWidth="10" markerHeight="7" orient="auto">
          <polygon points="0 0, 10 3.5, 0 7" fill="var(--diagram-arrow)"/>
        </marker>
      </defs>

      <rect x="40" y="50" width="200" height="60" rx="6"
            fill="var(--bg-secondary)" stroke="var(--border-color)" stroke-width="1.5"/>
      <text x="140" y="85" text-anchor="middle" fill="var(--diagram-text)" font-size="13">예시 프롬프트</text>

      <rect x="290" y="50" width="200" height="60" rx="6"
            fill="var(--bg-secondary)" stroke="var(--border-color)" stroke-width="1.5"/>
      <text x="390" y="85" text-anchor="middle" fill="var(--diagram-text)" font-size="13">패턴 추론</text>

      <rect x="540" y="50" width="140" height="60" rx="6"
            fill="var(--bg-secondary)" stroke="var(--border-color)" stroke-width="1.5"/>
      <text x="610" y="85" text-anchor="middle" fill="var(--diagram-text)" font-size="13">응답 생성</text>

      <line x1="240" y1="80" x2="290" y2="80" stroke="var(--diagram-arrow)" stroke-width="2"
            marker-end="url(#llm-theory-3-arrow)"/>
      <line x1="490" y1="80" x2="540" y2="80" stroke="var(--diagram-arrow)" stroke-width="2"
            marker-end="url(#llm-theory-3-arrow)"/>

      <text x="140" y="135" text-anchor="middle" fill="var(--diagram-text)" font-size="10" opacity="0.7">입력→출력 예시 N개</text>
      <text x="390" y="135" text-anchor="middle" fill="var(--diagram-text)" font-size="10" opacity="0.7">Attention으로 규칙 추출</text>
      <text x="610" y="135" text-anchor="middle" fill="var(--diagram-text)" font-size="10" opacity="0.7">새 입력에 적용</text>
    </svg>
    <p class="diagram-caption">In-Context Learning의 흐름 (예시 → 패턴 추론 → 응답 생성)</p>
  </div>
</section>

<section class="content-section">
  <h2 id="reasoning">추론 이론과 한계</h2>
  <ul>
    <li><strong>체인 구조</strong>: 긴 추론은 단계마다 오류가 누적될 가능성이 있음</li>
    <li><strong>샘플링 전략</strong>: 동일 질문에 여러 번 샘플링하여 다수결로 정확도 개선 (Self-Consistency)</li>
    <li><strong>검증 루프</strong>: 자기검증/도구 검증을 결합하여 추론 신뢰도 향상</li>
  </ul>

  <h3 id="cot">Chain-of-Thought (CoT) 작동 원리</h3>
  <p>CoT는 모델이 최종 답만 출력하는 대신, 중간 추론 과정을 단계별로 생성하도록 유도합니다. 이렇게 하면 각 단계의 출력이 다음 단계의 컨텍스트가 되어, 복잡한 문제를 작은 단위로 분해하는 효과를 얻습니다.</p>
  <pre><code><span class="cmt"># CoT 없이 (직접 응답)</span>
<span class="kw">Q</span>: <span class="str">"가게에 사과 23개가 있었습니다. 8개를 팔고,</span>
<span class="str">    12개를 새로 입고했습니다. 몇 개가 남았나요?"</span>
<span class="kw">A</span>: <span class="str">"27개"</span>

<span class="cmt"># CoT 적용 (단계별 추론)</span>
<span class="kw">Q</span>: <span class="str">"가게에 사과 23개가 있었습니다. 8개를 팔고,</span>
<span class="str">    12개를 새로 입고했습니다. 몇 개가 남았나요?</span>
<span class="str">    단계별로 풀어보세요."</span>
<span class="kw">A</span>: <span class="str">"1단계: 처음 사과 수 = 23개</span>
<span class="str">    2단계: 8개를 팔았으므로 = 23 - 8 = 15개</span>
<span class="str">    3단계: 12개를 입고했으므로 = 15 + 12 = 27개</span>
<span class="str">    답: 27개"</span>

<span class="cmt"># 직접 응답도 맞을 수 있지만, 복잡한 문제에서는</span>
<span class="cmt"># CoT가 정확도를 크게 높임 (특히 다단계 산술, 논리 문제)</span></code></pre>

  <h3 id="reasoning-failures">추론 실패 유형</h3>
  <p>LLM의 추론은 패턴 매칭에 기반하므로, 특정 유형의 논리적 추론에서 체계적인 한계를 보입니다:</p>
  <ul>
    <li><strong>다단계 산술</strong>: 3단계 이상의 연속 계산에서 오류율 급증. 중간 결과를 "기억"하지 못하고 토큰 단위로 생성하기 때문</li>
    <li><strong>반사실적 추론</strong>: "만약 지구의 중력이 2배라면?" 같은 가정적 시나리오에서 학습 데이터의 사실과 혼동</li>
    <li><strong>부정 논리</strong>: "~가 아닌 것은?" 같은 부정 조건이 포함된 문제에서 취약 — 긍정 패턴에 편향된 학습 때문</li>
    <li><strong>형식적 논리</strong>: 삼단논법, 전건부정 등 엄밀한 형식 논리에서 간헐적 실패</li>
  </ul>

  <h3 id="self-consistency">Self-Consistency: 다중 샘플링 기법</h3>
  <p>Self-Consistency는 같은 문제에 대해 여러 번 독립적으로 추론한 뒤, 가장 빈번한 답을 최종 답으로 선택하는 기법입니다.</p>
  <pre><code><span class="cmt"># Self-Consistency 예시</span>
<span class="cmt"># 같은 질문을 temperature=0.7로 5회 샘플링</span>

<span class="kw">샘플 1</span>: ... → 답: <span class="num">27</span>
<span class="kw">샘플 2</span>: ... → 답: <span class="num">27</span>
<span class="kw">샘플 3</span>: ... → 답: <span class="num">25</span>  <span class="cmt"># 중간 계산 오류</span>
<span class="kw">샘플 4</span>: ... → 답: <span class="num">27</span>
<span class="kw">샘플 5</span>: ... → 답: <span class="num">27</span>

<span class="cmt"># 다수결: 27이 4/5 → 최종 답 = 27 ✓</span>
<span class="cmt"># 단일 샘플링보다 정확도 향상 (특히 추론 문제에서)</span>
<span class="cmt"># 비용: N배의 API 호출 필요</span></code></pre>

  <div class="info-box warning">
    <strong>한계 인식:</strong> CoT와 Self-Consistency도 근본적 한계가 있습니다. 모든 샘플이 같은 방향으로 틀릴 수 있으며(체계적 편향), 학습 데이터에 없는 유형의 논리는 여전히 어렵습니다. 높은 신뢰성이 필요한 경우 도구 호출(계산기, 코드 실행)을 병행하세요.
  </div>
</section>

<!-- ============================================================ -->
<!--  통합 마무리                                                    -->
<!-- ============================================================ -->

<section class="content-section">
  <h2 id="theory-to-practice">이론에서 실전으로</h2>
  <p>위에서 다룬 이론들이 실무에서 어떻게 연결되는지 정리합니다.</p>

  <h3 id="practice-basics">기초 이론 → 실전</h3>
  <ul>
    <li><strong>확률적 생성</strong> → 온도/Top-p 제어로 품질 안정화</li>
    <li><strong>크로스엔트로피</strong> → 모델 비교 시 퍼플렉서티 활용</li>
    <li><strong>환각</strong> → RAG/도구 호출로 사실 검증</li>
  </ul>

  <h3 id="practice-advanced">심화 이론 → 실전</h3>
  <ul>
    <li><strong>Attention</strong> → 긴 컨텍스트에서 중요한 정보 위치 강조 (시스템 프롬프트 설계)</li>
    <li><strong>ICL</strong> → Few-shot 예시로 빠른 태스크 적응 (별도 파인튜닝 없이)</li>
    <li><strong>스케일링</strong> → 비용 대비 성능 목표 설정 (모델 선택 기준)</li>
  </ul>

  <div class="info-box info">
    <strong>실전 연결:</strong> 이론 이해는 프롬프트 설계, 모델 선택, 평가 기준 설정으로 직접 이어집니다. 이론은 모델 선택과 프롬프트 전략의 근거가 됩니다.
  </div>
</section>

<section class="content-section">
  <h2 id="case-study">케이스 스터디</h2>

  <h3 id="case-study-summary">사례 1: 요약 품질 개선 — 온도 조절</h3>
  <ol>
    <li>퍼플렉서티가 낮은 모델을 우선 후보로 선정</li>
    <li>온도 값을 낮추어 일관성을 강화</li>
    <li>RAG로 근거 문장을 함께 제공</li>
  </ol>
  <pre><code><span class="cmt"># 온도 조절 전/후 요약 품질 비교</span>
<span class="kw">원문</span>: <span class="str">"2024년 한국의 반도체 수출이 전년 대비 30% 증가하며</span>
<span class="str">       무역수지 흑자 전환에 기여했다."</span>

<span class="cmt"># Temperature=1.0 (높음) — 불안정한 변동</span>
<span class="kw">시도 1</span>: <span class="str">"한국 반도체 수출이 폭발적으로 성장했다"</span>  <span class="cmt"># "폭발적" 과장</span>
<span class="kw">시도 2</span>: <span class="str">"반도체 산업이 경제 회복을 이끌었다"</span>     <span class="cmt"># 원문에 없는 내용</span>
<span class="kw">시도 3</span>: <span class="str">"수출 30% 증가로 무역수지가 개선되었다"</span>  <span class="cmt"># 적절</span>

<span class="cmt"># Temperature=0.3 (낮음) — 안정적인 출력</span>
<span class="kw">시도 1</span>: <span class="str">"반도체 수출이 30% 증가하며 무역 흑자에 기여"</span>  ✓
<span class="kw">시도 2</span>: <span class="str">"반도체 수출 30% 증가, 무역수지 흑자 전환"</span>    ✓
<span class="kw">시도 3</span>: <span class="str">"반도체 수출이 30% 늘어 무역 흑자를 달성"</span>    ✓</code></pre>
  <div class="info-box tip">
    <strong>결과:</strong> 온도를 0.3으로 낮추면 환각 빈도가 줄고 요약의 일관성이 크게 향상됩니다. 사실 기반 요약에서는 낮은 온도가 거의 항상 유리합니다.
  </div>

  <h3 id="case-study-icl">사례 2: ICL 성능 개선 — Few-shot 수 조절</h3>
  <ol>
    <li>입력 예시를 2개에서 4개로 늘려 패턴 노출 강화</li>
    <li>예시 형식을 출력 형식과 동일하게 맞춤</li>
    <li>컨텍스트 길이 한계 내에서 불필요한 텍스트 제거</li>
  </ol>
  <table>
    <thead>
      <tr>
        <th>예시 수</th>
        <th>형식 일관성</th>
        <th>분류 정확도</th>
        <th>비고</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>0개 (Zero-shot)</td>
        <td>낮음</td>
        <td>~65%</td>
        <td>출력 형식이 불안정</td>
      </tr>
      <tr>
        <td>1개</td>
        <td>중간</td>
        <td>~75%</td>
        <td>형식은 학습하지만 편향 위험</td>
      </tr>
      <tr>
        <td>3개</td>
        <td>높음</td>
        <td>~85%</td>
        <td>비용 대비 최적의 균형점</td>
      </tr>
      <tr>
        <td>5개</td>
        <td>높음</td>
        <td>~88%</td>
        <td>수확체감 시작</td>
      </tr>
      <tr>
        <td>10개+</td>
        <td>매우 높음</td>
        <td>~90%</td>
        <td>컨텍스트 소비 대비 향상 미미</td>
      </tr>
    </tbody>
  </table>
  <div class="info-box tip">
    <strong>결과:</strong> 3~5개의 다양한 예시가 가장 효율적입니다. 10개 이상은 정확도 향상이 미미한 반면 컨텍스트 비용이 크게 증가합니다.
  </div>
</section>

<section class="content-section">
  <h2 id="references">참고자료</h2>
  <ul>
    <li><a href="llm-handbook-concepts.html">LLM 핸드북: 개념과 구조</a></li>
    <li><a href="llm-handbook-training.html">LLM 핸드북: 학습·정렬·추론</a></li>
    <li><a href="prompt-basics.html">프롬프트 기본</a></li>
    <li><a href="microgpt.html">GPT를 밑바닥부터: microgpt.py</a></li>
  </ul>
</section>

<!-- Page Navigation (이전/다음) -->
<div class="page-nav"></div>

</main>

<aside class="inline-toc">
  <div class="toc-title">목차</div>
  <div class="toc-nav"></div>
</aside>

<!-- ===== Footer ===== -->
<footer class="site-footer">
</footer>

</div>

<script src="../js/main.js"></script>
</body>
</html>
