<!DOCTYPE html>
<html lang="ko" data-theme="dark-kernel">
<head>
<!-- BEGIN: Google adsense -->
<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-2110881342960271" crossorigin="anonymous"></script>
<!-- END: Google adsense -->
<!-- BEGIN: Google adsense repair -->
<script async src="https://fundingchoicesmessages.google.com/i/pub-2110881342960271?ers=1" nonce="laI6FT8gpRxugDJv5AGJRA"></script><script nonce="laI6FT8gpRxugDJv5AGJRA">(function() {function signalGooglefcPresent() {if (!window.frames['googlefcPresent']) {if (document.body) {const iframe = document.createElement('iframe'); iframe.style = 'width: 0; height: 0; border: none; z-index: -1000; left: -1000px; top: -1000px;'; iframe.style.display = 'none'; iframe.name = 'googlefcPresent'; document.body.appendChild(iframe);} else {setTimeout(signalGooglefcPresent, 0);}}}signalGooglefcPresent();})();</script>
<!-- END: Google adsense repair -->
<!-- BEGIN: Google analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-1VWQF060SX"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-1VWQF060SX');
</script>
<!-- END: Google analytics -->

<!-- Flash 방지: 쿠키에서 테마 즉시 적용 -->
<script>
(function(){var m=document.cookie.match(/claude_theme=([^;]+)/);if(m)document.documentElement.setAttribute('data-theme',m[1]);})();
</script>
<meta charset="UTF-8">
<meta property="og:type" content="article">
<meta property="og:site_name" content="AI Vibe Coding 가이드 /with MINZKN">
<meta property="og:title" content="GPT를 밑바닥부터 (2): 아키텍처·어텐션·파이프라인 — microgpt.py 해설">
<meta property="og:description" content="microgpt.py의 모델 설계도, 임베딩, Multi-head Attention, 트랜스포머 블록, 완전한 GPT 파이프라인을 해설합니다.">
<meta property="og:url" content="https://minzkn.com/claude/pages/microgpt-architecture.html">
<meta property="og:image" content="https://minzkn.com/claude/images/og-image.png">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="microgpt.py의 모델 설계도, 임베딩, Multi-head Attention, 트랜스포머 블록, 완전한 GPT 파이프라인을 해설합니다.">
<meta name="keywords" content="microgpt gpt 아키텍처 attention transformer 임베딩 rmsnorm mlp kv캐시 gpt2 weight-tying">
<meta name="author" content="MINZKN">
<title>GPT를 밑바닥부터 (2): 아키텍처·어텐션·파이프라인 - AI Vibe Coding 가이드 /with MINZKN</title>
<link rel="icon" type="image/svg+xml" href="../images/favicon.svg">
<link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/joungkyun/font-d2coding/d2coding.css">
<link rel="stylesheet" href="../css/themes.css">
<link rel="stylesheet" href="../css/style.css">
<link rel="stylesheet" href="../css/responsive.css">
</head>
<body>
<div class="page-wrapper">

<!-- ===== Header ===== -->
<header class="site-header">
</header>

<!-- ===== Side Navigation ===== -->
<nav class="side-nav" aria-label="사이트 내비게이션">
</nav>

<!-- ===== Main Content ===== -->
<main class="main-content">
<div class="breadcrumb"></div>

<h1 id="top">GPT를 밑바닥부터 (2): 아키텍처·어텐션·파이프라인</h1>
<p class="page-description">microgpt.py의 모델 설계도(하이퍼파라미터), 토큰/위치 임베딩, Multi-head Attention, 트랜스포머 블록, 그리고 완전한 GPT 파이프라인을 단계별로 해설합니다.</p>

<section class="content-section">
  <h2 id="series">시리즈 안내</h2>
  <ul>
    <li><a href="microgpt-intro.html">1부: 저자 소개, 전체 흐름, 데이터 파이프라인, 토크나이저, Autograd 엔진</a></li>
    <li><strong>2부 (현재)</strong>: 모델 설계도, 임베딩, Attention, 트랜스포머 블록, GPT 파이프라인</li>
    <li><a href="microgpt-training.html">3부: 손실 계산, Adam 옵티마이저, 추론, LLM 비교, 요약</a></li>
  </ul>
</section>

<section class="content-section">
  <h2 id="blueprint">모델 설계도 (Model Blueprint)</h2>
  <p>microgpt의 하이퍼파라미터를 정의합니다. 이 숫자들이 모델의 크기와 능력을 결정합니다.</p>

  <div class="info-box tip">
    <strong>GPT-2 아키텍처 따르기:</strong> "Follow GPT-2, blessed among the GPTs, with minor differences:
    layernorm → rmsnorm, no biases, GeLU → ReLU"
  </div>

  <pre><code><span class="cmt"># Model Blueprint</span>
n_embd = <span class="num">16</span>      <span class="cmt"># 임베딩 차원 (각 토큰을 16차원 벡터로 표현)</span>
n_head = <span class="num">4</span>       <span class="cmt"># 어텐션 헤드 수 (16 / 4 = 헤드당 4차원)</span>
n_layer = <span class="num">1</span>      <span class="cmt"># 트랜스포머 블록 수</span>
block_size = <span class="num">16</span>  <span class="cmt"># 최대 시퀀스 길이 (컨텍스트 윈도우)</span>
head_dim = n_embd // n_head  <span class="cmt"># = 4</span></code></pre>

  <table>
    <thead><tr><th>파라미터</th><th>값</th><th>선택 이유</th><th>더 크게 하면?</th></tr></thead>
    <tbody>
      <tr><td>n_embd</td><td>16</td><td>27개 토큰을 표현하기에 충분</td><td>표현력↑ 속도↓ 메모리↑</td></tr>
      <tr><td>n_head</td><td>4</td><td>head_dim = 16/4 = 4 (정수 분할)</td><td>다양한 어텐션 패턴 가능↑</td></tr>
      <tr><td>n_layer</td><td>1</td><td>단순성 최대화, 이름 생성엔 충분</td><td>더 깊은 추론 가능↑</td></tr>
      <tr><td>block_size</td><td>16</td><td>최장 이름이 15글자 + BOS</td><td>더 긴 시퀀스 처리 가능</td></tr>
      <tr><td>head_dim</td><td>4</td><td>= n_embd / n_head (파생값)</td><td>어텐션 해상도↑</td></tr>
    </tbody>
  </table>

  <div class="info-box info">
    <strong>block_size=16의 근거:</strong> names.txt에서 가장 긴 이름이 15글자입니다.
    BOS + 15글자 = 16 위치면 충분합니다. 16이 넘는 이름은
    <code>n = min(block_size, len(tokens) - 1)</code>으로 잘립니다.
  </div>

  <h3 id="params-count">파라미터 계산 — 4,192개의 숫자</h3>
  <table>
    <thead><tr><th>레이어</th><th>Shape</th><th>파라미터 수</th></tr></thead>
    <tbody>
      <tr><td>wte (토큰 임베딩)</td><td>27 × 16</td><td>432</td></tr>
      <tr><td>wpe (위치 임베딩)</td><td>16 × 16</td><td>256</td></tr>
      <tr><td>lm_head (언임베딩)</td><td>27 × 16</td><td>432 ※</td></tr>
      <tr><td>attn_wq (Query)</td><td>16 × 16</td><td>256</td></tr>
      <tr><td>attn_wk (Key)</td><td>16 × 16</td><td>256</td></tr>
      <tr><td>attn_wv (Value)</td><td>16 × 16</td><td>256</td></tr>
      <tr><td>attn_wo (출력 프로젝션)</td><td>16 × 16</td><td>256</td></tr>
      <tr><td>mlp_fc1 (확장)</td><td>64 × 16</td><td>1,024</td></tr>
      <tr><td>mlp_fc2 (축소)</td><td>16 × 64</td><td>1,024</td></tr>
    </tbody>
    <tfoot>
      <tr><th>총합</th><td>—</td><th>4,192</th></tr>
    </tfoot>
  </table>

  <div class="info-box info">
    <strong>※ Weight Tying (가중치 공유):</strong> 실제 코드에서 <code>wte</code>와 <code>lm_head</code>는
    각각 별도로 초기화되어 있지만, 개념적으로 공유 가능합니다.
    nanoGPT에서는 명시적으로 <code>lm_head.weight = transformer.wte.weight</code>로 공유하여
    432개 파라미터를 절약합니다. 이 아이디어는 Press &amp; Wolf (2016)에서 제안된 것으로,
    입력 임베딩과 출력 임베딩이 같은 공간에 있다는 직관에 기반합니다.<br>
    <strong>RMSNorm 파라미터 없음:</strong> microgpt의 <code>rmsnorm(x)</code>는 학습 가능한 γ(scale) 파라미터 없이
    정규화만 수행합니다. LLaMA 등 프로덕션 모델의 RMSNorm과의 차이입니다.
  </div>

  <h3 id="state-dict">state_dict — 가중치 초기화</h3>
  <pre><code><span class="cmt"># N(0, 0.08²) 가우시안으로 초기화된 행렬 생성 함수</span>
matrix = <span class="kw">lambda</span> nout, nin, std=<span class="num">0.08</span>: \
    [[<span class="type">Value</span>(random.<span class="fn">gauss</span>(<span class="num">0</span>, std)) <span class="kw">for</span> _ <span class="kw">in</span> <span class="fn">range</span>(nin)] <span class="kw">for</span> _ <span class="kw">in</span> <span class="fn">range</span>(nout)]

state_dict = {
    <span class="str">'wte'</span>:     <span class="fn">matrix</span>(vocab_size, n_embd),  <span class="cmt"># 27×16 = 432</span>
    <span class="str">'wpe'</span>:     <span class="fn">matrix</span>(block_size, n_embd),  <span class="cmt"># 16×16 = 256</span>
    <span class="str">'lm_head'</span>: <span class="fn">matrix</span>(vocab_size, n_embd),  <span class="cmt"># 27×16 = 432</span>
}
<span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(n_layer):
    state_dict[<span class="str">f'layer{i}.attn_wq'</span>] = <span class="fn">matrix</span>(n_embd, n_embd)    <span class="cmt"># 16×16</span>
    state_dict[<span class="str">f'layer{i}.attn_wk'</span>] = <span class="fn">matrix</span>(n_embd, n_embd)
    state_dict[<span class="str">f'layer{i}.attn_wv'</span>] = <span class="fn">matrix</span>(n_embd, n_embd)
    state_dict[<span class="str">f'layer{i}.attn_wo'</span>] = <span class="fn">matrix</span>(n_embd, n_embd)
    state_dict[<span class="str">f'layer{i}.mlp_fc1'</span>] = <span class="fn">matrix</span>(<span class="num">4</span>*n_embd, n_embd)  <span class="cmt"># 64×16</span>
    state_dict[<span class="str">f'layer{i}.mlp_fc2'</span>] = <span class="fn">matrix</span>(n_embd, <span class="num">4</span>*n_embd)  <span class="cmt"># 16×64</span>

params = [p <span class="kw">for</span> mat <span class="kw">in</span> state_dict.<span class="fn">values</span>() <span class="kw">for</span> row <span class="kw">in</span> mat <span class="kw">for</span> p <span class="kw">in</span> row]
<span class="cmt"># len(params) == 4192</span></code></pre>

  <h3 id="std-rationale">std=0.08 선택 이유 — Xavier vs He vs microgpt</h3>
  <pre><code><span class="cmt"># 표준 초기화 비교 (n_in=16, n_out=16인 경우)</span>

<span class="cmt"># Xavier (Glorot) 초기화: 순전파/역전파 분산 균형</span>
<span class="cmt"># std = sqrt(2 / (n_in + n_out)) = sqrt(2/32) ≈ 0.25</span>

<span class="cmt"># He (Kaiming) 초기화: ReLU 네트워크에 최적화</span>
<span class="cmt"># std = sqrt(2 / n_in) = sqrt(2/16) ≈ 0.35</span>

<span class="cmt"># microgpt 선택: std = 0.08 (더 작게 시작)</span>
<span class="cmt"># 이유: bias 없는 네트워크는 초기값이 너무 크면</span>
<span class="cmt">#       softmax가 포화(saturation)되기 쉬움.</span>
<span class="cmt">#       작은 초기값으로 시작해 부드럽게 학습.</span>
<span class="cmt">#       또한 200줄의 단순 모델에서 안정적 수렴을 위해.</span></code></pre>

  <table>
    <thead><tr><th>초기화 방법</th><th>std 값</th><th>대상 활성화</th><th>단점</th></tr></thead>
    <tbody>
      <tr><td>Xavier</td><td>≈ 0.25</td><td>sigmoid, tanh</td><td>ReLU에는 부적합</td></tr>
      <tr><td>He (Kaiming)</td><td>≈ 0.35</td><td>ReLU 계열</td><td>bias 없으면 초기 포화 위험</td></tr>
      <tr><td>microgpt</td><td>0.08 (고정)</td><td>ReLU</td><td>단순하고 작은 모델에 최적</td></tr>
    </tbody>
  </table>

  <div class="info-box info">
    <strong>왜 0으로 초기화하면 안 되는가?</strong> 모든 파라미터가 0이면 순전파 결과가 전부 0이 되고,
    역전파 시 모든 뉴런이 동일한 기울기를 받아 영원히 0에서 벗어나지 못합니다(대칭성 문제).
    가우시안으로 초기화하면 각 파라미터가 다른 값을 가져 각자 다른 특징을 학습합니다.
  </div>
</section>

<section class="content-section">
  <h2 id="embedding">임베딩 &amp; 핵심 헬퍼 함수</h2>

  <h3 id="output-layer">출력 레이어 구조 — lm_head</h3>
  <p>최종 출력은 두 단계입니다: <code>lm_head</code> linear 변환 후 Softmax로 확률로 변환.</p>
  <pre><code><span class="cmt"># hidden state x (16차원) → linear(lm_head) → softmax → 확률</span>
<span class="cmt"># 16차원 → 27개 logit (어휘집 크기)</span>
<span class="cmt"># logit → 합이 1인 확률 분포</span>

logits = <span class="fn">linear</span>(x, state_dict[<span class="str">'lm_head'</span>])  <span class="cmt"># 16 → 27</span>
probs  = <span class="fn">softmax</span>(logits)                   <span class="cmt"># 27개 확률, 합=1</span>

<span class="cmt"># Softmax 강화 효과:</span>
<span class="cmt"># z = [10, 12, 8] → Softmax(z) ≈ [12%, 87%, 2%]</span>
<span class="cmt"># 입력 차이가 작아도 Softmax는 승자를 확실히 가림</span>
<span class="cmt"># 10과 12의 차이는 작지만, 87%라는 압도적 확률로 12가 선택됨</span></code></pre>

  <h3 id="helper-funcs">미니 PyTorch — 3가지 헬퍼 함수</h3>
  <pre><code><span class="cmt"># ① linear: 행렬-벡터 곱셈 (bias 없음)</span>
<span class="cmt"># x: [n_in]  →  return: [n_out]</span>
<span class="kw">def</span> <span class="fn">linear</span>(x, w):
    <span class="kw">return</span> [<span class="fn">sum</span>(wi * xi <span class="kw">for</span> wi, xi <span class="kw">in</span> <span class="fn">zip</span>(wo, x)) <span class="kw">for</span> wo <span class="kw">in</span> w]

<span class="cmt"># ② softmax: 로짓 → 확률 분포 (수치 안정성을 위해 max 차감)</span>
<span class="kw">def</span> <span class="fn">softmax</span>(logits):
    max_val = <span class="fn">max</span>(val.data <span class="kw">for</span> val <span class="kw">in</span> logits)   <span class="cmt"># overflow 방지</span>
    exps = [(val - max_val).<span class="fn">exp</span>() <span class="kw">for</span> val <span class="kw">in</span> logits]
    total = <span class="fn">sum</span>(exps)
    <span class="kw">return</span> [e / total <span class="kw">for</span> e <span class="kw">in</span> exps]

<span class="cmt"># ③ rmsnorm: 평균 없이 RMS로만 정규화 (학습 가능 파라미터 없음)</span>
<span class="kw">def</span> <span class="fn">rmsnorm</span>(x):
    ms = <span class="fn">sum</span>(xi * xi <span class="kw">for</span> xi <span class="kw">in</span> x) / <span class="fn">len</span>(x)    <span class="cmt"># 평균 제곱</span>
    scale = (ms + <span class="num">1e-5</span>) ** -<span class="num">0.5</span>               <span class="cmt"># 역수 RMS</span>
    <span class="kw">return</span> [xi * scale <span class="kw">for</span> xi <span class="kw">in</span> x]</code></pre>

  <dl>
    <dt><code>linear()</code> — Wx (bias 없음)</dt>
    <dd>행·열 내적으로 차원 변환. PyTorch의 <code>nn.Linear(bias=False)</code>에 해당.</dd>
    <dt><code>softmax()</code> — e<sup>x</sup> / Σe<sup>x</sup></dt>
    <dd>로짓을 확률로 변환. <code>max_val</code> 차감은 수치 안정성을 위한 필수 트릭 — <code>e^(큰 수)</code>의 overflow를 방지합니다.</dd>
    <dt><code>rmsnorm()</code> — x / √(mean(x²) + ε)</dt>
    <dd>RMS로만 정규화. <strong>학습 가능한 γ(scale) 파라미터가 없습니다.</strong>
    LLaMA 등 프로덕션 모델의 RMSNorm과 달리, microgpt는 정규화만 수행합니다.
    ε=1e-5로 수치 안정성 확보.</dd>
  </dl>

  <h3 id="token-pos-emb">토큰 + 위치 임베딩</h3>
  <p><strong>wte (토큰 임베딩):</strong> "이 문자가 무엇인가" — 문자의 정체성. 같은 문자는 항상 같은 임베딩 행을 가져옵니다.
  학습 후 모음('a','e','i','o','u')들은 벡터 공간에서 서로 가까워질 것입니다.</p>
  <p><strong>wpe (위치 임베딩):</strong> "이 문자가 몇 번째 위치에 있는가" — 어텐션은 위치를 모르므로(내적 연산은 순서가 없음)
  위치 임베딩을 더함으로써 모델이 "첫 번째 글자", "마지막 글자" 같은 위치 패턴을 학습합니다.</p>

  <pre><code><span class="cmt"># "emma"의 BOS 처리 예시 (token_id=26, pos_id=0)</span>
tok_emb = state_dict[<span class="str">'wte'</span>][<span class="num">26</span>]  <span class="cmt"># BOS 임베딩 → [0.03, -0.07, 0.11, ...] (16차원)</span>
pos_emb = state_dict[<span class="str">'wpe'</span>][<span class="num">0</span>]   <span class="cmt"># 위치 0 임베딩 → [0.05, 0.02, -0.08, ...]</span>
x = [t + p <span class="kw">for</span> t, p <span class="kw">in</span> <span class="fn">zip</span>(tok_emb, pos_emb)]  <span class="cmt"># 원소별 덧셈 → 16차원</span>
x = <span class="fn">rmsnorm</span>(x)  <span class="cmt"># 크기 정규화</span>

<span class="cmt"># 왜 concatenate가 아닌 add인가?</span>
<span class="cmt"># concat → 32차원 → 파라미터 2배 (wq, wk, wv도 32×32 필요)</span>
<span class="cmt"># add   → 16차원 유지, 두 정보를 같은 공간에서 합산, 파라미터 절약</span>
<span class="cmt"># 이론: 두 임베딩이 같은 공간(16차원)에서 "어떤 문자" + "어느 위치" 정보를 합산</span></code></pre>
</section>

<section class="content-section">
  <h2 id="attention">Attention = 내부 검색 엔진</h2>
  <p>어텐션은 <em>"현재 내가 처리 중인 토큰과 관련이 있는 이전 토큰들을 찾아라"</em>는 <strong>내부 검색 엔진</strong>입니다.</p>

  <dl>
    <dt>Query (Q) — 검색어</dt>
    <dd>"나는 어떤 정보를 찾고 있나?" — 현재 처리 중인 토큰이 묻는 질문</dd>

    <dt>Key (K) — 색인</dt>
    <dd>"나는 어떤 정보를 가지고 있나?" — 각 토큰이 제공하는 레이블</dd>

    <dt>Value (V) — 내용물</dt>
    <dd>"매칭 시 실제로 전달할 정보" — 검색 결과</dd>
  </dl>

  <pre><code><span class="cmt"># Q, K, V 행렬을 각각 가중치로 선형 변환</span>
q = <span class="fn">linear</span>(x, state_dict[<span class="str">f'layer{li}.attn_wq'</span>])  <span class="cmt"># 질의: "나는 무엇을 찾나?"</span>
k = <span class="fn">linear</span>(x, state_dict[<span class="str">f'layer{li}.attn_wk'</span>])  <span class="cmt"># 키:   "나는 무엇을 가졌나?"</span>
v = <span class="fn">linear</span>(x, state_dict[<span class="str">f'layer{li}.attn_wv'</span>])  <span class="cmt"># 값:   "매칭 시 무엇을 줄까?"</span></code></pre>

  <h3 id="scaled-dot-product">Scaled Dot-Product — 왜 √head_dim으로 나누나?</h3>
  <pre><code><span class="cmt"># score = Q · K / √head_dim</span>
<span class="cmt"># head_dim = 4인 경우</span>
<span class="cmt"># Q = [0.1, -0.3, 0.5, 0.2]   (현재 토큰의 Query 벡터)</span>
<span class="cmt"># K = [0.4, 0.2, -0.1, 0.3]   (과거 토큰의 Key 벡터)</span>
<span class="cmt"># 내적 = 0.1×0.4 + (-0.3)×0.2 + 0.5×(-0.1) + 0.2×0.3</span>
<span class="cmt">#      = 0.04 - 0.06 - 0.05 + 0.06 = -0.01</span>
<span class="cmt"># 스케일: -0.01 / sqrt(4) = -0.01 / 2 = -0.005</span></code></pre>

  <div class="info-box info">
    <strong>스케일링의 수학적 이유:</strong> head_dim 차원의 벡터를 랜덤 초기화(N(0,1))하면 내적의 분산은 head_dim에 비례합니다.
    head_dim=4이면 분산≈4 (표준편차≈2), head_dim=64이면 분산≈64 (표준편차≈8).
    Softmax에 큰 값이 들어가면 한 토큰에 100% 집중되는 "포화(saturation)"가 발생합니다.
    √head_dim으로 나누면 내적의 분산을 1로 정규화하여 softmax가 균형 있는 가중치를 생성합니다.
  </div>

  <h3 id="multi-head">Multi-head Attention — 왜 여러 헤드가 필요한가?</h3>
  <pre><code><span class="cmt"># head_dim = n_embd // n_head = 16 // 4 = 4</span>
<span class="kw">for</span> h <span class="kw">in</span> <span class="fn">range</span>(n_head):        <span class="cmt"># h = 0, 1, 2, 3</span>
    hs = h * head_dim          <span class="cmt"># 헤드 시작 인덱스: 0, 4, 8, 12</span>
    q_h = q[hs:hs+head_dim]    <span class="cmt"># 각 헤드의 Query 4차원 슬라이스</span>
    k_h = [ki[hs:hs+head_dim] <span class="kw">for</span> ki <span class="kw">in</span> keys[li]]   <span class="cmt"># 각 헤드의 Key [T, 4]</span>
    v_h = [vi[hs:hs+head_dim] <span class="kw">for</span> vi <span class="kw">in</span> values[li]]  <span class="cmt"># 각 헤드의 Value [T, 4]</span></code></pre>

  <p><strong>각 헤드가 서로 다른 패턴을 학습합니다:</strong></p>
  <ul>
    <li>헤드 0: "첫 글자 패턴" — BOS 다음에 어떤 자음이 자주 오는가</li>
    <li>헤드 1: "모음 연속 패턴" — 모음 뒤에 모음이 오는지 자음이 오는지</li>
    <li>헤드 2: "어미 패턴" — 이름 끝부분의 통계적 규칙</li>
    <li>헤드 3: "위치 정보" — 지금 시퀀스의 어느 위치인가</li>
  </ul>
  <div class="info-box tip">
    실제로 어떤 패턴을 학습하는지는 학습 후 어텐션 가중치를 시각화해야 확인할 수 있습니다.
    "각 헤드가 다른 것을 학습한다"는 것이 Multi-head Attention의 핵심 아이디어입니다.
  </div>

  <h3 id="emma-trace">"emma" — 어텐션 동작 단계별 추적 (pos=4)</h3>
  <pre><code><span class="cmt"># 입력 토큰 시퀀스: [BOS(26), e(4), m(12), m(12), a(0)]</span>
<span class="cmt"># pos_id:           [ 0,      1,    2,      3,      4  ]</span>

<span class="cmt"># pos=4, token='a' 처리 시:</span>
<span class="cmt">#   keys[0]   = [K_BOS, K_e, K_m, K_m, K_a]  (5개 누적)</span>
<span class="cmt">#   values[0] = [V_BOS, V_e, V_m, V_m, V_a]</span>
<span class="cmt">#</span>
<span class="cmt">#   헤드 h=0 (4차원):</span>
<span class="cmt">#     Q = q[0:4]      ← 'a'의 Query 벡터</span>
<span class="cmt">#</span>
<span class="cmt">#     scores = [</span>
<span class="cmt">#       dot(Q, K_BOS) / 2,   # BOS와의 연관성</span>
<span class="cmt">#       dot(Q, K_e)   / 2,   # 'e'와의 연관성</span>
<span class="cmt">#       dot(Q, K_m)   / 2,   # 첫 번째 'm'과의 연관성</span>
<span class="cmt">#       dot(Q, K_m)   / 2,   # 두 번째 'm'과의 연관성</span>
<span class="cmt">#       dot(Q, K_a)   / 2,   # 'a' 자신과의 연관성 (현재 위치도 포함!)</span>
<span class="cmt">#     ]</span>
<span class="cmt">#</span>
<span class="cmt">#     weights = softmax(scores)</span>
<span class="cmt">#             = [0.05, 0.20, 0.35, 0.30, 0.10]  ← 가중치 합=1.0</span>
<span class="cmt">#             (두 'm'에 0.65 집중 → 'emma'에서 'mm' 패턴 주목)</span>
<span class="cmt">#</span>
<span class="cmt">#     head_out = 0.05×V_BOS + 0.20×V_e + 0.35×V_m + 0.30×V_m + 0.10×V_a</span>
<span class="cmt">#              = 가중 평균 벡터 (4차원)</span>

<span class="cmt"># 4개 헤드 각각 head_out 4차원 계산 → x_attn.extend(head_out)</span>
<span class="cmt"># 결과: x_attn = [h0의 4차원, h1의 4차원, h2의 4차원, h3의 4차원] = 16차원</span></code></pre>

  <h3 id="causal-masking">인과적 마스킹 (Causal Masking) — 미래를 못 보게 하기</h3>
  <p>GPT는 자기회귀(autoregressive) 모델 — 미래 토큰을 보면 안 됩니다.
  GPT-2 원본에서는 어텐션 행렬에 "-∞" 마스크를 명시적으로 적용합니다.
  microgpt에서는 이것이 <strong>자동으로 처리</strong>됩니다:</p>
  <pre><code><span class="cmt"># 학습 시: pos_id=2를 처리할 때</span>
<span class="cmt"># keys[li]에는 이미 [K_pos0, K_pos1, K_pos2]만 있습니다.</span>
<span class="cmt"># K_pos3, K_pos4는 아직 append되지 않았습니다.</span>
<span class="cmt"># → 명시적 마스킹 없이도 미래 정보를 볼 수 없습니다!</span>
<span class="kw">for</span> pos_id <span class="kw">in</span> <span class="fn">range</span>(n):
    logits = <span class="fn">gpt</span>(token_id, pos_id, keys, values)
    <span class="cmt"># 내부에서 keys[li].append(k)가 실행됨</span>
    <span class="cmt"># 다음 pos_id 처리 시 현재 K도 포함됨</span></code></pre>
  <div class="info-box tip">
    이것이 microgpt가 Transformer 원본보다 단순한 이유 중 하나입니다.
    KV 캐시의 순차적 축적이 자동으로 인과적 마스킹을 구현합니다.
  </div>

  <h3 id="kv-cache">KV Cache — 추론 속도의 비밀</h3>
  <pre><code><span class="cmt"># 추론/학습 시작 시 빈 KV 캐시 초기화</span>
keys   = [[] <span class="kw">for</span> _ <span class="kw">in</span> <span class="fn">range</span>(n_layer)]   <span class="cmt"># 레이어별 Key 리스트</span>
values = [[] <span class="kw">for</span> _ <span class="kw">in</span> <span class="fn">range</span>(n_layer)]   <span class="cmt"># 레이어별 Value 리스트</span>

<span class="cmt"># gpt() 내부: 이번 토큰의 K,V를 캐시에 누적</span>
keys[li].<span class="fn">append</span>(k)    <span class="cmt"># k = linear(x, attn_wk), 모든 헤드 포함 [n_embd]</span>
values[li].<span class="fn">append</span>(v)  <span class="cmt"># v = linear(x, attn_wv)</span>

<span class="cmt"># 어텐션 계산: 과거 모든 K,V를 헤드별로 분리해서 사용</span>
k_h = [ki[hs:hs+head_dim] <span class="kw">for</span> ki <span class="kw">in</span> keys[li]]    <span class="cmt"># T개 과거 Key 헤드</span>
v_h = [vi[hs:hs+head_dim] <span class="kw">for</span> vi <span class="kw">in</span> values[li]]   <span class="cmt"># T개 과거 Value 헤드</span>
attn_logits = [
    <span class="fn">sum</span>(q_h[j] * k_h[t][j] <span class="kw">for</span> j <span class="kw">in</span> <span class="fn">range</span>(head_dim)) / head_dim**<span class="num">0.5</span>
    <span class="kw">for</span> t <span class="kw">in</span> <span class="fn">range</span>(<span class="fn">len</span>(k_h))   <span class="cmt"># 현재까지 누적된 토큰 수만큼</span>
]</code></pre>

  <div class="info-box info">
    <strong>KV Cache의 효과:</strong> 각 추론 스텝마다 이미 계산한 K, V를 다시 계산하지 않고 재사용합니다.
    O(T²) 반복 계산 → O(T) 증가분 계산. GPT-4 같은 대형 모델에서는 이 캐시가 추론 속도를 수십 배 높여줍니다.
  </div>

  <h3 id="residual-connection">잔차 연결 (Residual Connection) — 잊어버리지 않기</h3>
  <pre><code><span class="cmt"># 잔차 연결 패턴 (Attention 블록 예시)</span>
x_residual = x               <span class="cmt"># ① 원본 x 저장</span>
x = <span class="fn">rmsnorm</span>(x)              <span class="cmt"># ② 정규화</span>
x = attention(x)  <span class="cmt"># ...</span>   <span class="cmt"># ③ Attention 처리</span>
x = [a + b <span class="kw">for</span> a, b <span class="kw">in</span> <span class="fn">zip</span>(x, x_residual)]  <span class="cmt"># ④ 원본 + 결과</span></code></pre>

  <p><strong>잔차 연결을 빠뜨리면?</strong></p>
  <p>모델이 <em>"나는 B라는 글자를 읽고 있었지"</em>라는 사실을 잊어버리고,
  <em>"나는 A와 관련이 있어"</em>라는 어텐션 출력만 기억하게 됩니다.
  우리는 원본 정보(Content)와 새로운 컨텍스트(Context) 둘 다 필요합니다.<br>
  <code>x = x + attention_output</code> — 이 한 줄이 핵심입니다.</p>

  <p><strong>잔차 연결의 또 다른 효과:</strong></p>
  <ul>
    <li><strong>기울기 소실 방지</strong> — 깊은 네트워크에서 역전파 시 기울기가 층마다 곱해져 소실되는 문제를 해결합니다.
    잔차 경로를 통해 기울기가 직접 흐릅니다.</li>
    <li><strong>항등 함수 학습 용이</strong> — 레이어가 아무것도 하지 않아도 되면(정보 변환 불필요), 출력 = 0 + x_residual이 됩니다.</li>
    <li><strong>모든 현대 트랜스포머의 표준</strong> — GPT-2 이후 모든 대형 언어 모델이 채택</li>
  </ul>

  <div class="diagram-container">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 700 200"
         style="width:100%;max-width:700px;display:block;margin:1.5em auto;background:var(--diagram-fill);border-radius:8px;padding:20px;">
      <defs>
        <marker id="microgpt-arch-arrow" viewBox="0 0 10 7" refX="10" refY="3.5"
                markerWidth="10" markerHeight="7" orient="auto">
          <polygon points="0 0, 10 3.5, 0 7" fill="var(--diagram-arrow)"/>
        </marker>
      </defs>
      <!-- Input embedding -->
      <rect x="10" y="75" width="100" height="50" rx="6"
            fill="var(--bg-secondary)" stroke="var(--border-color)" stroke-width="1.5"/>
      <text x="60" y="97" text-anchor="middle" fill="var(--diagram-text)" font-size="11">입력</text>
      <text x="60" y="113" text-anchor="middle" fill="var(--diagram-text)" font-size="11">임베딩</text>
      <!-- Q, K, V -->
      <rect x="160" y="30" width="70" height="40" rx="5"
            fill="var(--bg-secondary)" stroke="var(--accent-primary)" stroke-width="1.5"/>
      <text x="195" y="55" text-anchor="middle" fill="var(--diagram-text)" font-size="12">Q (쿼리)</text>
      <rect x="160" y="80" width="70" height="40" rx="5"
            fill="var(--bg-secondary)" stroke="var(--accent-secondary)" stroke-width="1.5"/>
      <text x="195" y="105" text-anchor="middle" fill="var(--diagram-text)" font-size="12">K (키)</text>
      <rect x="160" y="130" width="70" height="40" rx="5"
            fill="var(--bg-secondary)" stroke="var(--border-color)" stroke-width="1.5"/>
      <text x="195" y="155" text-anchor="middle" fill="var(--diagram-text)" font-size="12">V (값)</text>
      <!-- Scaled dot product -->
      <rect x="290" y="50" width="120" height="50" rx="6"
            fill="var(--bg-secondary)" stroke="var(--border-color)" stroke-width="1.5"/>
      <text x="350" y="73" text-anchor="middle" fill="var(--diagram-text)" font-size="11">Q·Kᵀ / √d</text>
      <text x="350" y="89" text-anchor="middle" fill="var(--diagram-text)" font-size="11">→ softmax</text>
      <!-- Output -->
      <rect x="480" y="75" width="120" height="50" rx="6"
            fill="var(--bg-secondary)" stroke="var(--accent-primary)" stroke-width="1.5"/>
      <text x="540" y="98" text-anchor="middle" fill="var(--diagram-text)" font-size="11">Σ weights</text>
      <text x="540" y="114" text-anchor="middle" fill="var(--diagram-text)" font-size="11">× V → 출력</text>
      <!-- arrows -->
      <line x1="110" y1="100" x2="158" y2="55" stroke="var(--diagram-arrow)" stroke-width="1.5" marker-end="url(#microgpt-arch-arrow)"/>
      <line x1="110" y1="100" x2="158" y2="100" stroke="var(--diagram-arrow)" stroke-width="1.5" marker-end="url(#microgpt-arch-arrow)"/>
      <line x1="110" y1="100" x2="158" y2="148" stroke="var(--diagram-arrow)" stroke-width="1.5" marker-end="url(#microgpt-arch-arrow)"/>
      <line x1="230" y1="50" x2="288" y2="72" stroke="var(--diagram-arrow)" stroke-width="1.5" marker-end="url(#microgpt-arch-arrow)"/>
      <line x1="230" y1="100" x2="288" y2="82" stroke="var(--diagram-arrow)" stroke-width="1.5" marker-end="url(#microgpt-arch-arrow)"/>
      <line x1="410" y1="85" x2="478" y2="90" stroke="var(--diagram-arrow)" stroke-width="1.5" marker-end="url(#microgpt-arch-arrow)"/>
      <line x1="230" y1="150" x2="478" y2="108" stroke="var(--diagram-arrow)" stroke-width="1.5" stroke-dasharray="4,3" marker-end="url(#microgpt-arch-arrow)"/>
    </svg>
    <p class="diagram-caption">Scaled Dot-Product Attention 흐름: Q·Kᵀ/√d → softmax → 가중합 × V</p>
  </div>
</section>

<section class="content-section">
  <h2 id="transformer-block">트랜스포머 블록</h2>
  <p>트랜스포머의 핵심 반복 단위입니다. 각 레이어는 <strong>Attention → MLP</strong> 순서로 처리합니다.</p>
  <p>입력 x → Attention(컨텍스트 수집) → + → MLP(의미 처리) → + → 출력</p>

  <h3 id="mlp">MLP — 비선형성과 4× 확장의 이유</h3>
  <pre><code><span class="cmt"># fc1: 16차원 → 64차원 (4× 확장)</span>
x = <span class="fn">linear</span>(x, state_dict[<span class="str">f'layer{li}.mlp_fc1'</span>])  <span class="cmt"># [n_embd] → [4*n_embd]</span>
x = [xi.<span class="fn">relu</span>() <span class="kw">for</span> xi <span class="kw">in</span> x]                       <span class="cmt"># 비선형성: max(0, x)</span>
<span class="cmt"># fc2: 64차원 → 16차원 (원상복구)</span>
x = <span class="fn">linear</span>(x, state_dict[<span class="str">f'layer{li}.mlp_fc2'</span>])  <span class="cmt"># [4*n_embd] → [n_embd]</span></code></pre>

  <p><strong>왜 4× 확장인가?</strong>
  이 "bottleneck" 구조(좁게 → 넓게 → 좁게)는 GPT-2 원논문에서 정의된 표준입니다.
  중간 64차원에서 더 많은 특징 조합을 만든 뒤, 다시 16차원으로 압축합니다.
  4× 비율은 실험적으로 좋은 성능을 보인 경험적 수치이며, GPT-4도 동일한 비율을 씁니다.</p>

  <div class="info-box info">
    <strong>Attention vs MLP의 역할 구분:</strong>
    Attention은 다른 토큰의 정보를 "수집(Gather)"합니다.
    MLP는 수집된 정보를 "처리(Process)"합니다.
    ReLU 없이 Attention + Linear만 있다면, 아무리 쌓아도 단 하나의 선형 변환과 수학적으로 동일합니다.
    ReLU가 추가하는 비선형성이 복잡한 패턴을 학습 가능하게 합니다.
  </div>

  <h3 id="gelu-relu">GELU → ReLU — 의도적 단순화</h3>
  <dl>
    <dt>GELU (표준, GPT-2/3) — x · Φ(x)</dt>
    <dd>정규분포의 누적분포함수 사용. 수식 복잡 (<code>0.5 × x × (1 + tanh(√(2/π) × (x + 0.044715x³)))</code>).
    확률론적 해석으로 더 부드러운 활성화. 하지만 순수 파이썬으로 구현하면 느리고 복잡합니다.</dd>

    <dt>ReLU (microgpt) — max(0, x)</dt>
    <dd>단순하지만 충분히 효과적. <code>xi.relu()</code> 단 한 줄. 순수 파이썬 구현에 최적.
    이름 생성이라는 단순한 태스크에는 ReLU도 충분합니다.</dd>
  </dl>

  <h3 id="pre-ln">Pre-LN vs Post-LN — microgpt의 선택</h3>
  <table>
    <thead><tr><th>구분</th><th>Post-LN (GPT-2 원본)</th><th>Pre-LN (microgpt)</th></tr></thead>
    <tbody>
      <tr><td>순서</td><td>x → Attention → x+residual → Norm</td><td>x → Norm → Attention → x+residual</td></tr>
      <tr><td>잔차 경로</td><td>정규화된 값이 잔차로 흐름</td><td>원본 x가 그대로 잔차로 흐름 ✓</td></tr>
      <tr><td>학습 안정성</td><td>레이어 수 많으면 불안정</td><td>더 안정적 (LLaMA, PaLM 채택)</td></tr>
      <tr><td>초기화 의존</td><td>높음</td><td>낮음</td></tr>
    </tbody>
  </table>

  <pre><code><span class="cmt"># microgpt의 Pre-LN 패턴:</span>
x_residual = x         <span class="cmt"># ← 정규화 전 원본 저장</span>
x = <span class="fn">rmsnorm</span>(x)         <span class="cmt"># ← 정규화 후 Attention/MLP에 입력</span>
x = attention(x)  <span class="cmt"># ...</span>
x = [a + b <span class="kw">for</span> a, b <span class="kw">in</span> <span class="fn">zip</span>(x, x_residual)]  <span class="cmt"># ← 원본 + 결과</span></code></pre>

  <h3 id="rmsnorm">RMSNorm vs LayerNorm</h3>
  <table>
    <thead><tr><th>항목</th><th>LayerNorm (기존)</th><th>RMSNorm (LLaMA 등)</th><th>RMSNorm (microgpt)</th></tr></thead>
    <tbody>
      <tr><td>수식</td><td>(x − μ) / √(σ² + ε)</td><td>x / √(RMS² + ε)</td><td>x / √(RMS² + ε)</td></tr>
      <tr><td>학습 파라미터</td><td>γ + β (2개)</td><td>γ만 (1개)</td><td><strong>없음</strong> (정규화만)</td></tr>
      <tr><td>계산 비용</td><td>평균·분산 계산</td><td>분산만 계산</td><td>분산만 계산 (γ 없음)</td></tr>
    </tbody>
  </table>
  <p>RMSNorm은 평균 계산이 필요 없어 LayerNorm 대비 <strong>7~64% 더 빠르고</strong>, 성능은 거의 동일합니다.</p>
</section>

<section class="content-section">
  <h2 id="gpt-pipeline">완전한 gpt() 함수 — GPT 파이프라인</h2>
  <pre><code><span class="kw">def</span> <span class="fn">gpt</span>(token_id, pos_id, keys, values):

    <span class="cmt"># ─── ① 임베딩: 토큰 + 위치 합산 ─────────────────────</span>
    tok_emb = state_dict[<span class="str">'wte'</span>][token_id]   <span class="cmt"># 16차원 벡터</span>
    pos_emb = state_dict[<span class="str">'wpe'</span>][pos_id]     <span class="cmt"># 16차원 벡터</span>
    x = [t + p <span class="kw">for</span> t, p <span class="kw">in</span> <span class="fn">zip</span>(tok_emb, pos_emb)]
    x = <span class="fn">rmsnorm</span>(x)  <span class="cmt"># ← 입력 정규화 (잔차 경로 때문에 필요)</span>

    <span class="cmt"># ─── ② 트랜스포머 레이어 × n_layer ──────────────────</span>
    <span class="kw">for</span> li <span class="kw">in</span> <span class="fn">range</span>(n_layer):

        <span class="cmt"># Attention Block: Pre-LN 방식</span>
        x_residual = x
        x = <span class="fn">rmsnorm</span>(x)
        q = <span class="fn">linear</span>(x, state_dict[<span class="str">f'layer{li}.attn_wq'</span>])    <span class="cmt"># Q: [n_embd]</span>
        k = <span class="fn">linear</span>(x, state_dict[<span class="str">f'layer{li}.attn_wk'</span>])    <span class="cmt"># K: [n_embd]</span>
        v = <span class="fn">linear</span>(x, state_dict[<span class="str">f'layer{li}.attn_wv'</span>])    <span class="cmt"># V: [n_embd]</span>
        keys[li].<span class="fn">append</span>(k)    <span class="cmt"># KV 캐시에 현재 토큰 저장</span>
        values[li].<span class="fn">append</span>(v)

        <span class="cmt"># 멀티헤드 어텐션 (헤드별로 분리 계산)</span>
        x_attn = []
        <span class="kw">for</span> h <span class="kw">in</span> <span class="fn">range</span>(n_head):
            hs = h * head_dim
            q_h = q[hs:hs+head_dim]                                   <span class="cmt"># [head_dim]</span>
            k_h = [ki[hs:hs+head_dim] <span class="kw">for</span> ki <span class="kw">in</span> keys[li]]             <span class="cmt"># [T, head_dim]</span>
            v_h = [vi[hs:hs+head_dim] <span class="kw">for</span> vi <span class="kw">in</span> values[li]]           <span class="cmt"># [T, head_dim]</span>
            <span class="cmt"># Scaled Dot-Product: Q·Kᵀ / √head_dim</span>
            attn_logits = [
                <span class="fn">sum</span>(q_h[j] * k_h[t][j] <span class="kw">for</span> j <span class="kw">in</span> <span class="fn">range</span>(head_dim)) / head_dim**<span class="num">0.5</span>
                <span class="kw">for</span> t <span class="kw">in</span> <span class="fn">range</span>(<span class="fn">len</span>(k_h))
            ]
            attn_weights = <span class="fn">softmax</span>(attn_logits)                        <span class="cmt"># [T]</span>
            <span class="cmt"># Weighted sum of Values</span>
            head_out = [
                <span class="fn">sum</span>(attn_weights[t] * v_h[t][j] <span class="kw">for</span> t <span class="kw">in</span> <span class="fn">range</span>(<span class="fn">len</span>(v_h)))
                <span class="kw">for</span> j <span class="kw">in</span> <span class="fn">range</span>(head_dim)
            ]
            x_attn.<span class="fn">extend</span>(head_out)

        <span class="cmt"># 출력 프로젝션 + 잔차 연결</span>
        x = <span class="fn">linear</span>(x_attn, state_dict[<span class="str">f'layer{li}.attn_wo'</span>])
        x = [a + b <span class="kw">for</span> a, b <span class="kw">in</span> <span class="fn">zip</span>(x, x_residual)]   <span class="cmt"># x = attn_out + x_residual</span>

        <span class="cmt"># MLP Block: Pre-LN 방식</span>
        x_residual = x
        x = <span class="fn">rmsnorm</span>(x)
        x = <span class="fn">linear</span>(x, state_dict[<span class="str">f'layer{li}.mlp_fc1'</span>])  <span class="cmt"># 16 → 64</span>
        x = [xi.<span class="fn">relu</span>() <span class="kw">for</span> xi <span class="kw">in</span> x]                      <span class="cmt"># 비선형성</span>
        x = <span class="fn">linear</span>(x, state_dict[<span class="str">f'layer{li}.mlp_fc2'</span>])  <span class="cmt"># 64 → 16</span>
        x = [a + b <span class="kw">for</span> a, b <span class="kw">in</span> <span class="fn">zip</span>(x, x_residual)]         <span class="cmt"># x = mlp_out + x_residual</span>

    <span class="cmt"># ─── ③ 언임베딩: 16차원 → 27개 로짓 ─────────────────</span>
    logits = <span class="fn">linear</span>(x, state_dict[<span class="str">'lm_head'</span>])
    <span class="kw">return</span> logits</code></pre>

  <div class="info-box tip">
    <strong>입력 rmsnorm의 비밀:</strong>
    <code>x = rmsnorm(x)</code>가 임베딩 직후 한 번 더 있습니다.
    이것은 Karpathy가 주석에서 언급한 부분 — "잔차 경로(residual stream) 덕분에 불필요하지 않다."
    잔차 연결은 원본 임베딩을 그대로 더하므로, 첫 번째 rmsnorm이 없으면
    비정규화된 임베딩이 그대로 잔차로 흘러들어갑니다.
    임베딩의 크기가 제각각이면 잔차 경로 전체가 불안정해집니다.
  </div>

  <p>이 함수 하나가 토큰 ID와 위치 ID를 받아 27개 어휘에 대한 확률 분포(로짓)를 반환합니다.</p>

  <h3 id="four-pillars">트랜스포머 4대 구성요소</h3>
  <ol>
    <li><strong>Attention</strong> — 컨텍스트 수집 (다른 토큰의 정보를 Gather)</li>
    <li><strong>MLP</strong> — 의미 처리 (수집된 정보를 Process)</li>
    <li><strong>잔차 연결(Residuals)</strong> — 안정적인 학습 + 기울기 소실 방지</li>
    <li><strong>RMSNorm</strong> — 연산 안정성 확보 (LayerNorm 대비 단순화)</li>
  </ol>
  <div class="info-box tip">
    이 네 가지 구성요소가 현대 모든 트랜스포머 모델(GPT-4, LLaMA, Gemini 등)의 기본 빌딩 블록입니다.
    규모(Scale)만 다를 뿐, 원리는 microgpt와 완전히 동일합니다.
  </div>

  <div class="info-box info">
    <strong>Karpathy의 말:</strong>
    "The most atomic way to train and run inference for a GPT in pure, dependency-free Python.
    <em>This file is the complete algorithm. Everything else is just efficiency.</em>"
  </div>
</section>

<section class="content-section">
  <h2 id="next">다음 파트</h2>
  <ul>
    <li><a href="microgpt-intro.html">1부: 저자 소개, 데이터 파이프라인, 토크나이저, Autograd 엔진</a></li>
    <li><a href="microgpt-training.html">3부: 손실 계산, Adam 옵티마이저, 추론, MicroGPT vs LLM 비교, 요약</a></li>
  </ul>
</section>

<section class="content-section">
  <h2 id="references">참고 자료</h2>

  <h3>원본 소스</h3>
  <ul>
    <li><a href="https://karpathy.github.io/2026/02/12/microgpt/" target="_blank" rel="noopener">Andrej Karpathy — microgpt 블로그 포스트 (2026.02.12)</a> — 저자 직접 작성 공식 가이드</li>
    <li><a href="https://gist.github.com/karpathy/8627fe009c40f57531cb18360106ce95" target="_blank" rel="noopener">microgpt.py 원본 소스 코드 (GitHub Gist)</a> — ~200줄 순수 파이썬 전체 알고리즘</li>
    <li><a href="https://github.com/karpathy" target="_blank" rel="noopener">@karpathy GitHub</a> — 저자의 모든 오픈소스 교육용 AI 프로젝트</li>
    <li><a href="https://x.com/karpathy" target="_blank" rel="noopener">@karpathy X(Twitter)</a> — 최신 연구·강의 업데이트</li>
  </ul>

  <h3>관련 Karpathy 프로젝트</h3>
  <ul>
    <li><a href="https://github.com/karpathy/nanoGPT" target="_blank" rel="noopener">nanoGPT</a> — microgpt 아키텍처의 PyTorch·GPU 실용 확장. <code>model.py</code>가 이 페이지의 <code>gpt()</code> 함수에 해당</li>
    <li><a href="https://github.com/karpathy/nanoGPT/blob/master/model.py" target="_blank" rel="noopener">nanoGPT model.py</a> — <code>CausalSelfAttention</code>·<code>MLP</code>·<code>Block</code>·<code>GPT</code> 클래스 소스</li>
    <li><a href="https://github.com/karpathy/micrograd" target="_blank" rel="noopener">micrograd</a> — <code>Value</code> 클래스 원형 (autograd 엔진)</li>
    <li><a href="https://github.com/karpathy/makemore" target="_blank" rel="noopener">makemore</a> — Transformer 아키텍처를 단계적으로 구축하는 강의 시리즈</li>
    <li><a href="https://github.com/karpathy/llm.c" target="_blank" rel="noopener">llm.c</a> — C/CUDA로만 LLM 훈련. Flash Attention을 직접 CUDA로 구현</li>
  </ul>

  <h3>학습 영상 (Karpathy YouTube)</h3>
  <ul>
    <li><a href="https://www.youtube.com/watch?v=kCc8FmEb1nY" target="_blank" rel="noopener">Let's build GPT: from scratch, in code, spelled out</a> — nanoGPT 라이브 코딩 (2시간). 이 페이지의 아키텍처 전체를 단계별로 구현</li>
    <li><a href="https://www.youtube.com/watch?v=zduSFxRajkE" target="_blank" rel="noopener">Let's build the GPT Tokenizer</a> — BPE 토크나이저 구현 (2시간)</li>
    <li><a href="https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ" target="_blank" rel="noopener">Neural Networks: Zero to Hero (재생목록)</a> — Karpathy 전체 강의 시리즈</li>
  </ul>

  <h3>핵심 논문</h3>
  <ul>
    <li><a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">Attention Is All You Need (Vaswani et al., 2017)</a> — Transformer 원논문. Q·K·V·Scaled Dot-Product·Multi-head 수식 출처</li>
    <li><a href="https://openai.com/research/language-unsupervised" target="_blank" rel="noopener">GPT-2: Language Models are Unsupervised Multitask Learners (OpenAI, 2019)</a> — microgpt가 "follow GPT-2"라고 명시한 기준 아키텍처</li>
    <li><a href="https://arxiv.org/abs/1910.07467" target="_blank" rel="noopener">Root Mean Square Layer Normalization (Zhang &amp; Sennrich, 2019)</a> — microgpt의 <code>rmsnorm</code> 수식 근거</li>
    <li><a href="https://arxiv.org/abs/2205.14135" target="_blank" rel="noopener">FlashAttention (Dao et al., 2022)</a> — nanoGPT가 채택한 메모리 효율 어텐션 (microgpt와 비교 시 참고)</li>
    <li><a href="https://arxiv.org/abs/2002.05202" target="_blank" rel="noopener">Press &amp; Wolf (2016) — Using the Output Embedding to Improve Language Models</a> — Weight Tying 원논문</li>
    <li><a href="https://arxiv.org/abs/1603.05027" target="_blank" rel="noopener">Identity Mappings in Deep Residual Networks (He et al., 2016)</a> — Pre-LN vs Post-LN 잔차 연결 연구</li>
  </ul>

  <h3>관련 vibecoding 문서</h3>
  <ul>
    <li><a href="llm-theory-advanced.html">LLM 이론 심화 (Transformer, Attention 상세)</a></li>
    <li><a href="llm-theory-math-appendix.html">LLM 이론 수학 부록 (어텐션 수식 유도)</a></li>
    <li><a href="llm-theory-foundations.html">LLM 이론 기초</a></li>
    <li><a href="llm-handbook-training.html">LLM 핸드북: 학습·정렬·추론</a></li>
  </ul>
</section>

<!-- Page Navigation (이전/다음) -->
<div class="page-nav"></div>

</main>

<aside class="inline-toc">
  <div class="toc-title">목차</div>
  <div class="toc-nav"></div>
</aside>

<!-- ===== Footer ===== -->
<footer class="site-footer">
</footer>

</div>

<script src="../js/main.js"></script>
</body>
</html>
