<!DOCTYPE html>
<html lang="ko" data-theme="dark-kernel">
<head>
<!-- BEGIN: Google adsense -->
<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-2110881342960271" crossorigin="anonymous"></script>
<!-- END: Google adsense -->
<!-- BEGIN: Google adsense repair -->
<script async src="https://fundingchoicesmessages.google.com/i/pub-2110881342960271?ers=1" nonce="laI6FT8gpRxugDJv5AGJRA"></script><script nonce="laI6FT8gpRxugDJv5AGJRA">(function() {function signalGooglefcPresent() {if (!window.frames['googlefcPresent']) {if (document.body) {const iframe = document.createElement('iframe'); iframe.style = 'width: 0; height: 0; border: none; z-index: -1000; left: -1000px; top: -1000px;'; iframe.style.display = 'none'; iframe.name = 'googlefcPresent'; document.body.appendChild(iframe);} else {setTimeout(signalGooglefcPresent, 0);}}}signalGooglefcPresent();})();</script>
<!-- END: Google adsense repair -->
<!-- BEGIN: Google analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-1VWQF060SX"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-1VWQF060SX');
</script>
<!-- END: Google analytics -->
<script>
(function(){var m=document.cookie.match(/claude_theme=([^;]+)/);if(m)document.documentElement.setAttribute('data-theme',m[1]);})();
</script>
<meta charset="UTF-8">
<meta property="og:type" content="article">
<meta property="og:site_name" content="AI Vibe Coding 가이드 /with MINZKN">
<meta property="og:title" content="GPT를 밑바닥부터: microgpt.py 완전 해설 — 저자·데이터·Autograd·아키텍처·학습·추론">
<meta property="og:description" content="Andrej Karpathy의 microgpt.py — 순수 파이썬 200줄, 외부 ML 라이브러리 0개. 저자·데이터·Autograd·GPT 아키텍처·학습·Adam·추론·현대 LLM 비교까지 한 페이지에서 완전 해설합니다.">
<meta property="og:url" content="https://minzkn.com/claude/pages/microgpt.html">
<meta property="og:image" content="https://minzkn.com/claude/images/og-image.png">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="Andrej Karpathy의 microgpt.py — 순수 파이썬 200줄, 외부 ML 라이브러리 0개. 저자·데이터·Autograd·GPT 아키텍처·학습·Adam·추론·현대 LLM 비교까지 한 페이지에서 완전 해설합니다.">
<meta name="keywords" content="microgpt karpathy gpt 밑바닥 autograd value 역전파 토크나이저 순수파이썬 micrograd 아키텍처 attention transformer 임베딩 rmsnorm mlp kv캐시 gpt2 weight-tying 학습 adam 옵티마이저 크로스엔트로피 추론 temperature llm 비교 nanogpt names.txt bos">
<meta name="author" content="MINZKN">
<title>GPT를 밑바닥부터: microgpt.py 완전 해설 - AI Vibe Coding 가이드 /with MINZKN</title>
<link rel="icon" type="image/svg+xml" href="../images/favicon.svg">
<link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/joungkyun/font-d2coding/d2coding.css">
<link rel="stylesheet" href="../css/themes.css">
<link rel="stylesheet" href="../css/style.css">
<link rel="stylesheet" href="../css/responsive.css">
</head>
<body>
<div class="page-wrapper">
<header class="site-header"></header>
<nav class="side-nav" aria-label="사이트 내비게이션"></nav>
<main class="main-content">
<div class="breadcrumb"></div>

<h1 id="top">GPT를 밑바닥부터: microgpt.py 완전 해설</h1>
<p class="page-description">Andrej Karpathy의 microgpt.py (2026.02) — 순수 파이썬(<code>os</code>, <code>math</code>, <code>random</code>) 200줄, 외부 ML 라이브러리 0개. 저자 소개, 데이터 파이프라인, 토크나이저, Autograd 엔진, GPT 아키텍처, 학습 루프, Adam 옵티마이저, 추론, 현대 LLM 비교까지 한 페이지에서 완전 해설합니다.</p>

<!-- ===================================================== -->
<!-- 1부: 저자·데이터·Autograd                             -->
<!-- ===================================================== -->

<section class="content-section">
  <h2 id="overview">개요</h2>
  <p><strong>microgpt.py</strong>는 Andrej Karpathy가 2026년 2월 공개한 "가장 원자적인 GPT 구현"입니다.
  <code>os</code>, <code>math</code>, <code>random</code> 세 표준 모듈만으로 GPT를 완성합니다.</p>
  <div class="info-box tip">
    <strong>핵심 철학:</strong> "This file contains the full algorithmic content of what is needed:
    dataset · tokenizer · autograd engine · GPT-2-like architecture · Adam optimizer · training loop · inference loop.
    Everything else is just efficiency. I cannot simplify this any further."
    — Andrej Karpathy, 2026.02.12
  </div>
  <table>
    <thead><tr><th>항목</th><th>값</th><th>의미</th></tr></thead>
    <tbody>
      <tr><td>파라미터</td><td><strong>4,192개</strong></td><td>학습 가능한 숫자 전체</td></tr>
      <tr><td>코드 줄 수</td><td>~200줄</td><td>데이터부터 추론까지 한 파일</td></tr>
      <tr><td>학습 문서(이름)</td><td>32,033개</td><td>names.txt 영어 이름 데이터셋</td></tr>
      <tr><td>외부 ML 라이브러리</td><td><strong>0개</strong></td><td>os, math, random만 사용</td></tr>
      <tr><td>학습 결과</td><td>loss 3.30 → 2.37</td><td>Perplexity 27 → 10.7 (약 2.5배 향상)</td></tr>
    </tbody>
  </table>
  <p>이 페이지는 다음 순서로 해설합니다:</p>
  <ul>
    <li><a href="#author">저자 소개</a>, <a href="#flow">전체 코드 흐름</a>, <a href="#data">데이터 파이프라인</a>, <a href="#tokenizer">토크나이저</a>, <a href="#autograd">Autograd 엔진</a></li>
    <li><a href="#blueprint">모델 설계도</a>, <a href="#embedding">임베딩</a>, <a href="#attention">Attention</a>, <a href="#transformer-block">트랜스포머 블록</a>, <a href="#gpt-pipeline">GPT 파이프라인</a></li>
    <li><a href="#loss">손실 계산</a>, <a href="#training-loop">학습 루프</a>, <a href="#adam">Adam 옵티마이저</a>, <a href="#inference">추론</a>, <a href="#comparison">LLM 비교</a></li>
    <li><a href="#numerical-stability">수치 안정성</a>, <a href="#forward-trace">순전파 수치 추적</a>, <a href="#adamw">AdamW &amp; 옵티마이저 변형</a>, <a href="#sampling-strategies">고급 샘플링 전략</a></li>
    <li><a href="#pytorch-migration">PyTorch 마이그레이션</a>, <a href="#debugging">디버깅 가이드</a>, <a href="#performance">성능 분석</a>, <a href="#nanogpt">nanoGPT 비교</a>, <a href="#summary">요약</a></li>
  </ul>
</section>

<section class="content-section">
  <h2 id="author">저자 — Andrej Karpathy</h2>
  <dl>
    <dt>학력</dt>
    <dd><strong>Stanford PhD</strong> — Fei-Fei Li 지도 하에 Computer Vision · Deep Learning 전공.
    박사 논문 주제: "Connecting Images and Natural Language" (이미지-언어 연결).
    재학 중 Google Brain · DeepMind에서 인턴.</dd>

    <dt>커리어</dt>
    <dd>
      <strong>OpenAI 창립 멤버(2015)</strong> — GPT 시리즈 초기 연구에 직접 참여.<br>
      <strong>Tesla AI 디렉터(2017~2022)</strong> — Autopilot의 신경망 인식 시스템 총괄.
      "Data Engine" 개념(모델이 스스로 학습 데이터를 생성)을 실제 자율주행에 적용.<br>
      <strong>OpenAI 복귀(2023)</strong> — AGI 안전성 연구.<br>
      <strong>독립(2024~)</strong> — AI 교육 콘텐츠에 집중.
    </dd>

    <dt>교육 철학</dt>
    <dd><strong>"Everything else is just efficiency"</strong> —
    복잡한 AI를 의존성 없는 가장 원자적 형태로 단순화함으로써, 블랙박스를 투명하게 만드는 교육자.
    "이해하려면 직접 만들어봐야 한다"는 원칙으로 수만 명에게 딥러닝의 문을 열었습니다.</dd>
  </dl>

  <h3 id="lineage">프로젝트 계보 — 10년의 단순화 여정</h3>
  <p>각 프로젝트는 "한 가지를 더 단순하게"라는 목표로 이어집니다.</p>
  <table>
    <thead>
      <tr><th>프로젝트</th><th>연도</th><th>핵심 아이디어</th><th>microgpt와의 관계</th></tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>micrograd</strong></td>
        <td>2020</td>
        <td>스칼라 Autograd — Value 클래스로 역전파 구현</td>
        <td>microgpt <code>Value</code> 클래스의 직접적 원형</td>
      </tr>
      <tr>
        <td><strong>makemore</strong></td>
        <td>2022</td>
        <td>문자 수준 언어 모델, names.txt 데이터셋</td>
        <td>데이터셋·토크나이저 구조 계승</td>
      </tr>
      <tr>
        <td><strong>nanoGPT</strong></td>
        <td>2022</td>
        <td>PyTorch+GPU 기반 실용 GPT 훈련 코드베이스 — GPT-2(124M) 완전 재현, Shakespeare 예제, DDP 분산학습, Flash Attention</td>
        <td>GPT 아키텍처의 "Production 최소 구현" — microgpt가 배운 원리를 실제 규모로 확장</td>
      </tr>
      <tr>
        <td><strong>llm.c</strong></td>
        <td>2024</td>
        <td>C/CUDA로 LLM 훈련 — NumPy·PyTorch 없이</td>
        <td>의존성 제거 철학 계승</td>
      </tr>
      <tr>
        <td><strong>microgpt</strong></td>
        <td>2026</td>
        <td>순수 파이썬 200줄 — 모든 것을 하나의 파일에</td>
        <td>집대성</td>
      </tr>
    </tbody>
  </table>
</section>

<section class="content-section">
  <h2 id="flow">전체 코드 흐름</h2>
  <pre><code>【입력】 names.txt (32,033개 이름)
    │
    ▼
【Phase 1 — 데이터 파이프라인】 (~20줄)
    ├─ import os/math/random
    ├─ random.shuffle(docs)          ← 학습 순서 무작위화
    ├─ uchars = sorted(set(''.join(docs)))  ← 고유 문자 추출
    └─ vocab_size = 27 (a~z + BOS)

    │  각 이름 "emma" → [26, 4, 12, 12, 0, 26]
    ▼
【Phase 2 — Autograd 엔진】 (~40줄)
    ├─ class Value: data, grad, _children, _local_grads
    ├─ 6가지 연산: add, mul, pow, log, exp, relu
    └─ backward(): 위상 정렬 → 연쇄 법칙 적용

    │  모든 숫자가 미분 가능한 Value 객체가 됨
    ▼
【Phase 3 — 신경망 정의】 (~50줄)
    ├─ state_dict: 4,192개 Value 파라미터
    ├─ linear(), softmax(), rmsnorm()
    └─ gpt(token_id, pos_id, keys, values) → logits[27]

    │  토큰 → 다음 토큰 확률 분포
    ▼
【Phase 4 — 학습 루프 (1,000 스텝)】 (~30줄)
    ├─ 순전파: loss = CrossEntropy(gpt(tokens), targets)
    ├─ 역전파: loss.backward()
    ├─ Adam: p.data -= lr * m_hat / (v_hat**0.5 + ε)
    └─ lr 선형 감쇠: 0.01 → 0.00001

    │  loss: 3.30 → 2.37
    ▼
【Phase 5 — 추론 (이름 생성)】
    BOS → 't' → 'o' → 'm' → BOS  →  "tom"</code></pre>

  <div class="diagram-container">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 750 140"
         style="width:100%;max-width:750px;display:block;margin:1.5em auto;background:var(--diagram-fill);border-radius:8px;padding:16px;">
      <defs>
        <marker id="microgpt-flow-arrow" viewBox="0 0 10 7" refX="10" refY="3.5"
                markerWidth="8" markerHeight="7" orient="auto">
          <polygon points="0 0, 10 3.5, 0 7" fill="var(--diagram-arrow)"/>
        </marker>
      </defs>
      <!-- Phase 1 -->
      <rect x="10" y="30" width="120" height="80" rx="7"
            fill="var(--bg-secondary)" stroke="var(--accent-primary)" stroke-width="2"/>
      <text x="70" y="55" text-anchor="middle" fill="var(--accent-primary)" font-size="10" font-weight="bold">Phase 1</text>
      <text x="70" y="72" text-anchor="middle" fill="var(--diagram-text)" font-size="11">데이터·토크나이저</text>
      <text x="70" y="89" text-anchor="middle" fill="var(--text-muted)" font-size="9">names.txt → 토큰</text>
      <text x="70" y="103" text-anchor="middle" fill="var(--text-muted)" font-size="9">vocab_size=27</text>
      <!-- Arrow 1 -->
      <line x1="130" y1="70" x2="153" y2="70" stroke="var(--diagram-arrow)" stroke-width="1.5" marker-end="url(#microgpt-flow-arrow)"/>
      <!-- Phase 2 -->
      <rect x="155" y="30" width="120" height="80" rx="7"
            fill="var(--bg-secondary)" stroke="var(--accent-secondary)" stroke-width="2"/>
      <text x="215" y="55" text-anchor="middle" fill="var(--accent-secondary)" font-size="10" font-weight="bold">Phase 2</text>
      <text x="215" y="72" text-anchor="middle" fill="var(--diagram-text)" font-size="11">Autograd 엔진</text>
      <text x="215" y="89" text-anchor="middle" fill="var(--text-muted)" font-size="9">class Value</text>
      <text x="215" y="103" text-anchor="middle" fill="var(--text-muted)" font-size="9">add·mul·log·exp</text>
      <!-- Arrow 2 -->
      <line x1="275" y1="70" x2="298" y2="70" stroke="var(--diagram-arrow)" stroke-width="1.5" marker-end="url(#microgpt-flow-arrow)"/>
      <!-- Phase 3 -->
      <rect x="300" y="30" width="120" height="80" rx="7"
            fill="var(--bg-secondary)" stroke="var(--border-color)" stroke-width="1.5"/>
      <text x="360" y="55" text-anchor="middle" fill="var(--diagram-text)" font-size="10" font-weight="bold">Phase 3</text>
      <text x="360" y="72" text-anchor="middle" fill="var(--diagram-text)" font-size="11">신경망 정의</text>
      <text x="360" y="89" text-anchor="middle" fill="var(--text-muted)" font-size="9">gpt() 함수</text>
      <text x="360" y="103" text-anchor="middle" fill="var(--text-muted)" font-size="9">4,192 파라미터</text>
      <!-- Arrow 3 -->
      <line x1="420" y1="70" x2="443" y2="70" stroke="var(--diagram-arrow)" stroke-width="1.5" marker-end="url(#microgpt-flow-arrow)"/>
      <!-- Phase 4 -->
      <rect x="445" y="30" width="120" height="80" rx="7"
            fill="var(--bg-secondary)" stroke="var(--border-color)" stroke-width="1.5"/>
      <text x="505" y="55" text-anchor="middle" fill="var(--diagram-text)" font-size="10" font-weight="bold">Phase 4</text>
      <text x="505" y="72" text-anchor="middle" fill="var(--diagram-text)" font-size="11">학습 루프</text>
      <text x="505" y="89" text-anchor="middle" fill="var(--text-muted)" font-size="9">1000 스텝 · Adam</text>
      <text x="505" y="103" text-anchor="middle" fill="var(--text-muted)" font-size="9">loss 3.30 → 2.37</text>
      <!-- Arrow 4 -->
      <line x1="565" y1="70" x2="588" y2="70" stroke="var(--diagram-arrow)" stroke-width="1.5" marker-end="url(#microgpt-flow-arrow)"/>
      <!-- Phase 5 -->
      <rect x="590" y="30" width="145" height="80" rx="7"
            fill="var(--bg-secondary)" stroke="var(--accent-primary)" stroke-width="2"/>
      <text x="662" y="55" text-anchor="middle" fill="var(--accent-primary)" font-size="10" font-weight="bold">Phase 5</text>
      <text x="662" y="72" text-anchor="middle" fill="var(--diagram-text)" font-size="11">추론 (이름 생성)</text>
      <text x="662" y="89" text-anchor="middle" fill="var(--text-muted)" font-size="9">BOS → 't'→'o'→'m'</text>
      <text x="662" y="103" text-anchor="middle" fill="var(--text-muted)" font-size="9">→ BOS → "tom"</text>
    </svg>
    <p class="diagram-caption">microgpt.py 5단계 파이프라인: 데이터·토크나이저 → Autograd → 신경망 정의 → 학습 → 추론</p>
  </div>

  <div class="info-box info">
    <strong>왜 이 순서인가?</strong> 데이터 없이는 토크나이저가 없고, Autograd 없이는 학습이 없고,
    모델 없이는 추론이 없습니다. 각 블록이 다음 블록의 토대가 되는 완벽한 의존성 체인입니다.
    GPT-4도 이 4단계 구조를 따릅니다 — 규모만 다를 뿐입니다.
  </div>
</section>

<section class="content-section">
  <h2 id="data">임포트 &amp; 데이터 로딩</h2>
  <p>microgpt.py가 사용하는 임포트는 단 세 줄입니다.
  ML 라이브러리가 없다는 것 자체가 이 프로젝트의 핵심 철학입니다.</p>

  <pre><code><span class="cmt"># 수학적 원리를 라이브러리 뒤에 숨기지 않고 직접 보여주기 위함</span>
<span class="kw">import</span> os        <span class="cmt"># os.path.exists — 파일 존재 여부 확인</span>
<span class="kw">import</span> math      <span class="cmt"># math.log, math.exp — 수학 함수</span>
<span class="kw">import</span> random    <span class="cmt"># random.seed, random.gauss, random.choices</span>

random.<span class="fn">seed</span>(<span class="num">42</span>)  <span class="cmt"># 재현 가능한 결과 — 동일한 random.seed → 동일한 학습 결과</span></code></pre>

  <h3 id="dataset">데이터셋 — makemore 저장소</h3>
  <p><strong>데이터 출처:</strong> <code>names.txt</code>는 Karpathy의 makemore 프로젝트의 영어 이름 데이터셋입니다.
  약 <strong>32,000개</strong>의 고유 이름이 포함되어 있습니다.</p>
  <pre><code><span class="kw">if not</span> os.path.<span class="fn">exists</span>(<span class="str">'input.txt'</span>):
    <span class="kw">import</span> urllib.request
    names_url = <span class="str">'https://raw.githubusercontent.com/karpathy/makemore/988aa59/names.txt'</span>
    urllib.request.<span class="fn">urlretrieve</span>(names_url, <span class="str">'input.txt'</span>)

docs = [line.<span class="fn">strip</span>() <span class="kw">for</span> line <span class="kw">in</span> <span class="fn">open</span>(<span class="str">'input.txt'</span>) <span class="kw">if</span> line.<span class="fn">strip</span>()]
random.<span class="fn">shuffle</span>(docs)  <span class="cmt"># 순서 섞기 — 학습 편향 방지</span>
<span class="fn">print</span>(<span class="str">f"num docs: {len(docs)}"</span>)  <span class="cmt"># → num docs: 32033</span></code></pre>

  <p>input.txt — 각 줄이 하나의 이름(= 하나의 훈련 문서):</p>
  <pre><code>emma        ← 4글자
olivia      ← 6글자
ava         ← 3글자
isabella    ← 8글자
sophia      ← 6글자
...
(총 32,033줄)</code></pre>

  <table>
    <thead><tr><th>특성</th><th>값</th><th>이유</th></tr></thead>
    <tbody>
      <tr><td>총 이름 수</td><td>32,033개</td><td>미국 신생아 이름 통계 기반</td></tr>
      <tr><td>문자 종류</td><td>소문자 a~z (26종)</td><td>대소문자·숫자·특수문자 없음</td></tr>
      <tr><td>최단 이름</td><td>2글자 (예: "jo")</td><td>–</td></tr>
      <tr><td>최장 이름</td><td>15글자</td><td>block_size=16 설정 근거</td></tr>
      <tr><td>평균 길이</td><td>약 5~6글자</td><td>–</td></tr>
    </tbody>
  </table>

  <h3 id="shuffle-reason">random.shuffle — 왜 섞는가?</h3>
  <p>학습 루프는 <code>docs[step % len(docs)]</code>로 순환합니다.
  섞지 않으면 처음 1,000 스텝 동안 항상 같은 이름 순서로 학습하게 됩니다.
  데이터셋의 앞부분에 특정 패턴(예: 짧은 이름, 특정 알파벳으로 시작하는 이름)이
  몰려 있으면 모델이 그 패턴에 편향됩니다.</p>
  <pre><code>random.<span class="fn">seed</span>(<span class="num">42</span>)       <span class="cmt"># ← 먼저 시드를 설정한 뒤</span>
random.<span class="fn">shuffle</span>(docs)  <span class="cmt"># ← 섞어야 재현 가능한 순서가 됩니다</span></code></pre>

  <h3 id="batch-size-1">배치 크기 1 — 왜 이름 하나씩 학습하는가?</h3>
  <p>microgpt는 한 번에 이름 하나(batch_size=1)를 학습합니다.
  배치 크기가 크면 GPU 메모리와 병렬 연산이 필요하지만,
  microgpt는 CPU에서 스칼라 연산으로 작동하므로 배치 크기 1이 가장 단순합니다.</p>
  <div class="info-box info">
    <strong>학습 vs 추론의 차이:</strong>
    학습 시에는 이름 전체 토큰을 한 번에 순전파합니다(각 위치에서 다음 토큰 예측).
    추론 시에는 BOS부터 시작해서 한 번에 토큰 하나씩 생성합니다.
  </div>

  <h3 id="why-names">왜 이름 데이터인가?</h3>
  <ul>
    <li><strong>검증이 쉽다:</strong> 생성된 이름이 발음 가능한지 사람이 즉시 판단 가능.</li>
    <li><strong>규모가 적당하다:</strong> 32,033개 × 평균 5글자 = 약 160,000 토큰 — 노트북으로 1분 내 학습.</li>
    <li><strong>패턴이 명확하다:</strong> 이름은 모음-자음 교대, 특정 접미사(-a, -on, -er) 등 학습 가능한 통계 패턴이 있음.</li>
    <li><strong>범용성:</strong> "이름"이나 "대화"나 GPT 입장에서는 모두 토큰 시퀀스. 원리는 동일.</li>
  </ul>
</section>

<section class="content-section">
  <h2 id="tokenizer">토크나이저 구축</h2>
  <p>텍스트를 숫자로, 숫자를 다시 텍스트로 변환하는 번역기를 만듭니다.
  microgpt.py는 <strong>문자 수준(character-level)</strong> 토크나이저를 사용합니다.</p>

  <pre><code><span class="cmt"># 모든 이름을 합쳐 고유한 문자만 추출 (정렬)</span>
uchars = <span class="fn">sorted</span>(<span class="fn">set</span>(<span class="str">''</span>.<span class="fn">join</span>(docs)))
<span class="cmt"># 결과: ['a', 'b', 'c', ..., 'z'] → 26개 소문자</span>

BOS = <span class="fn">len</span>(uchars)            <span class="cmt"># = 26, Beginning of Sequence 특수 토큰 ID</span>
vocab_size = <span class="fn">len</span>(uchars) + <span class="num">1</span>  <span class="cmt"># = 27 (a~z 26개 + BOS 1개)</span>

<span class="cmt"># 인코딩: uchars.index(ch)  디코딩: uchars[token_id]</span>

<span class="cmt"># 예시: "emma" → [BOS, e, m, m, a, BOS]</span>
name = <span class="str">"emma"</span>
tokens = [BOS] + [uchars.<span class="fn">index</span>(ch) <span class="kw">for</span> ch <span class="kw">in</span> name] + [BOS]
<span class="cmt"># → [26, 4, 12, 12, 0, 26]</span>

<span class="cmt"># 디코딩: 정수 → 문자 (BOS 제외)</span>
decoded = <span class="str">''</span>.<span class="fn">join</span>(uchars[t] <span class="kw">for</span> t <span class="kw">in</span> tokens <span class="kw">if</span> t != BOS)
<span class="cmt"># → "emma"</span></code></pre>

  <h3 id="bos-wrap">BOS 래핑 — 문서 경계 표현</h3>
  <pre><code><span class="str">"emma"</span> → tokens = [<span class="num">26</span>, <span class="num">4</span>, <span class="num">12</span>, <span class="num">12</span>, <span class="num">0</span>, <span class="num">26</span>]

<span class="cmt">【학습 입력/타겟 쌍】</span>
pos 0: 입력=BOS(26)  → 타겟=<span class="str">'e'</span>(4)   ← "이름의 시작엔 무엇이 오는가?"
pos 1: 입력=<span class="str">'e'</span>(4)   → 타겟=<span class="str">'m'</span>(12)  ← "e 다음엔 무엇이 오는가?"
pos 2: 입력=<span class="str">'m'</span>(12)  → 타겟=<span class="str">'m'</span>(12)  ← "em 다음엔?"
pos 3: 입력=<span class="str">'m'</span>(12)  → 타겟=<span class="str">'a'</span>(0)   ← "emm 다음엔?"
pos 4: 입력=<span class="str">'a'</span>(0)   → 타겟=BOS(<span class="num">26</span>)  ← "emma 다음엔? → 끝!"</code></pre>

  <div class="info-box info">
    <strong>BOS를 양쪽에 붙이는 이유:</strong>
    앞에 BOS → 모델이 "이름이 시작된다"는 신호를 받고 첫 글자를 예측.
    뒤에 BOS → 모델이 "이름이 끝났다"는 것을 학습(추론 시 생성 종료 조건).
    BOS 하나가 시작 신호이자 종료 신호 두 역할을 모두 합니다.
  </div>

  <div class="diagram-container">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 700 140"
         style="width:100%;max-width:700px;display:block;margin:1.5em auto;background:var(--diagram-fill);border-radius:8px;padding:16px;">
      <defs>
        <marker id="microgpt-tok-arrow" viewBox="0 0 10 7" refX="10" refY="3.5"
                markerWidth="8" markerHeight="7" orient="auto">
          <polygon points="0 0, 10 3.5, 0 7" fill="var(--diagram-arrow)"/>
        </marker>
      </defs>
      <!-- Step 1: 문자열 -->
      <rect x="8" y="35" width="90" height="70" rx="6"
            fill="var(--bg-secondary)" stroke="var(--accent-primary)" stroke-width="2"/>
      <text x="53" y="60" text-anchor="middle" fill="var(--accent-primary)" font-size="9" font-weight="bold">문자열</text>
      <text x="53" y="78" text-anchor="middle" fill="var(--diagram-text)" font-size="14" font-family="monospace">"emma"</text>
      <text x="53" y="97" text-anchor="middle" fill="var(--text-muted)" font-size="9">4글자</text>
      <!-- Arrow -->
      <line x1="98" y1="70" x2="116" y2="70" stroke="var(--diagram-arrow)" stroke-width="1.5" marker-end="url(#microgpt-tok-arrow)"/>
      <!-- Step 2: 문자 분해 -->
      <rect x="118" y="35" width="110" height="70" rx="6"
            fill="var(--bg-secondary)" stroke="var(--border-color)" stroke-width="1.5"/>
      <text x="173" y="57" text-anchor="middle" fill="var(--diagram-text)" font-size="9" font-weight="bold">문자 분해</text>
      <text x="173" y="76" text-anchor="middle" fill="var(--diagram-text)" font-size="12" font-family="monospace">e m m a</text>
      <text x="173" y="95" text-anchor="middle" fill="var(--text-muted)" font-size="9">uchars.index(ch)</text>
      <!-- Arrow -->
      <line x1="228" y1="70" x2="246" y2="70" stroke="var(--diagram-arrow)" stroke-width="1.5" marker-end="url(#microgpt-tok-arrow)"/>
      <!-- Step 3: 인덱스 조회 -->
      <rect x="248" y="35" width="110" height="70" rx="6"
            fill="var(--bg-secondary)" stroke="var(--border-color)" stroke-width="1.5"/>
      <text x="303" y="57" text-anchor="middle" fill="var(--diagram-text)" font-size="9" font-weight="bold">인덱스 변환</text>
      <text x="303" y="76" text-anchor="middle" fill="var(--diagram-text)" font-size="12" font-family="monospace">4 12 12 0</text>
      <text x="303" y="95" text-anchor="middle" fill="var(--text-muted)" font-size="9">a=0 … z=25</text>
      <!-- Arrow -->
      <line x1="358" y1="70" x2="376" y2="70" stroke="var(--diagram-arrow)" stroke-width="1.5" marker-end="url(#microgpt-tok-arrow)"/>
      <!-- Step 4: BOS 래핑 -->
      <rect x="378" y="35" width="110" height="70" rx="6"
            fill="var(--bg-secondary)" stroke="var(--accent-secondary)" stroke-width="2"/>
      <text x="433" y="57" text-anchor="middle" fill="var(--accent-secondary)" font-size="9" font-weight="bold">BOS 래핑</text>
      <text x="433" y="76" text-anchor="middle" fill="var(--diagram-text)" font-size="10" font-family="monospace">[26]+ids+[26]</text>
      <text x="433" y="95" text-anchor="middle" fill="var(--text-muted)" font-size="9">BOS=26</text>
      <!-- Arrow -->
      <line x1="488" y1="70" x2="506" y2="70" stroke="var(--diagram-arrow)" stroke-width="1.5" marker-end="url(#microgpt-tok-arrow)"/>
      <!-- Step 5: 토큰 시퀀스 -->
      <rect x="508" y="20" width="180" height="100" rx="6"
            fill="var(--bg-secondary)" stroke="var(--accent-primary)" stroke-width="2"/>
      <text x="598" y="44" text-anchor="middle" fill="var(--accent-primary)" font-size="9" font-weight="bold">토큰 시퀀스</text>
      <text x="598" y="68" text-anchor="middle" fill="var(--diagram-text)" font-size="12" font-family="monospace">[26, 4, 12, 12, 0, 26]</text>
      <text x="598" y="88" text-anchor="middle" fill="var(--text-muted)" font-size="9">BOS e  m  m  a BOS</text>
      <text x="598" y="107" text-anchor="middle" fill="var(--text-muted)" font-size="9">길이 6 (4글자 + BOS×2)</text>
    </svg>
    <p class="diagram-caption">"emma" 토크나이저 변환 파이프라인: 문자열 → 문자 분해 → 인덱스 조회 → BOS 래핑 → 토큰 시퀀스</p>
  </div>

  <h3 id="token-ids">토큰 ID 구조 — 27개 심볼</h3>
  <table>
    <thead><tr><th>ID</th><th>심볼</th><th>ID</th><th>심볼</th><th>ID</th><th>심볼</th></tr></thead>
    <tbody>
      <tr><td>0</td><td>'a'</td><td>9</td><td>'j'</td><td>18</td><td>'s'</td></tr>
      <tr><td>1</td><td>'b'</td><td>10</td><td>'k'</td><td>19</td><td>'t'</td></tr>
      <tr><td>2</td><td>'c'</td><td>11</td><td>'l'</td><td>20</td><td>'u'</td></tr>
      <tr><td>3</td><td>'d'</td><td>12</td><td>'m'</td><td>21</td><td>'v'</td></tr>
      <tr><td>4</td><td>'e'</td><td>13</td><td>'n'</td><td>22</td><td>'w'</td></tr>
      <tr><td>5</td><td>'f'</td><td>14</td><td>'o'</td><td>23</td><td>'x'</td></tr>
      <tr><td>6</td><td>'g'</td><td>15</td><td>'p'</td><td>24</td><td>'y'</td></tr>
      <tr><td>7</td><td>'h'</td><td>16</td><td>'q'</td><td>25</td><td>'z'</td></tr>
      <tr><td>8</td><td>'i'</td><td>17</td><td>'r'</td><td><strong>26</strong></td><td><strong>BOS</strong></td></tr>
    </tbody>
  </table>

  <table>
    <thead><tr><th>항목</th><th>microgpt (문자 수준)</th><th>tiktoken / BPE (프로덕션)</th></tr></thead>
    <tbody>
      <tr><td>어휘집 크기</td><td>27개 (a~z + BOS)</td><td>~100,000개</td></tr>
      <tr><td>특징</td><td>구현 단순, 이해 쉬움</td><td>위치당 더 많은 의미 압축</td></tr>
      <tr><td>인코딩 방식</td><td><code>uchars.index(ch)</code> 사용</td><td>자주 쓰이는 문자열 병합(BPE)</td></tr>
      <tr><td>인코딩 복잡도</td><td>O(n) 선형 탐색</td><td>O(1) 딕셔너리 룩업</td></tr>
      <tr><td>용도</td><td>소규모 교육용에 적합</td><td>GPT-4 등 실제 LLM에서 사용</td></tr>
    </tbody>
  </table>

  <h3 id="tokenizer-pitfalls">흔한 실수들</h3>
  <ul>
    <li>
      <strong>Off-by-One 오류:</strong>
      어휘집 크기를 26으로 잘못 지정하면 BOS(ID=26)를 임베딩 테이블에서 찾지 못해 IndexError 발생.
      <code>wte = matrix(vocab_size, n_embd)</code>에서 <code>vocab_size=27</code>이어야 합니다.
    </li>
    <li>
      <strong>일관성 없는 어휘집:</strong>
      소문자로 훈련된 모델에 대문자("Emma")를 입력하면 <code>uchars.index('E')</code>가 ValueError를 냅니다.
      모든 이름은 <strong>소문자 전처리</strong> 후 토크나이저에 입력해야 합니다.
    </li>
    <li>
      <strong>uchars.index() 시간 복잡도:</strong>
      <code>list.index()</code>는 O(n) 선형 탐색입니다. 어휘 크기가 27개로 작기 때문에 문제없지만,
      vocab_size가 50,000이라면 반드시 dict 기반 룩업테이블로 바꿔야 합니다.
    </li>
  </ul>
</section>

<section class="content-section">
  <h2 id="autograd">Autograd 엔진 — 학습의 심장</h2>
  <p>PyTorch의 핵심 기능인 <strong>자동 미분(Automatic Differentiation)</strong>을 밑바닥부터 직접 구현합니다.
  모든 숫자가 그냥 숫자가 아니라, <em>자신이 어디서 왔는지</em>를 기억하는 <code>Value</code> 객체가 됩니다.</p>

  <div class="info-box tip">
    <strong>핵심 아이디어:</strong> 순전파(forward pass) 중에 계산 그래프를 동적으로 구성하고,
    역전파(backward pass) 시 연쇄 법칙(Chain Rule)을 적용해 모든 파라미터의 기울기를 자동으로 계산합니다.
  </div>

  <h3 id="chain-rule">연쇄 법칙 (Chain Rule)</h3>
  <p><strong>dy/dx = (dy/du) · (du/dx)</strong> — 미적분학의 핵심 — 복합 함수의 미분</p>
  <dl>
    <dt><code>self.grad</code> — 전역 미분값</dt>
    <dd>d(Loss)/d(self) — 역전파 시 계산됨</dd>
    <dt><code>_local_grads</code> — 지역 미분값</dt>
    <dd>d(self)/d(child) — 순전파 시 계산됨</dd>
  </dl>

  <h3 id="ops-table">6가지 연산의 지역 기울기 — 한눈에 보기</h3>
  <table>
    <thead>
      <tr><th>연산</th><th>순전파 f(a, b)</th><th>지역 기울기 ∂f/∂a</th><th>직관</th></tr>
    </thead>
    <tbody>
      <tr><td>add</td><td>a + b</td><td>1 (∂f/∂b = 1)</td><td>덧셈은 기울기를 그대로 흘림</td></tr>
      <tr><td>mul</td><td>a × b</td><td>b (∂f/∂b = a)</td><td>상대방 값이 기울기 배율</td></tr>
      <tr><td>pow</td><td>a<sup>n</sup></td><td>n · a<sup>n−1</sup></td><td>지수 법칙</td></tr>
      <tr><td>log</td><td>ln(a)</td><td>1 / a</td><td>로그의 도함수</td></tr>
      <tr><td>exp</td><td>e<sup>a</sup></td><td>e<sup>a</sup> (자기 자신!)</td><td>e^x의 도함수 = e^x</td></tr>
      <tr><td>relu</td><td>max(0, a)</td><td>1 if a &gt; 0 else 0</td><td>양수만 기울기 통과</td></tr>
    </tbody>
  </table>
  <div class="info-box info">
    <strong>연쇄 법칙 직관 — 속도 비유:</strong>
    자동차가 자전거보다 <strong>2배</strong> 빠르고, 자전거가 사람보다 <strong>4배</strong> 빠르다면 →
    자동차는 사람보다 <strong>2 × 4 = 8배</strong> 빠릅니다.
    dL/dx = dL/dy · dy/dx — 각 단계의 기울기를 <strong>곱하면</strong> 전체 기울기가 됩니다.
  </div>

  <h3 id="value-class">Value 클래스 전체 코드</h3>
  <pre><code><span class="kw">class</span> <span class="type">Value</span>:
    __slots__ = (<span class="str">'data'</span>, <span class="str">'grad'</span>, <span class="str">'_children'</span>, <span class="str">'_local_grads'</span>)

    <span class="kw">def</span> <span class="fn">__init__</span>(self, data, children=(), local_grads=()):
        self.data = data          <span class="cmt"># float: 순전파 결과값</span>
        self.grad = <span class="num">0</span>             <span class="cmt"># float: d(Loss)/d(self), 역전파 시 채워짐</span>
        self._children = children         <span class="cmt"># tuple[Value]: 이 노드를 만든 입력들</span>
        self._local_grads = local_grads   <span class="cmt"># tuple[float]: d(self)/d(child)</span>

    <span class="cmt"># ── 핵심 이진 연산 두 개 (나머지는 모두 이 두 개의 조합) ──</span>
    <span class="kw">def</span> <span class="fn">__add__</span>(self, other):
        other = other <span class="kw">if</span> <span class="fn">isinstance</span>(other, <span class="type">Value</span>) <span class="kw">else</span> <span class="type">Value</span>(other)
        <span class="cmt"># f = a + b  →  ∂f/∂a = 1,  ∂f/∂b = 1</span>
        <span class="kw">return</span> <span class="type">Value</span>(self.data + other.data, (self, other), (<span class="num">1</span>, <span class="num">1</span>))

    <span class="kw">def</span> <span class="fn">__mul__</span>(self, other):
        other = other <span class="kw">if</span> <span class="fn">isinstance</span>(other, <span class="type">Value</span>) <span class="kw">else</span> <span class="type">Value</span>(other)
        <span class="cmt"># f = a × b  →  ∂f/∂a = b,  ∂f/∂b = a</span>
        <span class="kw">return</span> <span class="type">Value</span>(self.data * other.data, (self, other), (other.data, self.data))

    <span class="cmt"># ── 나머지 기본 연산 ──</span>
    <span class="kw">def</span> <span class="fn">__pow__</span>(self, other):
        <span class="cmt"># f = a^n  →  ∂f/∂a = n·a^(n-1)</span>
        <span class="kw">return</span> <span class="type">Value</span>(self.data**other, (self,), (other * self.data**(other-<span class="num">1</span>),))

    <span class="kw">def</span> <span class="fn">log</span>(self):
        <span class="cmt"># f = ln(a)  →  ∂f/∂a = 1/a</span>
        <span class="kw">return</span> <span class="type">Value</span>(math.<span class="fn">log</span>(self.data), (self,), (<span class="num">1</span>/self.data,))

    <span class="kw">def</span> <span class="fn">exp</span>(self):
        <span class="cmt"># f = e^a  →  ∂f/∂a = e^a  (도함수가 자기 자신!)</span>
        <span class="kw">return</span> <span class="type">Value</span>(math.<span class="fn">exp</span>(self.data), (self,), (math.<span class="fn">exp</span>(self.data),))

    <span class="kw">def</span> <span class="fn">relu</span>(self):
        <span class="cmt"># f = max(0, a)  →  ∂f/∂a = 1 if a>0 else 0</span>
        <span class="kw">return</span> <span class="type">Value</span>(<span class="fn">max</span>(<span class="num">0</span>, self.data), (self,), (<span class="fn">float</span>(self.data &gt; <span class="num">0</span>),))

    <span class="cmt"># ── 파생 연산 (위 연산들의 조합으로 자동 처리) ──</span>
    <span class="kw">def</span> <span class="fn">__neg__</span>(self):   <span class="kw">return</span> self * -<span class="num">1</span>           <span class="cmt"># -a = a × (-1)</span>
    <span class="kw">def</span> <span class="fn">__sub__</span>(self, other):  <span class="kw">return</span> self + (-other)    <span class="cmt"># a - b = a + (-b)</span>
    <span class="kw">def</span> <span class="fn">__rsub__</span>(self, other): <span class="kw">return</span> other + (-self)    <span class="cmt"># b - a</span>
    <span class="kw">def</span> <span class="fn">__radd__</span>(self, other): <span class="kw">return</span> self + other       <span class="cmt"># b + a (sum()에서 필요)</span>
    <span class="kw">def</span> <span class="fn">__rmul__</span>(self, other): <span class="kw">return</span> self * other       <span class="cmt"># b × a</span>
    <span class="kw">def</span> <span class="fn">__truediv__</span>(self, other): <span class="kw">return</span> self * other**-<span class="num">1</span>  <span class="cmt"># a/b = a × b^(-1)</span>
    <span class="kw">def</span> <span class="fn">__rtruediv__</span>(self, other): <span class="kw">return</span> other * self**-<span class="num">1</span> <span class="cmt"># b/a</span></code></pre>

  <h3 id="slots">__slots__ — 메모리 최적화</h3>
  <p>파이썬 객체는 기본적으로 <code>__dict__</code>라는 딕셔너리로 속성을 저장합니다.
  <code>__slots__</code>를 선언하면 이 딕셔너리 대신 고정된 슬롯(C 배열)을 사용합니다.</p>
  <table>
    <thead><tr><th>방식</th><th>인스턴스당 메모리</th><th>4,192개 기준</th></tr></thead>
    <tbody>
      <tr><td>__dict__ (기본)</td><td>~200~400 bytes</td><td>~1.5 MB</td></tr>
      <tr><td>__slots__ (microgpt)</td><td>~100~150 bytes</td><td>~0.6 MB</td></tr>
    </tbody>
  </table>
  <p>학습 중에는 순전파 시 파라미터 수의 수십~수백 배의 중간 Value 노드가 생성됩니다.
  메모리 절약 외에도 속성 접근 속도도 향상됩니다.</p>

  <h3 id="radd">__radd__ 가 필요한 이유</h3>
  <pre><code><span class="cmt"># Python의 sum()은 내부적으로 0 + first_element 를 실행합니다</span>
<span class="cmt"># → 0.__add__(Value) → int는 Value를 모르므로 NotImplemented 반환</span>
<span class="cmt"># → Python이 Value.__radd__(0) 를 대신 호출</span>
<span class="cmt"># → self + other = Value + 0 → 정상 작동</span>

total = <span class="fn">sum</span>(losses)   <span class="cmt"># sum([Value, Value, ...]) → 내부적으로 0 + Value(...) 실행</span>
<span class="cmt"># __radd__ 없으면 TypeError!</span></code></pre>

  <h3 id="backward">역전파 — backward()</h3>
  <pre><code><span class="kw">def</span> <span class="fn">backward</span>(self):
    <span class="cmt"># 1) 위상 정렬(Topological Sort)로 그래프 순회</span>
    topo = []
    visited = <span class="fn">set</span>()

    <span class="kw">def</span> <span class="fn">build_topo</span>(v):
        <span class="kw">if</span> v <span class="kw">not in</span> visited:
            visited.<span class="fn">add</span>(v)
            <span class="kw">for</span> child <span class="kw">in</span> v._children:
                <span class="fn">build_topo</span>(child)
            topo.<span class="fn">append</span>(v)   <span class="cmt"># 자식 처리 후 자신을 추가 (후위 순회)</span>

    <span class="fn">build_topo</span>(self)

    <span class="cmt"># 2) 역순으로 기울기 전파</span>
    self.grad = <span class="num">1</span>  <span class="cmt"># d(Loss)/d(Loss) = 1 (역전파 출발점)</span>
    <span class="kw">for</span> v <span class="kw">in</span> <span class="fn">reversed</span>(topo):
        <span class="kw">for</span> child, local_grad <span class="kw">in</span> <span class="fn">zip</span>(v._children, v._local_grads):
            child.grad += local_grad * v.grad  <span class="cmt"># += 누적! (다변수 연쇄 법칙)</span></code></pre>

  <div class="info-box tip">
    <strong>위상 정렬(Topological Sort)의 비밀:</strong> 계산 그래프를 역순으로 순회함으로써,
    모든 child 노드의 기울기를 정확히 계산할 수 있습니다. 이것이 PyTorch의 <code>torch.autograd</code>가
    내부적으로 수행하는 작업과 동일한 원리입니다.
  </div>

  <h3 id="backprop-example">역전파 단계별 추적 — <code>L = (a * b) + a</code></h3>
  <p>a = 2.0, b = 3.0인 경우. 수학적으로 dL/da = b + 1 = 4.0, dL/db = a = 2.0</p>
  <pre><code><span class="cmt"># ── 순전파 ──</span>
a = <span class="type">Value</span>(<span class="num">2.0</span>)   <span class="cmt"># 파라미터</span>
b = <span class="type">Value</span>(<span class="num">3.0</span>)   <span class="cmt"># 파라미터</span>
c = a * b        <span class="cmt"># c.data=6.0, c._children=(a,b), c._local_grads=(3.0, 2.0)</span>
                 <span class="cmt">#   ∂c/∂a = b.data = 3.0,  ∂c/∂b = a.data = 2.0</span>
L = c + a        <span class="cmt"># L.data=8.0, L._children=(c,a), L._local_grads=(1, 1)</span>
                 <span class="cmt">#   ∂L/∂c = 1,  ∂L/∂a = 1</span>

<span class="cmt"># ── 위상 정렬 결과 (DFS 후위 순회) ──</span>
<span class="cmt"># topo = [a, b, c, L]  ← 의존성 순서</span>
<span class="cmt"># reversed(topo) = [L, c, b, a]  ← 역전파 순서</span>

<span class="cmt"># ── 역전파 ──</span>
L.grad = <span class="num">1</span>   <span class="cmt"># d(L)/d(L) = 1 (출발점)</span>

<span class="cmt"># v=L: children=(c,a), local_grads=(1, 1)</span>
<span class="cmt"># c.grad += 1 * L.grad  = 1 * 1 = 1.0</span>
<span class="cmt"># a.grad += 1 * L.grad  = 1 * 1 = 1.0   (L = c + a 에서 a 경로)</span>

<span class="cmt"># v=c: children=(a,b), local_grads=(3.0, 2.0)</span>
<span class="cmt"># a.grad += 3.0 * c.grad = 3.0 * 1.0    → a.grad = 1.0 + 3.0 = 4.0</span>
<span class="cmt"># b.grad += 2.0 * c.grad = 2.0 * 1.0    → b.grad = 0.0 + 2.0 = 2.0</span>

<span class="cmt"># 최종: a.grad=4.0 ✓, b.grad=2.0 ✓ (수학 계산과 일치)</span></code></pre>

  <h3 id="comp-graph">계산 그래프 시각화</h3>
  <pre><code>       a(2.0)    b(3.0)
        │  \      │
        │   ×─────┘  ← __mul__: local_grads=(b, a)=(3.0, 2.0)
        │    \
        │     c(6.0)
        │      │
        └──────+      ← __add__: local_grads=(1, 1)
               │
              L(8.0)   ← loss.backward() 시작점, L.grad=1

역전파 방향: L → c,a → a,b  (화살표 반대 방향으로 grad 전파)</code></pre>

  <div class="diagram-container">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 700 260"
         style="width:100%;max-width:700px;display:block;margin:1.5em auto;background:var(--diagram-fill);border-radius:8px;padding:20px;">
      <defs>
        <marker id="microgpt-intro-arrow" viewBox="0 0 10 7" refX="10" refY="3.5"
                markerWidth="10" markerHeight="7" orient="auto">
          <polygon points="0 0, 10 3.5, 0 7" fill="var(--diagram-arrow)"/>
        </marker>
      </defs>
      <rect x="20" y="80" width="80" height="40" rx="6"
            fill="var(--bg-secondary)" stroke="var(--accent-primary)" stroke-width="2"/>
      <text x="60" y="97" text-anchor="middle" fill="var(--diagram-text)" font-size="12" font-family="monospace">a=2.0</text>
      <text x="60" y="113" text-anchor="middle" fill="var(--accent-secondary)" font-size="10">grad=4.0</text>
      <rect x="20" y="160" width="80" height="40" rx="6"
            fill="var(--bg-secondary)" stroke="var(--border-color)" stroke-width="1.5"/>
      <text x="60" y="177" text-anchor="middle" fill="var(--diagram-text)" font-size="12" font-family="monospace">b=3.0</text>
      <text x="60" y="193" text-anchor="middle" fill="var(--accent-secondary)" font-size="10">grad=2.0</text>
      <rect x="230" y="120" width="100" height="40" rx="6"
            fill="var(--bg-secondary)" stroke="var(--border-color)" stroke-width="1.5"/>
      <text x="280" y="137" text-anchor="middle" fill="var(--diagram-text)" font-size="11" font-family="monospace">c=a×b=6.0</text>
      <text x="280" y="153" text-anchor="middle" fill="var(--accent-secondary)" font-size="10">grad=1.0</text>
      <rect x="460" y="100" width="120" height="40" rx="6"
            fill="var(--bg-secondary)" stroke="var(--accent-primary)" stroke-width="2"/>
      <text x="520" y="117" text-anchor="middle" fill="var(--diagram-text)" font-size="11" font-family="monospace">L=c+a=8.0</text>
      <text x="520" y="133" text-anchor="middle" fill="var(--accent-secondary)" font-size="10">grad=1.0 (시작)</text>
      <line x1="100" y1="100" x2="228" y2="135" stroke="var(--diagram-arrow)" stroke-width="1.5" marker-end="url(#microgpt-intro-arrow)"/>
      <line x1="100" y1="180" x2="228" y2="148" stroke="var(--diagram-arrow)" stroke-width="1.5" marker-end="url(#microgpt-intro-arrow)"/>
      <line x1="330" y1="140" x2="458" y2="123" stroke="var(--diagram-arrow)" stroke-width="1.5" marker-end="url(#microgpt-intro-arrow)"/>
      <line x1="100" y1="90" x2="458" y2="114" stroke="var(--diagram-arrow)" stroke-width="1.5" stroke-dasharray="4,3" marker-end="url(#microgpt-intro-arrow)"/>
      <text x="160" y="112" text-anchor="middle" fill="var(--text-muted)" font-size="10">∂c/∂a=3.0</text>
      <text x="160" y="173" text-anchor="middle" fill="var(--text-muted)" font-size="10">∂c/∂b=2.0</text>
      <text x="400" y="122" text-anchor="middle" fill="var(--text-muted)" font-size="10">∂L/∂c=1</text>
      <text x="320" y="97" text-anchor="middle" fill="var(--text-muted)" font-size="10">∂L/∂a=1</text>
    </svg>
    <p class="diagram-caption">L = (a×b) + a 의 계산 그래프와 역전파. a는 두 경로에서 기울기를 받아 grad=4.0(누적)</p>
  </div>

  <h3 id="topo-sort">재귀 위상 정렬의 동작 원리</h3>
  <pre><code><span class="kw">def</span> <span class="fn">build_topo</span>(v):
    <span class="kw">if</span> v <span class="kw">not in</span> visited:
        visited.<span class="fn">add</span>(v)
        <span class="kw">for</span> child <span class="kw">in</span> v._children:
            <span class="fn">build_topo</span>(child)  <span class="cmt"># 자식들을 먼저 처리</span>
        topo.<span class="fn">append</span>(v)         <span class="cmt"># 자식 처리 후 자신을 추가 (후위 순회)</span>

<span class="cmt"># L의 경우:</span>
<span class="cmt"># build_topo(L)</span>
<span class="cmt">#   build_topo(c)   ← L의 child</span>
<span class="cmt">#     build_topo(a) ← c의 child → visited, topo=[a]</span>
<span class="cmt">#     build_topo(b) ← c의 child → visited, topo=[a,b]</span>
<span class="cmt">#   topo=[a,b,c]</span>
<span class="cmt">#   build_topo(a)   ← L의 child → already visited, skip</span>
<span class="cmt"># topo=[a, b, c, L]</span>
<span class="cmt"># reversed → [L, c, b, a] ← 역전파 순서</span></code></pre>

  <div class="info-box info">
    <strong>+= 누적의 핵심 이유:</strong>
    변수 <code>a</code>가 두 연산(<code>a*b</code>와 <code>c+a</code>)에서 사용될 때,
    각 경로에서 오는 기울기를 <strong>더해야(+=)</strong> 합니다.
    이것이 미적분의 "다변수 연쇄 법칙(multivariable chain rule)"입니다.
    PyTorch에서도 동일하게 <code>.grad</code>는 누적됩니다
    (이 때문에 매 스텝마다 <code>optimizer.zero_grad()</code>를 호출해야 합니다).
  </div>
</section>

<!-- ===================================================== -->
<!-- 2부: 아키텍처·어텐션·파이프라인                        -->
<!-- ===================================================== -->

<section class="content-section">
  <h2 id="blueprint">모델 설계도 (Model Blueprint)</h2>
  <p>microgpt의 하이퍼파라미터를 정의합니다. 이 숫자들이 모델의 크기와 능력을 결정합니다.</p>

  <div class="info-box tip">
    <strong>GPT-2 아키텍처 따르기:</strong> "Follow GPT-2, blessed among the GPTs, with minor differences:
    layernorm → rmsnorm, no biases, GeLU → ReLU"
  </div>

  <pre><code><span class="cmt"># Model Blueprint</span>
n_embd = <span class="num">16</span>      <span class="cmt"># 임베딩 차원 (각 토큰을 16차원 벡터로 표현)</span>
n_head = <span class="num">4</span>       <span class="cmt"># 어텐션 헤드 수 (16 / 4 = 헤드당 4차원)</span>
n_layer = <span class="num">1</span>      <span class="cmt"># 트랜스포머 블록 수</span>
block_size = <span class="num">16</span>  <span class="cmt"># 최대 시퀀스 길이 (컨텍스트 윈도우)</span>
head_dim = n_embd // n_head  <span class="cmt"># = 4</span></code></pre>

  <table>
    <thead><tr><th>파라미터</th><th>값</th><th>선택 이유</th><th>더 크게 하면?</th></tr></thead>
    <tbody>
      <tr><td>n_embd</td><td>16</td><td>27개 토큰을 표현하기에 충분</td><td>표현력↑ 속도↓ 메모리↑</td></tr>
      <tr><td>n_head</td><td>4</td><td>head_dim = 16/4 = 4 (정수 분할)</td><td>다양한 어텐션 패턴 가능↑</td></tr>
      <tr><td>n_layer</td><td>1</td><td>단순성 최대화, 이름 생성엔 충분</td><td>더 깊은 추론 가능↑</td></tr>
      <tr><td>block_size</td><td>16</td><td>최장 이름이 15글자 + BOS</td><td>더 긴 시퀀스 처리 가능</td></tr>
      <tr><td>head_dim</td><td>4</td><td>= n_embd / n_head (파생값)</td><td>어텐션 해상도↑</td></tr>
    </tbody>
  </table>

  <div class="info-box info">
    <strong>block_size=16의 근거:</strong> names.txt에서 가장 긴 이름이 15글자입니다.
    BOS + 15글자 = 16 위치면 충분합니다. 16이 넘는 이름은
    <code>n = min(block_size, len(tokens) - 1)</code>으로 잘립니다.
  </div>

  <h3 id="params-count">파라미터 계산 — 4,192개의 숫자</h3>
  <table>
    <thead><tr><th>레이어</th><th>Shape</th><th>파라미터 수</th></tr></thead>
    <tbody>
      <tr><td>wte (토큰 임베딩)</td><td>27 × 16</td><td>432</td></tr>
      <tr><td>wpe (위치 임베딩)</td><td>16 × 16</td><td>256</td></tr>
      <tr><td>lm_head (언임베딩)</td><td>27 × 16</td><td>432 ※</td></tr>
      <tr><td>attn_wq (Query)</td><td>16 × 16</td><td>256</td></tr>
      <tr><td>attn_wk (Key)</td><td>16 × 16</td><td>256</td></tr>
      <tr><td>attn_wv (Value)</td><td>16 × 16</td><td>256</td></tr>
      <tr><td>attn_wo (출력 프로젝션)</td><td>16 × 16</td><td>256</td></tr>
      <tr><td>mlp_fc1 (확장)</td><td>64 × 16</td><td>1,024</td></tr>
      <tr><td>mlp_fc2 (축소)</td><td>16 × 64</td><td>1,024</td></tr>
    </tbody>
    <tfoot>
      <tr><th>총합</th><td>—</td><th>4,192</th></tr>
    </tfoot>
  </table>

  <div class="info-box info">
    <strong>※ Weight Tying (가중치 공유):</strong> 실제 코드에서 <code>wte</code>와 <code>lm_head</code>는
    각각 별도로 초기화되어 있지만, 개념적으로 공유 가능합니다.
    nanoGPT에서는 명시적으로 <code>lm_head.weight = transformer.wte.weight</code>로 공유하여
    432개 파라미터를 절약합니다. 이 아이디어는 Press &amp; Wolf (2016)에서 제안된 것으로,
    입력 임베딩과 출력 임베딩이 같은 공간에 있다는 직관에 기반합니다.<br>
    <strong>RMSNorm 파라미터 없음:</strong> microgpt의 <code>rmsnorm(x)</code>는 학습 가능한 γ(scale) 파라미터 없이
    정규화만 수행합니다. LLaMA 등 프로덕션 모델의 RMSNorm과의 차이입니다.
  </div>

  <h3 id="state-dict">state_dict — 가중치 초기화</h3>
  <pre><code><span class="cmt"># N(0, 0.08²) 가우시안으로 초기화된 행렬 생성 함수</span>
matrix = <span class="kw">lambda</span> nout, nin, std=<span class="num">0.08</span>: \
    [[<span class="type">Value</span>(random.<span class="fn">gauss</span>(<span class="num">0</span>, std)) <span class="kw">for</span> _ <span class="kw">in</span> <span class="fn">range</span>(nin)] <span class="kw">for</span> _ <span class="kw">in</span> <span class="fn">range</span>(nout)]

state_dict = {
    <span class="str">'wte'</span>:     <span class="fn">matrix</span>(vocab_size, n_embd),  <span class="cmt"># 27×16 = 432</span>
    <span class="str">'wpe'</span>:     <span class="fn">matrix</span>(block_size, n_embd),  <span class="cmt"># 16×16 = 256</span>
    <span class="str">'lm_head'</span>: <span class="fn">matrix</span>(vocab_size, n_embd),  <span class="cmt"># 27×16 = 432</span>
}
<span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(n_layer):
    state_dict[<span class="str">f'layer{i}.attn_wq'</span>] = <span class="fn">matrix</span>(n_embd, n_embd)    <span class="cmt"># 16×16</span>
    state_dict[<span class="str">f'layer{i}.attn_wk'</span>] = <span class="fn">matrix</span>(n_embd, n_embd)
    state_dict[<span class="str">f'layer{i}.attn_wv'</span>] = <span class="fn">matrix</span>(n_embd, n_embd)
    state_dict[<span class="str">f'layer{i}.attn_wo'</span>] = <span class="fn">matrix</span>(n_embd, n_embd)
    state_dict[<span class="str">f'layer{i}.mlp_fc1'</span>] = <span class="fn">matrix</span>(<span class="num">4</span>*n_embd, n_embd)  <span class="cmt"># 64×16</span>
    state_dict[<span class="str">f'layer{i}.mlp_fc2'</span>] = <span class="fn">matrix</span>(n_embd, <span class="num">4</span>*n_embd)  <span class="cmt"># 16×64</span>

params = [p <span class="kw">for</span> mat <span class="kw">in</span> state_dict.<span class="fn">values</span>() <span class="kw">for</span> row <span class="kw">in</span> mat <span class="kw">for</span> p <span class="kw">in</span> row]
<span class="cmt"># len(params) == 4192</span></code></pre>

  <h3 id="std-rationale">std=0.08 선택 이유 — Xavier vs He vs microgpt</h3>
  <pre><code><span class="cmt"># 표준 초기화 비교 (n_in=16, n_out=16인 경우)</span>

<span class="cmt"># Xavier (Glorot) 초기화: 순전파/역전파 분산 균형</span>
<span class="cmt"># std = sqrt(2 / (n_in + n_out)) = sqrt(2/32) ≈ 0.25</span>

<span class="cmt"># He (Kaiming) 초기화: ReLU 네트워크에 최적화</span>
<span class="cmt"># std = sqrt(2 / n_in) = sqrt(2/16) ≈ 0.35</span>

<span class="cmt"># microgpt 선택: std = 0.08 (더 작게 시작)</span>
<span class="cmt"># 이유: bias 없는 네트워크는 초기값이 너무 크면</span>
<span class="cmt">#       softmax가 포화(saturation)되기 쉬움.</span>
<span class="cmt">#       작은 초기값으로 시작해 부드럽게 학습.</span>
<span class="cmt">#       또한 200줄의 단순 모델에서 안정적 수렴을 위해.</span></code></pre>

  <table>
    <thead><tr><th>초기화 방법</th><th>std 값</th><th>대상 활성화</th><th>단점</th></tr></thead>
    <tbody>
      <tr><td>Xavier</td><td>≈ 0.25</td><td>sigmoid, tanh</td><td>ReLU에는 부적합</td></tr>
      <tr><td>He (Kaiming)</td><td>≈ 0.35</td><td>ReLU 계열</td><td>bias 없으면 초기 포화 위험</td></tr>
      <tr><td>microgpt</td><td>0.08 (고정)</td><td>ReLU</td><td>단순하고 작은 모델에 최적</td></tr>
    </tbody>
  </table>

  <div class="info-box info">
    <strong>왜 0으로 초기화하면 안 되는가?</strong> 모든 파라미터가 0이면 순전파 결과가 전부 0이 되고,
    역전파 시 모든 뉴런이 동일한 기울기를 받아 영원히 0에서 벗어나지 못합니다(대칭성 문제).
    가우시안으로 초기화하면 각 파라미터가 다른 값을 가져 각자 다른 특징을 학습합니다.
  </div>
</section>

<section class="content-section">
  <h2 id="embedding">임베딩 &amp; 핵심 헬퍼 함수</h2>

  <h3 id="output-layer">출력 레이어 구조 — lm_head</h3>
  <p>최종 출력은 두 단계입니다: <code>lm_head</code> linear 변환 후 Softmax로 확률로 변환.</p>
  <pre><code><span class="cmt"># hidden state x (16차원) → linear(lm_head) → softmax → 확률</span>
<span class="cmt"># 16차원 → 27개 logit (어휘집 크기)</span>
<span class="cmt"># logit → 합이 1인 확률 분포</span>

logits = <span class="fn">linear</span>(x, state_dict[<span class="str">'lm_head'</span>])  <span class="cmt"># 16 → 27</span>
probs  = <span class="fn">softmax</span>(logits)                   <span class="cmt"># 27개 확률, 합=1</span>

<span class="cmt"># Softmax 강화 효과:</span>
<span class="cmt"># z = [10, 12, 8] → Softmax(z) ≈ [12%, 87%, 2%]</span>
<span class="cmt"># 입력 차이가 작아도 Softmax는 승자를 확실히 가림</span>
<span class="cmt"># 10과 12의 차이는 작지만, 87%라는 압도적 확률로 12가 선택됨</span></code></pre>

  <h3 id="helper-funcs">미니 PyTorch — 3가지 헬퍼 함수</h3>
  <pre><code><span class="cmt"># ① linear: 행렬-벡터 곱셈 (bias 없음)</span>
<span class="cmt"># x: [n_in]  →  return: [n_out]</span>
<span class="kw">def</span> <span class="fn">linear</span>(x, w):
    <span class="kw">return</span> [<span class="fn">sum</span>(wi * xi <span class="kw">for</span> wi, xi <span class="kw">in</span> <span class="fn">zip</span>(wo, x)) <span class="kw">for</span> wo <span class="kw">in</span> w]

<span class="cmt"># ② softmax: 로짓 → 확률 분포 (수치 안정성을 위해 max 차감)</span>
<span class="kw">def</span> <span class="fn">softmax</span>(logits):
    max_val = <span class="fn">max</span>(val.data <span class="kw">for</span> val <span class="kw">in</span> logits)   <span class="cmt"># overflow 방지</span>
    exps = [(val - max_val).<span class="fn">exp</span>() <span class="kw">for</span> val <span class="kw">in</span> logits]
    total = <span class="fn">sum</span>(exps)
    <span class="kw">return</span> [e / total <span class="kw">for</span> e <span class="kw">in</span> exps]

<span class="cmt"># ③ rmsnorm: 평균 없이 RMS로만 정규화 (학습 가능 파라미터 없음)</span>
<span class="kw">def</span> <span class="fn">rmsnorm</span>(x):
    ms = <span class="fn">sum</span>(xi * xi <span class="kw">for</span> xi <span class="kw">in</span> x) / <span class="fn">len</span>(x)    <span class="cmt"># 평균 제곱</span>
    scale = (ms + <span class="num">1e-5</span>) ** -<span class="num">0.5</span>               <span class="cmt"># 역수 RMS</span>
    <span class="kw">return</span> [xi * scale <span class="kw">for</span> xi <span class="kw">in</span> x]</code></pre>

  <dl>
    <dt><code>linear()</code> — Wx (bias 없음)</dt>
    <dd>행·열 내적으로 차원 변환. PyTorch의 <code>nn.Linear(bias=False)</code>에 해당.</dd>
    <dt><code>softmax()</code> — e<sup>x</sup> / Σe<sup>x</sup></dt>
    <dd>로짓을 확률로 변환. <code>max_val</code> 차감은 수치 안정성을 위한 필수 트릭 — <code>e^(큰 수)</code>의 overflow를 방지합니다.</dd>
    <dt><code>rmsnorm()</code> — x / √(mean(x²) + ε)</dt>
    <dd>RMS로만 정규화. <strong>학습 가능한 γ(scale) 파라미터가 없습니다.</strong>
    LLaMA 등 프로덕션 모델의 RMSNorm과 달리, microgpt는 정규화만 수행합니다.
    ε=1e-5로 수치 안정성 확보.</dd>
  </dl>

  <h3 id="token-pos-emb">토큰 + 위치 임베딩</h3>
  <p><strong>wte (토큰 임베딩):</strong> "이 문자가 무엇인가" — 문자의 정체성. 같은 문자는 항상 같은 임베딩 행을 가져옵니다.
  학습 후 모음('a','e','i','o','u')들은 벡터 공간에서 서로 가까워질 것입니다.</p>
  <p><strong>wpe (위치 임베딩):</strong> "이 문자가 몇 번째 위치에 있는가" — 어텐션은 위치를 모르므로(내적 연산은 순서가 없음)
  위치 임베딩을 더함으로써 모델이 "첫 번째 글자", "마지막 글자" 같은 위치 패턴을 학습합니다.</p>

  <pre><code><span class="cmt"># "emma"의 BOS 처리 예시 (token_id=26, pos_id=0)</span>
tok_emb = state_dict[<span class="str">'wte'</span>][<span class="num">26</span>]  <span class="cmt"># BOS 임베딩 → [0.03, -0.07, 0.11, ...] (16차원)</span>
pos_emb = state_dict[<span class="str">'wpe'</span>][<span class="num">0</span>]   <span class="cmt"># 위치 0 임베딩 → [0.05, 0.02, -0.08, ...]</span>
x = [t + p <span class="kw">for</span> t, p <span class="kw">in</span> <span class="fn">zip</span>(tok_emb, pos_emb)]  <span class="cmt"># 원소별 덧셈 → 16차원</span>
x = <span class="fn">rmsnorm</span>(x)  <span class="cmt"># 크기 정규화</span>

<span class="cmt"># 왜 concatenate가 아닌 add인가?</span>
<span class="cmt"># concat → 32차원 → 파라미터 2배 (wq, wk, wv도 32×32 필요)</span>
<span class="cmt"># add   → 16차원 유지, 두 정보를 같은 공간에서 합산, 파라미터 절약</span>
<span class="cmt"># 이론: 두 임베딩이 같은 공간(16차원)에서 "어떤 문자" + "어느 위치" 정보를 합산</span></code></pre>
</section>

<section class="content-section">
  <h2 id="attention">Attention = 내부 검색 엔진</h2>
  <p>어텐션은 <em>"현재 내가 처리 중인 토큰과 관련이 있는 이전 토큰들을 찾아라"</em>는 <strong>내부 검색 엔진</strong>입니다.</p>

  <dl>
    <dt>Query (Q) — 검색어</dt>
    <dd>"나는 어떤 정보를 찾고 있나?" — 현재 처리 중인 토큰이 묻는 질문</dd>

    <dt>Key (K) — 색인</dt>
    <dd>"나는 어떤 정보를 가지고 있나?" — 각 토큰이 제공하는 레이블</dd>

    <dt>Value (V) — 내용물</dt>
    <dd>"매칭 시 실제로 전달할 정보" — 검색 결과</dd>
  </dl>

  <pre><code><span class="cmt"># Q, K, V 행렬을 각각 가중치로 선형 변환</span>
q = <span class="fn">linear</span>(x, state_dict[<span class="str">f'layer{li}.attn_wq'</span>])  <span class="cmt"># 질의: "나는 무엇을 찾나?"</span>
k = <span class="fn">linear</span>(x, state_dict[<span class="str">f'layer{li}.attn_wk'</span>])  <span class="cmt"># 키:   "나는 무엇을 가졌나?"</span>
v = <span class="fn">linear</span>(x, state_dict[<span class="str">f'layer{li}.attn_wv'</span>])  <span class="cmt"># 값:   "매칭 시 무엇을 줄까?"</span></code></pre>

  <h3 id="scaled-dot-product">Scaled Dot-Product — 왜 √head_dim으로 나누나?</h3>
  <pre><code><span class="cmt"># score = Q · K / √head_dim</span>
<span class="cmt"># head_dim = 4인 경우</span>
<span class="cmt"># Q = [0.1, -0.3, 0.5, 0.2]   (현재 토큰의 Query 벡터)</span>
<span class="cmt"># K = [0.4, 0.2, -0.1, 0.3]   (과거 토큰의 Key 벡터)</span>
<span class="cmt"># 내적 = 0.1×0.4 + (-0.3)×0.2 + 0.5×(-0.1) + 0.2×0.3</span>
<span class="cmt">#      = 0.04 - 0.06 - 0.05 + 0.06 = -0.01</span>
<span class="cmt"># 스케일: -0.01 / sqrt(4) = -0.01 / 2 = -0.005</span></code></pre>

  <div class="info-box info">
    <strong>스케일링의 수학적 이유:</strong> head_dim 차원의 벡터를 랜덤 초기화(N(0,1))하면 내적의 분산은 head_dim에 비례합니다.
    head_dim=4이면 분산≈4 (표준편차≈2), head_dim=64이면 분산≈64 (표준편차≈8).
    Softmax에 큰 값이 들어가면 한 토큰에 100% 집중되는 "포화(saturation)"가 발생합니다.
    √head_dim으로 나누면 내적의 분산을 1로 정규화하여 softmax가 균형 있는 가중치를 생성합니다.
  </div>

  <h3 id="multi-head">Multi-head Attention — 왜 여러 헤드가 필요한가?</h3>
  <pre><code><span class="cmt"># head_dim = n_embd // n_head = 16 // 4 = 4</span>
<span class="kw">for</span> h <span class="kw">in</span> <span class="fn">range</span>(n_head):        <span class="cmt"># h = 0, 1, 2, 3</span>
    hs = h * head_dim          <span class="cmt"># 헤드 시작 인덱스: 0, 4, 8, 12</span>
    q_h = q[hs:hs+head_dim]    <span class="cmt"># 각 헤드의 Query 4차원 슬라이스</span>
    k_h = [ki[hs:hs+head_dim] <span class="kw">for</span> ki <span class="kw">in</span> keys[li]]   <span class="cmt"># 각 헤드의 Key [T, 4]</span>
    v_h = [vi[hs:hs+head_dim] <span class="kw">for</span> vi <span class="kw">in</span> values[li]]  <span class="cmt"># 각 헤드의 Value [T, 4]</span></code></pre>

  <p><strong>각 헤드가 서로 다른 패턴을 학습합니다:</strong></p>
  <ul>
    <li>헤드 0: "첫 글자 패턴" — BOS 다음에 어떤 자음이 자주 오는가</li>
    <li>헤드 1: "모음 연속 패턴" — 모음 뒤에 모음이 오는지 자음이 오는지</li>
    <li>헤드 2: "어미 패턴" — 이름 끝부분의 통계적 규칙</li>
    <li>헤드 3: "위치 정보" — 지금 시퀀스의 어느 위치인가</li>
  </ul>
  <div class="info-box tip">
    실제로 어떤 패턴을 학습하는지는 학습 후 어텐션 가중치를 시각화해야 확인할 수 있습니다.
    "각 헤드가 다른 것을 학습한다"는 것이 Multi-head Attention의 핵심 아이디어입니다.
  </div>

  <div class="diagram-container">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 700 230"
         style="width:100%;max-width:700px;display:block;margin:1.5em auto;background:var(--diagram-fill);border-radius:8px;padding:16px;">
      <defs>
        <marker id="microgpt-mha-arrow" viewBox="0 0 10 7" refX="10" refY="3.5"
                markerWidth="7" markerHeight="7" orient="auto">
          <polygon points="0 0, 10 3.5, 0 7" fill="var(--diagram-arrow)"/>
        </marker>
      </defs>
      <!-- 입력 임베딩 16차원 -->
      <rect x="8" y="85" width="100" height="60" rx="6"
            fill="var(--bg-secondary)" stroke="var(--accent-primary)" stroke-width="2"/>
      <text x="58" y="107" text-anchor="middle" fill="var(--accent-primary)" font-size="10" font-weight="bold">임베딩 x</text>
      <text x="58" y="124" text-anchor="middle" fill="var(--diagram-text)" font-size="11">16차원</text>
      <text x="58" y="139" text-anchor="middle" fill="var(--text-muted)" font-size="9">n_embd=16</text>
      <!-- 분기 화살표 -->
      <line x1="108" y1="115" x2="145" y2="45" stroke="var(--diagram-arrow)" stroke-width="1.2" marker-end="url(#microgpt-mha-arrow)"/>
      <line x1="108" y1="115" x2="145" y2="90" stroke="var(--diagram-arrow)" stroke-width="1.2" marker-end="url(#microgpt-mha-arrow)"/>
      <line x1="108" y1="115" x2="145" y2="135" stroke="var(--diagram-arrow)" stroke-width="1.2" marker-end="url(#microgpt-mha-arrow)"/>
      <line x1="108" y1="115" x2="145" y2="180" stroke="var(--diagram-arrow)" stroke-width="1.2" marker-end="url(#microgpt-mha-arrow)"/>
      <!-- Head 0 -->
      <rect x="147" y="20" width="140" height="50" rx="5"
            fill="var(--bg-secondary)" stroke="var(--accent-primary)" stroke-width="1.5"/>
      <text x="217" y="40" text-anchor="middle" fill="var(--accent-primary)" font-size="10" font-weight="bold">헤드 0 (dim 0~3)</text>
      <text x="217" y="57" text-anchor="middle" fill="var(--diagram-text)" font-size="9">Q·K·V 각 4차원 → head_out[0:4]</text>
      <!-- Head 1 -->
      <rect x="147" y="77" width="140" height="50" rx="5"
            fill="var(--bg-secondary)" stroke="var(--accent-secondary)" stroke-width="1.5"/>
      <text x="217" y="97" text-anchor="middle" fill="var(--accent-secondary)" font-size="10" font-weight="bold">헤드 1 (dim 4~7)</text>
      <text x="217" y="114" text-anchor="middle" fill="var(--diagram-text)" font-size="9">Q·K·V 각 4차원 → head_out[4:8]</text>
      <!-- Head 2 -->
      <rect x="147" y="112" width="140" height="50" rx="5"
            fill="var(--bg-secondary)" stroke="var(--border-color)" stroke-width="1.5"/>
      <text x="217" y="132" text-anchor="middle" fill="var(--diagram-text)" font-size="10" font-weight="bold">헤드 2 (dim 8~11)</text>
      <text x="217" y="149" text-anchor="middle" fill="var(--diagram-text)" font-size="9">Q·K·V 각 4차원 → head_out[8:12]</text>
      <!-- Head 3 -->
      <rect x="147" y="157" width="140" height="50" rx="5"
            fill="var(--bg-secondary)" stroke="var(--border-color)" stroke-width="1.5"/>
      <text x="217" y="177" text-anchor="middle" fill="var(--diagram-text)" font-size="10" font-weight="bold">헤드 3 (dim 12~15)</text>
      <text x="217" y="194" text-anchor="middle" fill="var(--diagram-text)" font-size="9">Q·K·V 각 4차원 → head_out[12:16]</text>
      <!-- 합치기 화살표 -->
      <line x1="287" y1="45" x2="410" y2="105" stroke="var(--diagram-arrow)" stroke-width="1.2" marker-end="url(#microgpt-mha-arrow)"/>
      <line x1="287" y1="102" x2="410" y2="110" stroke="var(--diagram-arrow)" stroke-width="1.2" marker-end="url(#microgpt-mha-arrow)"/>
      <line x1="287" y1="137" x2="410" y2="117" stroke="var(--diagram-arrow)" stroke-width="1.2" marker-end="url(#microgpt-mha-arrow)"/>
      <line x1="287" y1="182" x2="410" y2="125" stroke="var(--diagram-arrow)" stroke-width="1.2" marker-end="url(#microgpt-mha-arrow)"/>
      <!-- Concat 결과 -->
      <rect x="412" y="75" width="130" height="70" rx="6"
            fill="var(--bg-secondary)" stroke="var(--accent-primary)" stroke-width="2"/>
      <text x="477" y="98" text-anchor="middle" fill="var(--accent-primary)" font-size="10" font-weight="bold">Concat</text>
      <text x="477" y="116" text-anchor="middle" fill="var(--diagram-text)" font-size="11">x_attn 16차원</text>
      <text x="477" y="133" text-anchor="middle" fill="var(--text-muted)" font-size="9">4헤드 × 4차원</text>
      <!-- 출력 프로젝션 화살표 -->
      <line x1="542" y1="110" x2="568" y2="110" stroke="var(--diagram-arrow)" stroke-width="1.5" marker-end="url(#microgpt-mha-arrow)"/>
      <!-- 출력 프로젝션 -->
      <rect x="570" y="75" width="120" height="70" rx="6"
            fill="var(--bg-secondary)" stroke="var(--border-color)" stroke-width="1.5"/>
      <text x="630" y="98" text-anchor="middle" fill="var(--diagram-text)" font-size="10" font-weight="bold">attn_proj</text>
      <text x="630" y="116" text-anchor="middle" fill="var(--diagram-text)" font-size="11">16차원 출력</text>
      <text x="630" y="133" text-anchor="middle" fill="var(--text-muted)" font-size="9">linear(x_attn)</text>
    </svg>
    <p class="diagram-caption">Multi-head Attention: n_embd=16을 4개 헤드가 4차원씩 분담 → 각각 독립적 패턴 학습 → Concat → 선형 변환</p>
  </div>

  <h3 id="emma-trace">"emma" — 어텐션 동작 단계별 추적 (pos=4)</h3>
  <pre><code><span class="cmt"># 입력 토큰 시퀀스: [BOS(26), e(4), m(12), m(12), a(0)]</span>
<span class="cmt"># pos_id:           [ 0,      1,    2,      3,      4  ]</span>

<span class="cmt"># pos=4, token='a' 처리 시:</span>
<span class="cmt">#   keys[0]   = [K_BOS, K_e, K_m, K_m, K_a]  (5개 누적)</span>
<span class="cmt">#   values[0] = [V_BOS, V_e, V_m, V_m, V_a]</span>
<span class="cmt">#</span>
<span class="cmt">#   헤드 h=0 (4차원):</span>
<span class="cmt">#     Q = q[0:4]      ← 'a'의 Query 벡터</span>
<span class="cmt">#</span>
<span class="cmt">#     scores = [</span>
<span class="cmt">#       dot(Q, K_BOS) / 2,   # BOS와의 연관성</span>
<span class="cmt">#       dot(Q, K_e)   / 2,   # 'e'와의 연관성</span>
<span class="cmt">#       dot(Q, K_m)   / 2,   # 첫 번째 'm'과의 연관성</span>
<span class="cmt">#       dot(Q, K_m)   / 2,   # 두 번째 'm'과의 연관성</span>
<span class="cmt">#       dot(Q, K_a)   / 2,   # 'a' 자신과의 연관성 (현재 위치도 포함!)</span>
<span class="cmt">#     ]</span>
<span class="cmt">#</span>
<span class="cmt">#     weights = softmax(scores)</span>
<span class="cmt">#             = [0.05, 0.20, 0.35, 0.30, 0.10]  ← 가중치 합=1.0</span>
<span class="cmt">#             (두 'm'에 0.65 집중 → 'emma'에서 'mm' 패턴 주목)</span>
<span class="cmt">#</span>
<span class="cmt">#     head_out = 0.05×V_BOS + 0.20×V_e + 0.35×V_m + 0.30×V_m + 0.10×V_a</span>
<span class="cmt">#              = 가중 평균 벡터 (4차원)</span>

<span class="cmt"># 4개 헤드 각각 head_out 4차원 계산 → x_attn.extend(head_out)</span>
<span class="cmt"># 결과: x_attn = [h0의 4차원, h1의 4차원, h2의 4차원, h3의 4차원] = 16차원</span></code></pre>

  <h3 id="causal-masking">인과적 마스킹 (Causal Masking) — 미래를 못 보게 하기</h3>
  <p>GPT는 자기회귀(autoregressive) 모델 — 미래 토큰을 보면 안 됩니다.
  GPT-2 원본에서는 어텐션 행렬에 "-∞" 마스크를 명시적으로 적용합니다.
  microgpt에서는 이것이 <strong>자동으로 처리</strong>됩니다:</p>
  <pre><code><span class="cmt"># 학습 시: pos_id=2를 처리할 때</span>
<span class="cmt"># keys[li]에는 이미 [K_pos0, K_pos1, K_pos2]만 있습니다.</span>
<span class="cmt"># K_pos3, K_pos4는 아직 append되지 않았습니다.</span>
<span class="cmt"># → 명시적 마스킹 없이도 미래 정보를 볼 수 없습니다!</span>
<span class="kw">for</span> pos_id <span class="kw">in</span> <span class="fn">range</span>(n):
    logits = <span class="fn">gpt</span>(token_id, pos_id, keys, values)
    <span class="cmt"># 내부에서 keys[li].append(k)가 실행됨</span>
    <span class="cmt"># 다음 pos_id 처리 시 현재 K도 포함됨</span></code></pre>
  <div class="info-box tip">
    이것이 microgpt가 Transformer 원본보다 단순한 이유 중 하나입니다.
    KV 캐시의 순차적 축적이 자동으로 인과적 마스킹을 구현합니다.
  </div>

  <h3 id="kv-cache">KV Cache — 추론 속도의 비밀</h3>
  <pre><code><span class="cmt"># 추론/학습 시작 시 빈 KV 캐시 초기화</span>
keys   = [[] <span class="kw">for</span> _ <span class="kw">in</span> <span class="fn">range</span>(n_layer)]   <span class="cmt"># 레이어별 Key 리스트</span>
values = [[] <span class="kw">for</span> _ <span class="kw">in</span> <span class="fn">range</span>(n_layer)]   <span class="cmt"># 레이어별 Value 리스트</span>

<span class="cmt"># gpt() 내부: 이번 토큰의 K,V를 캐시에 누적</span>
keys[li].<span class="fn">append</span>(k)    <span class="cmt"># k = linear(x, attn_wk), 모든 헤드 포함 [n_embd]</span>
values[li].<span class="fn">append</span>(v)  <span class="cmt"># v = linear(x, attn_wv)</span>

<span class="cmt"># 어텐션 계산: 과거 모든 K,V를 헤드별로 분리해서 사용</span>
k_h = [ki[hs:hs+head_dim] <span class="kw">for</span> ki <span class="kw">in</span> keys[li]]    <span class="cmt"># T개 과거 Key 헤드</span>
v_h = [vi[hs:hs+head_dim] <span class="kw">for</span> vi <span class="kw">in</span> values[li]]   <span class="cmt"># T개 과거 Value 헤드</span>
attn_logits = [
    <span class="fn">sum</span>(q_h[j] * k_h[t][j] <span class="kw">for</span> j <span class="kw">in</span> <span class="fn">range</span>(head_dim)) / head_dim**<span class="num">0.5</span>
    <span class="kw">for</span> t <span class="kw">in</span> <span class="fn">range</span>(<span class="fn">len</span>(k_h))   <span class="cmt"># 현재까지 누적된 토큰 수만큼</span>
]</code></pre>

  <div class="info-box info">
    <strong>KV Cache의 효과:</strong> 각 추론 스텝마다 이미 계산한 K, V를 다시 계산하지 않고 재사용합니다.
    O(T²) 반복 계산 → O(T) 증가분 계산. GPT-4 같은 대형 모델에서는 이 캐시가 추론 속도를 수십 배 높여줍니다.
  </div>

  <div class="diagram-container">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 700 180"
         style="width:100%;max-width:700px;display:block;margin:1.5em auto;background:var(--diagram-fill);border-radius:8px;padding:16px;">
      <defs>
        <marker id="microgpt-kv-arrow" viewBox="0 0 10 7" refX="10" refY="3.5"
                markerWidth="8" markerHeight="7" orient="auto">
          <polygon points="0 0, 10 3.5, 0 7" fill="var(--diagram-arrow)"/>
        </marker>
      </defs>
      <!-- 타이틀 -->
      <text x="350" y="18" text-anchor="middle" fill="var(--diagram-text)" font-size="11" font-weight="bold">KV 캐시 순차 누적 (추론 시, "kamon" 생성)</text>
      <!-- Step 0 열 -->
      <rect x="10" y="28" width="195" height="130" rx="6"
            fill="var(--bg-secondary)" stroke="var(--border-color)" stroke-width="1.5"/>
      <text x="107" y="46" text-anchor="middle" fill="var(--accent-primary)" font-size="10" font-weight="bold">Step 0 (pos=0)</text>
      <text x="107" y="63" text-anchor="middle" fill="var(--diagram-text)" font-size="10">입력: BOS(26)</text>
      <rect x="25" y="70" width="165" height="24" rx="4"
            fill="var(--accent-primary)" fill-opacity="0.15" stroke="var(--accent-primary)" stroke-width="1"/>
      <text x="107" y="86" text-anchor="middle" fill="var(--accent-primary)" font-size="10" font-family="monospace">keys = [K_BOS]</text>
      <text x="107" y="112" text-anchor="middle" fill="var(--text-muted)" font-size="9">→ Attention 범위: BOS만</text>
      <text x="107" y="128" text-anchor="middle" fill="var(--text-muted)" font-size="9">→ 샘플링: 'k'(10)</text>
      <text x="107" y="148" text-anchor="middle" fill="var(--text-muted)" font-size="9">미래 정보 없음 (자동 마스킹)</text>
      <!-- Arrow 0→1 -->
      <line x1="205" y1="92" x2="225" y2="92" stroke="var(--diagram-arrow)" stroke-width="1.5" marker-end="url(#microgpt-kv-arrow)"/>
      <!-- Step 1 열 -->
      <rect x="227" y="28" width="210" height="130" rx="6"
            fill="var(--bg-secondary)" stroke="var(--border-color)" stroke-width="1.5"/>
      <text x="332" y="46" text-anchor="middle" fill="var(--accent-secondary)" font-size="10" font-weight="bold">Step 1 (pos=1)</text>
      <text x="332" y="63" text-anchor="middle" fill="var(--diagram-text)" font-size="10">입력: 'k'(10)</text>
      <rect x="242" y="70" width="180" height="24" rx="4"
            fill="var(--accent-secondary)" fill-opacity="0.15" stroke="var(--accent-secondary)" stroke-width="1"/>
      <text x="332" y="86" text-anchor="middle" fill="var(--accent-secondary)" font-size="10" font-family="monospace">keys = [K_BOS, K_k]</text>
      <text x="332" y="112" text-anchor="middle" fill="var(--text-muted)" font-size="9">→ Attention 범위: BOS + k</text>
      <text x="332" y="128" text-anchor="middle" fill="var(--text-muted)" font-size="9">→ 샘플링: 'a'(0)</text>
      <text x="332" y="148" text-anchor="middle" fill="var(--text-muted)" font-size="9">K_BOS 재계산 없이 재사용</text>
      <!-- Arrow 1→2 -->
      <line x1="437" y1="92" x2="457" y2="92" stroke="var(--diagram-arrow)" stroke-width="1.5" marker-end="url(#microgpt-kv-arrow)"/>
      <!-- Step 2 열 -->
      <rect x="459" y="28" width="230" height="130" rx="6"
            fill="var(--bg-secondary)" stroke="var(--border-color)" stroke-width="1.5"/>
      <text x="574" y="46" text-anchor="middle" fill="var(--diagram-text)" font-size="10" font-weight="bold">Step 2 (pos=2)</text>
      <text x="574" y="63" text-anchor="middle" fill="var(--diagram-text)" font-size="10">입력: 'a'(0)</text>
      <rect x="474" y="70" width="200" height="24" rx="4"
            fill="var(--bg-secondary)" stroke="var(--border-color)" stroke-width="1"/>
      <text x="574" y="86" text-anchor="middle" fill="var(--diagram-text)" font-size="10" font-family="monospace">keys = [K_BOS, K_k, K_a]</text>
      <text x="574" y="112" text-anchor="middle" fill="var(--text-muted)" font-size="9">→ Attention 범위: BOS+k+a</text>
      <text x="574" y="128" text-anchor="middle" fill="var(--text-muted)" font-size="9">→ 다음: 'm'(12)</text>
      <text x="574" y="148" text-anchor="middle" fill="var(--text-muted)" font-size="9">K_BOS, K_k 모두 재사용</text>
    </svg>
    <p class="diagram-caption">KV 캐시 누적: 각 스텝마다 새 K·V만 추가 — 기존 K·V 재계산 없이 재사용. 인과적 마스킹도 자동 구현.</p>
  </div>

  <h3 id="residual-connection">잔차 연결 (Residual Connection) — 잊어버리지 않기</h3>
  <pre><code><span class="cmt"># 잔차 연결 패턴 (Attention 블록 예시)</span>
x_residual = x               <span class="cmt"># ① 원본 x 저장</span>
x = <span class="fn">rmsnorm</span>(x)              <span class="cmt"># ② 정규화</span>
x = attention(x)  <span class="cmt"># ...</span>   <span class="cmt"># ③ Attention 처리</span>
x = [a + b <span class="kw">for</span> a, b <span class="kw">in</span> <span class="fn">zip</span>(x, x_residual)]  <span class="cmt"># ④ 원본 + 결과</span></code></pre>

  <p><strong>잔차 연결을 빠뜨리면?</strong></p>
  <p>모델이 <em>"나는 B라는 글자를 읽고 있었지"</em>라는 사실을 잊어버리고,
  <em>"나는 A와 관련이 있어"</em>라는 어텐션 출력만 기억하게 됩니다.
  우리는 원본 정보(Content)와 새로운 컨텍스트(Context) 둘 다 필요합니다.<br>
  <code>x = x + attention_output</code> — 이 한 줄이 핵심입니다.</p>

  <p><strong>잔차 연결의 또 다른 효과:</strong></p>
  <ul>
    <li><strong>기울기 소실 방지</strong> — 깊은 네트워크에서 역전파 시 기울기가 층마다 곱해져 소실되는 문제를 해결합니다.
    잔차 경로를 통해 기울기가 직접 흐릅니다.</li>
    <li><strong>항등 함수 학습 용이</strong> — 레이어가 아무것도 하지 않아도 되면(정보 변환 불필요), 출력 = 0 + x_residual이 됩니다.</li>
    <li><strong>모든 현대 트랜스포머의 표준</strong> — GPT-2 이후 모든 대형 언어 모델이 채택</li>
  </ul>

  <div class="diagram-container">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 700 200"
         style="width:100%;max-width:700px;display:block;margin:1.5em auto;background:var(--diagram-fill);border-radius:8px;padding:20px;">
      <defs>
        <marker id="microgpt-arch-arrow" viewBox="0 0 10 7" refX="10" refY="3.5"
                markerWidth="10" markerHeight="7" orient="auto">
          <polygon points="0 0, 10 3.5, 0 7" fill="var(--diagram-arrow)"/>
        </marker>
      </defs>
      <rect x="10" y="75" width="100" height="50" rx="6"
            fill="var(--bg-secondary)" stroke="var(--border-color)" stroke-width="1.5"/>
      <text x="60" y="97" text-anchor="middle" fill="var(--diagram-text)" font-size="11">입력</text>
      <text x="60" y="113" text-anchor="middle" fill="var(--diagram-text)" font-size="11">임베딩</text>
      <rect x="160" y="30" width="70" height="40" rx="5"
            fill="var(--bg-secondary)" stroke="var(--accent-primary)" stroke-width="1.5"/>
      <text x="195" y="55" text-anchor="middle" fill="var(--diagram-text)" font-size="12">Q (쿼리)</text>
      <rect x="160" y="80" width="70" height="40" rx="5"
            fill="var(--bg-secondary)" stroke="var(--accent-secondary)" stroke-width="1.5"/>
      <text x="195" y="105" text-anchor="middle" fill="var(--diagram-text)" font-size="12">K (키)</text>
      <rect x="160" y="130" width="70" height="40" rx="5"
            fill="var(--bg-secondary)" stroke="var(--border-color)" stroke-width="1.5"/>
      <text x="195" y="155" text-anchor="middle" fill="var(--diagram-text)" font-size="12">V (값)</text>
      <rect x="290" y="50" width="120" height="50" rx="6"
            fill="var(--bg-secondary)" stroke="var(--border-color)" stroke-width="1.5"/>
      <text x="350" y="73" text-anchor="middle" fill="var(--diagram-text)" font-size="11">Q·Kᵀ / √d</text>
      <text x="350" y="89" text-anchor="middle" fill="var(--diagram-text)" font-size="11">→ softmax</text>
      <rect x="480" y="75" width="120" height="50" rx="6"
            fill="var(--bg-secondary)" stroke="var(--accent-primary)" stroke-width="1.5"/>
      <text x="540" y="98" text-anchor="middle" fill="var(--diagram-text)" font-size="11">Σ weights</text>
      <text x="540" y="114" text-anchor="middle" fill="var(--diagram-text)" font-size="11">× V → 출력</text>
      <line x1="110" y1="100" x2="158" y2="55" stroke="var(--diagram-arrow)" stroke-width="1.5" marker-end="url(#microgpt-arch-arrow)"/>
      <line x1="110" y1="100" x2="158" y2="100" stroke="var(--diagram-arrow)" stroke-width="1.5" marker-end="url(#microgpt-arch-arrow)"/>
      <line x1="110" y1="100" x2="158" y2="148" stroke="var(--diagram-arrow)" stroke-width="1.5" marker-end="url(#microgpt-arch-arrow)"/>
      <line x1="230" y1="50" x2="288" y2="72" stroke="var(--diagram-arrow)" stroke-width="1.5" marker-end="url(#microgpt-arch-arrow)"/>
      <line x1="230" y1="100" x2="288" y2="82" stroke="var(--diagram-arrow)" stroke-width="1.5" marker-end="url(#microgpt-arch-arrow)"/>
      <line x1="410" y1="85" x2="478" y2="90" stroke="var(--diagram-arrow)" stroke-width="1.5" marker-end="url(#microgpt-arch-arrow)"/>
      <line x1="230" y1="150" x2="478" y2="108" stroke="var(--diagram-arrow)" stroke-width="1.5" stroke-dasharray="4,3" marker-end="url(#microgpt-arch-arrow)"/>
    </svg>
    <p class="diagram-caption">Scaled Dot-Product Attention 흐름: Q·Kᵀ/√d → softmax → 가중합 × V</p>
  </div>
</section>

<section class="content-section">
  <h2 id="transformer-block">트랜스포머 블록</h2>
  <p>트랜스포머의 핵심 반복 단위입니다. 각 레이어는 <strong>Attention → MLP</strong> 순서로 처리합니다.</p>
  <p>입력 x → Attention(컨텍스트 수집) → + → MLP(의미 처리) → + → 출력</p>

  <div class="diagram-container">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 560 400"
         style="width:100%;max-width:560px;display:block;margin:1.5em auto;background:var(--diagram-fill);border-radius:8px;padding:16px;">
      <defs>
        <marker id="microgpt-block-arrow" viewBox="0 0 10 7" refX="10" refY="3.5"
                markerWidth="8" markerHeight="7" orient="auto">
          <polygon points="0 0, 10 3.5, 0 7" fill="var(--diagram-arrow)"/>
        </marker>
        <marker id="microgpt-block-arrow-dash" viewBox="0 0 10 7" refX="10" refY="3.5"
                markerWidth="8" markerHeight="7" orient="auto">
          <polygon points="0 0, 10 3.5, 0 7" fill="var(--accent-secondary)"/>
        </marker>
      </defs>
      <!-- 입력 x -->
      <rect x="185" y="10" width="120" height="40" rx="6"
            fill="var(--bg-secondary)" stroke="var(--accent-primary)" stroke-width="2"/>
      <text x="245" y="35" text-anchor="middle" fill="var(--accent-primary)" font-size="13" font-weight="bold">입력 x (16차원)</text>
      <!-- 화살표: 입력 → Pre-LN(Attn) -->
      <line x1="245" y1="50" x2="245" y2="68" stroke="var(--diagram-arrow)" stroke-width="1.5" marker-end="url(#microgpt-block-arrow)"/>
      <!-- 잔차 분기 왼쪽 라인 (입력 → 첫 + 노드) -->
      <line x1="195" y1="30" x2="110" y2="30" stroke="var(--accent-secondary)" stroke-width="1.5" stroke-dasharray="5,3"/>
      <line x1="110" y1="30" x2="110" y2="215" stroke="var(--accent-secondary)" stroke-width="1.5" stroke-dasharray="5,3"/>
      <line x1="110" y1="215" x2="175" y2="215" stroke="var(--accent-secondary)" stroke-width="1.5" stroke-dasharray="5,3" marker-end="url(#microgpt-block-arrow-dash)"/>
      <!-- Pre-LN (Attn) -->
      <rect x="185" y="70" width="120" height="40" rx="6"
            fill="var(--bg-secondary)" stroke="var(--border-color)" stroke-width="1.5"/>
      <text x="245" y="87" text-anchor="middle" fill="var(--diagram-text)" font-size="11">Pre-LN</text>
      <text x="245" y="102" text-anchor="middle" fill="var(--text-muted)" font-size="9">RMSNorm(x)</text>
      <!-- 화살표 -->
      <line x1="245" y1="110" x2="245" y2="128" stroke="var(--diagram-arrow)" stroke-width="1.5" marker-end="url(#microgpt-block-arrow)"/>
      <!-- Attention -->
      <rect x="165" y="130" width="160" height="40" rx="6"
            fill="var(--bg-secondary)" stroke="var(--accent-primary)" stroke-width="2"/>
      <text x="245" y="148" text-anchor="middle" fill="var(--accent-primary)" font-size="11" font-weight="bold">Multi-head Attention</text>
      <text x="245" y="163" text-anchor="middle" fill="var(--text-muted)" font-size="9">Q·K·V → 컨텍스트 수집</text>
      <!-- 화살표 -->
      <line x1="245" y1="170" x2="245" y2="188" stroke="var(--diagram-arrow)" stroke-width="1.5" marker-end="url(#microgpt-block-arrow)"/>
      <!-- 첫 번째 + 잔차 -->
      <circle cx="245" cy="210" r="18" fill="var(--bg-secondary)" stroke="var(--accent-secondary)" stroke-width="2"/>
      <text x="245" y="216" text-anchor="middle" fill="var(--accent-secondary)" font-size="16" font-weight="bold">+</text>
      <!-- 화살표: + → Pre-LN(MLP) -->
      <line x1="245" y1="228" x2="245" y2="248" stroke="var(--diagram-arrow)" stroke-width="1.5" marker-end="url(#microgpt-block-arrow)"/>
      <!-- 잔차 분기 오른쪽 라인 (첫 + → MLP 잔차 +) -->
      <line x1="263" y1="210" x2="420" y2="210" stroke="var(--accent-secondary)" stroke-width="1.5" stroke-dasharray="5,3"/>
      <line x1="420" y1="210" x2="420" y2="355" stroke="var(--accent-secondary)" stroke-width="1.5" stroke-dasharray="5,3"/>
      <line x1="420" y1="355" x2="310" y2="355" stroke="var(--accent-secondary)" stroke-width="1.5" stroke-dasharray="5,3" marker-end="url(#microgpt-block-arrow-dash)"/>
      <!-- Pre-LN (MLP) -->
      <rect x="185" y="250" width="120" height="40" rx="6"
            fill="var(--bg-secondary)" stroke="var(--border-color)" stroke-width="1.5"/>
      <text x="245" y="267" text-anchor="middle" fill="var(--diagram-text)" font-size="11">Pre-LN</text>
      <text x="245" y="282" text-anchor="middle" fill="var(--text-muted)" font-size="9">RMSNorm(x)</text>
      <!-- 화살표 -->
      <line x1="245" y1="290" x2="245" y2="308" stroke="var(--diagram-arrow)" stroke-width="1.5" marker-end="url(#microgpt-block-arrow)"/>
      <!-- MLP -->
      <rect x="165" y="310" width="160" height="40" rx="6"
            fill="var(--bg-secondary)" stroke="var(--border-color)" stroke-width="1.5"/>
      <text x="245" y="328" text-anchor="middle" fill="var(--diagram-text)" font-size="11" font-weight="bold">MLP (FC1→ReLU→FC2)</text>
      <text x="245" y="343" text-anchor="middle" fill="var(--text-muted)" font-size="9">16 → 64 → 16차원</text>
      <!-- 화살표 -->
      <line x1="245" y1="350" x2="245" y2="368" stroke="var(--diagram-arrow)" stroke-width="1.5" marker-end="url(#microgpt-block-arrow)"/>
      <!-- 두 번째 + 잔차 -->
      <circle cx="245" cy="370" r="18" fill="var(--bg-secondary)" stroke="var(--accent-secondary)" stroke-width="2"/>
      <text x="245" y="376" text-anchor="middle" fill="var(--accent-secondary)" font-size="16" font-weight="bold">+</text>
      <!-- 화살표: + → 출력 -->
      <line x1="245" y1="388" x2="245" y2="393" stroke="var(--diagram-arrow)" stroke-width="1.5" marker-end="url(#microgpt-block-arrow)"/>
      <!-- 출력 레이블 -->
      <text x="245" y="400" text-anchor="middle" fill="var(--accent-primary)" font-size="12" font-weight="bold">출력 x (16차원)</text>
      <!-- 잔차 연결 레이블 -->
      <text x="85" y="127" text-anchor="middle" fill="var(--accent-secondary)" font-size="9" font-style="italic">잔차 경로</text>
      <text x="447" y="285" text-anchor="middle" fill="var(--accent-secondary)" font-size="9" font-style="italic">잔차 경로</text>
    </svg>
    <p class="diagram-caption">트랜스포머 블록: Pre-LN → Multi-head Attention → 잔차합 → Pre-LN → MLP → 잔차합. 점선은 잔차(bypass) 경로.</p>
  </div>

  <h3 id="mlp">MLP — 비선형성과 4× 확장의 이유</h3>
  <pre><code><span class="cmt"># fc1: 16차원 → 64차원 (4× 확장)</span>
x = <span class="fn">linear</span>(x, state_dict[<span class="str">f'layer{li}.mlp_fc1'</span>])  <span class="cmt"># [n_embd] → [4*n_embd]</span>
x = [xi.<span class="fn">relu</span>() <span class="kw">for</span> xi <span class="kw">in</span> x]                       <span class="cmt"># 비선형성: max(0, x)</span>
<span class="cmt"># fc2: 64차원 → 16차원 (원상복구)</span>
x = <span class="fn">linear</span>(x, state_dict[<span class="str">f'layer{li}.mlp_fc2'</span>])  <span class="cmt"># [4*n_embd] → [n_embd]</span></code></pre>

  <p><strong>왜 4× 확장인가?</strong>
  이 "bottleneck" 구조(좁게 → 넓게 → 좁게)는 GPT-2 원논문에서 정의된 표준입니다.
  중간 64차원에서 더 많은 특징 조합을 만든 뒤, 다시 16차원으로 압축합니다.
  4× 비율은 실험적으로 좋은 성능을 보인 경험적 수치이며, GPT-4도 동일한 비율을 씁니다.</p>

  <div class="info-box info">
    <strong>Attention vs MLP의 역할 구분:</strong>
    Attention은 다른 토큰의 정보를 "수집(Gather)"합니다.
    MLP는 수집된 정보를 "처리(Process)"합니다.
    ReLU 없이 Attention + Linear만 있다면, 아무리 쌓아도 단 하나의 선형 변환과 수학적으로 동일합니다.
    ReLU가 추가하는 비선형성이 복잡한 패턴을 학습 가능하게 합니다.
  </div>

  <h3 id="gelu-relu">GELU → ReLU — 의도적 단순화</h3>
  <dl>
    <dt>GELU (표준, GPT-2/3) — x · Φ(x)</dt>
    <dd>정규분포의 누적분포함수 사용. 수식 복잡 (<code>0.5 × x × (1 + tanh(√(2/π) × (x + 0.044715x³)))</code>).
    확률론적 해석으로 더 부드러운 활성화. 하지만 순수 파이썬으로 구현하면 느리고 복잡합니다.</dd>

    <dt>ReLU (microgpt) — max(0, x)</dt>
    <dd>단순하지만 충분히 효과적. <code>xi.relu()</code> 단 한 줄. 순수 파이썬 구현에 최적.
    이름 생성이라는 단순한 태스크에는 ReLU도 충분합니다.</dd>
  </dl>

  <h3 id="pre-ln">Pre-LN vs Post-LN — microgpt의 선택</h3>
  <table>
    <thead><tr><th>구분</th><th>Post-LN (GPT-2 원본)</th><th>Pre-LN (microgpt)</th></tr></thead>
    <tbody>
      <tr><td>순서</td><td>x → Attention → x+residual → Norm</td><td>x → Norm → Attention → x+residual</td></tr>
      <tr><td>잔차 경로</td><td>정규화된 값이 잔차로 흐름</td><td>원본 x가 그대로 잔차로 흐름 ✓</td></tr>
      <tr><td>학습 안정성</td><td>레이어 수 많으면 불안정</td><td>더 안정적 (LLaMA, PaLM 채택)</td></tr>
      <tr><td>초기화 의존</td><td>높음</td><td>낮음</td></tr>
    </tbody>
  </table>

  <pre><code><span class="cmt"># microgpt의 Pre-LN 패턴:</span>
x_residual = x         <span class="cmt"># ← 정규화 전 원본 저장</span>
x = <span class="fn">rmsnorm</span>(x)         <span class="cmt"># ← 정규화 후 Attention/MLP에 입력</span>
x = attention(x)  <span class="cmt"># ...</span>
x = [a + b <span class="kw">for</span> a, b <span class="kw">in</span> <span class="fn">zip</span>(x, x_residual)]  <span class="cmt"># ← 원본 + 결과</span></code></pre>

  <h3 id="rmsnorm">RMSNorm vs LayerNorm</h3>
  <table>
    <thead><tr><th>항목</th><th>LayerNorm (기존)</th><th>RMSNorm (LLaMA 등)</th><th>RMSNorm (microgpt)</th></tr></thead>
    <tbody>
      <tr><td>수식</td><td>(x − μ) / √(σ² + ε)</td><td>x / √(RMS² + ε)</td><td>x / √(RMS² + ε)</td></tr>
      <tr><td>학습 파라미터</td><td>γ + β (2개)</td><td>γ만 (1개)</td><td><strong>없음</strong> (정규화만)</td></tr>
      <tr><td>계산 비용</td><td>평균·분산 계산</td><td>분산만 계산</td><td>분산만 계산 (γ 없음)</td></tr>
    </tbody>
  </table>
  <p>RMSNorm은 평균 계산이 필요 없어 LayerNorm 대비 <strong>7~64% 더 빠르고</strong>, 성능은 거의 동일합니다.</p>
</section>

<section class="content-section">
  <h2 id="gpt-pipeline">완전한 gpt() 함수 — GPT 파이프라인</h2>
  <pre><code><span class="kw">def</span> <span class="fn">gpt</span>(token_id, pos_id, keys, values):

    <span class="cmt"># ─── ① 임베딩: 토큰 + 위치 합산 ─────────────────────</span>
    tok_emb = state_dict[<span class="str">'wte'</span>][token_id]   <span class="cmt"># 16차원 벡터</span>
    pos_emb = state_dict[<span class="str">'wpe'</span>][pos_id]     <span class="cmt"># 16차원 벡터</span>
    x = [t + p <span class="kw">for</span> t, p <span class="kw">in</span> <span class="fn">zip</span>(tok_emb, pos_emb)]
    x = <span class="fn">rmsnorm</span>(x)  <span class="cmt"># ← 입력 정규화 (잔차 경로 때문에 필요)</span>

    <span class="cmt"># ─── ② 트랜스포머 레이어 × n_layer ──────────────────</span>
    <span class="kw">for</span> li <span class="kw">in</span> <span class="fn">range</span>(n_layer):

        <span class="cmt"># Attention Block: Pre-LN 방식</span>
        x_residual = x
        x = <span class="fn">rmsnorm</span>(x)
        q = <span class="fn">linear</span>(x, state_dict[<span class="str">f'layer{li}.attn_wq'</span>])    <span class="cmt"># Q: [n_embd]</span>
        k = <span class="fn">linear</span>(x, state_dict[<span class="str">f'layer{li}.attn_wk'</span>])    <span class="cmt"># K: [n_embd]</span>
        v = <span class="fn">linear</span>(x, state_dict[<span class="str">f'layer{li}.attn_wv'</span>])    <span class="cmt"># V: [n_embd]</span>
        keys[li].<span class="fn">append</span>(k)    <span class="cmt"># KV 캐시에 현재 토큰 저장</span>
        values[li].<span class="fn">append</span>(v)

        <span class="cmt"># 멀티헤드 어텐션 (헤드별로 분리 계산)</span>
        x_attn = []
        <span class="kw">for</span> h <span class="kw">in</span> <span class="fn">range</span>(n_head):
            hs = h * head_dim
            q_h = q[hs:hs+head_dim]                                   <span class="cmt"># [head_dim]</span>
            k_h = [ki[hs:hs+head_dim] <span class="kw">for</span> ki <span class="kw">in</span> keys[li]]             <span class="cmt"># [T, head_dim]</span>
            v_h = [vi[hs:hs+head_dim] <span class="kw">for</span> vi <span class="kw">in</span> values[li]]           <span class="cmt"># [T, head_dim]</span>
            <span class="cmt"># Scaled Dot-Product: Q·Kᵀ / √head_dim</span>
            attn_logits = [
                <span class="fn">sum</span>(q_h[j] * k_h[t][j] <span class="kw">for</span> j <span class="kw">in</span> <span class="fn">range</span>(head_dim)) / head_dim**<span class="num">0.5</span>
                <span class="kw">for</span> t <span class="kw">in</span> <span class="fn">range</span>(<span class="fn">len</span>(k_h))
            ]
            attn_weights = <span class="fn">softmax</span>(attn_logits)                        <span class="cmt"># [T]</span>
            <span class="cmt"># Weighted sum of Values</span>
            head_out = [
                <span class="fn">sum</span>(attn_weights[t] * v_h[t][j] <span class="kw">for</span> t <span class="kw">in</span> <span class="fn">range</span>(<span class="fn">len</span>(v_h)))
                <span class="kw">for</span> j <span class="kw">in</span> <span class="fn">range</span>(head_dim)
            ]
            x_attn.<span class="fn">extend</span>(head_out)

        <span class="cmt"># 출력 프로젝션 + 잔차 연결</span>
        x = <span class="fn">linear</span>(x_attn, state_dict[<span class="str">f'layer{li}.attn_wo'</span>])
        x = [a + b <span class="kw">for</span> a, b <span class="kw">in</span> <span class="fn">zip</span>(x, x_residual)]   <span class="cmt"># x = attn_out + x_residual</span>

        <span class="cmt"># MLP Block: Pre-LN 방식</span>
        x_residual = x
        x = <span class="fn">rmsnorm</span>(x)
        x = <span class="fn">linear</span>(x, state_dict[<span class="str">f'layer{li}.mlp_fc1'</span>])  <span class="cmt"># 16 → 64</span>
        x = [xi.<span class="fn">relu</span>() <span class="kw">for</span> xi <span class="kw">in</span> x]                      <span class="cmt"># 비선형성</span>
        x = <span class="fn">linear</span>(x, state_dict[<span class="str">f'layer{li}.mlp_fc2'</span>])  <span class="cmt"># 64 → 16</span>
        x = [a + b <span class="kw">for</span> a, b <span class="kw">in</span> <span class="fn">zip</span>(x, x_residual)]         <span class="cmt"># x = mlp_out + x_residual</span>

    <span class="cmt"># ─── ③ 언임베딩: 16차원 → 27개 로짓 ─────────────────</span>
    logits = <span class="fn">linear</span>(x, state_dict[<span class="str">'lm_head'</span>])
    <span class="kw">return</span> logits</code></pre>

  <div class="info-box tip">
    <strong>입력 rmsnorm의 비밀:</strong>
    <code>x = rmsnorm(x)</code>가 임베딩 직후 한 번 더 있습니다.
    이것은 Karpathy가 주석에서 언급한 부분 — "잔차 경로(residual stream) 덕분에 불필요하지 않다."
    잔차 연결은 원본 임베딩을 그대로 더하므로, 첫 번째 rmsnorm이 없으면
    비정규화된 임베딩이 그대로 잔차로 흘러들어갑니다.
    임베딩의 크기가 제각각이면 잔차 경로 전체가 불안정해집니다.
  </div>

  <p>이 함수 하나가 토큰 ID와 위치 ID를 받아 27개 어휘에 대한 확률 분포(로짓)를 반환합니다.</p>

  <h3 id="four-pillars">트랜스포머 4대 구성요소</h3>
  <ol>
    <li><strong>Attention</strong> — 컨텍스트 수집 (다른 토큰의 정보를 Gather)</li>
    <li><strong>MLP</strong> — 의미 처리 (수집된 정보를 Process)</li>
    <li><strong>잔차 연결(Residuals)</strong> — 안정적인 학습 + 기울기 소실 방지</li>
    <li><strong>RMSNorm</strong> — 연산 안정성 확보 (LayerNorm 대비 단순화)</li>
  </ol>
  <div class="info-box tip">
    이 네 가지 구성요소가 현대 모든 트랜스포머 모델(GPT-4, LLaMA, Gemini 등)의 기본 빌딩 블록입니다.
    규모(Scale)만 다를 뿐, 원리는 microgpt와 완전히 동일합니다.
  </div>
</section>

<!-- ===================================================== -->
<!-- 3부: 학습·Adam·추론·LLM 비교                          -->
<!-- ===================================================== -->

<section class="content-section">
  <h2 id="loss">Cross-Entropy 손실 계산</h2>
  <p>Cross-Entropy는 정보 이론에서 나온 개념입니다. 모델의 예측 분포 P가 정답에서 얼마나 멀리 있는지를 측정합니다.</p>

  <pre><code><span class="cmt"># 수식: L = -log(P(정답 토큰))</span>
<span class="cmt"># 코드: loss_t = -probs[target_id].log()</span>

<span class="cmt"># 직관: 정답 토큰에 높은 확률 부여 → log(높은 값) → 음수 작음 → loss 작음</span>
<span class="cmt"># P(target) = 0.9  → loss = -log(0.9) ≈ 0.105  (잘 맞춤)</span>
<span class="cmt"># P(target) = 0.5  → loss = -log(0.5) ≈ 0.693</span>
<span class="cmt"># P(target) = 0.1  → loss = -log(0.1) ≈ 2.303  (많이 틀림)</span>
<span class="cmt"># P(target) = 0.01 → loss = -log(0.01) ≈ 4.605 (매우 틀림)</span>

<span class="cmt"># 전체 이름에 대한 평균:</span>
<span class="cmt"># loss = (1/n) * Σ -log(P(tokens[pos+1] | tokens[:pos+1]))</span></code></pre>

  <h3 id="loss-examples">위치별 손실 계산 예시 — "ab" 이름</h3>
  <p>이름 "ab"를 BOS로 감싸면 토큰 시퀀스: <code>[BOS, a, b, BOS]</code></p>
  <table>
    <thead><tr><th>Pos</th><th>Input</th><th>Target</th><th>P(target) 예시</th><th>Loss ≈</th></tr></thead>
    <tbody>
      <tr><td>0</td><td>BOS</td><td>'a'</td><td>0.10</td><td>2.30</td></tr>
      <tr><td>1</td><td>'a'</td><td>'b'</td><td>0.50</td><td>0.69</td></tr>
      <tr><td>2</td><td>'b'</td><td>BOS</td><td>0.20</td><td>1.61</td></tr>
    </tbody>
    <tfoot>
      <tr><th>평균</th><td>—</td><td>—</td><td>—</td><th>L₀ = (2.30 + 0.69 + 1.61) / 3 ≈ 1.53</th></tr>
    </tfoot>
  </table>

  <div class="info-box info">
    <strong>학습 = 각 위치에서 다음 토큰을 예측하도록 훈련</strong>
    BOS → 첫 글자 예측. 각 글자 → 다음 글자 예측. 마지막 글자 → BOS(종료) 예측.
    위치별 cross-entropy를 전부 합산하고 평균을 냅니다.
  </div>

  <h3 id="baseline">랜덤 베이스라인 — 학습 시작점 3.30</h3>
  <pre><code><span class="cmt"># 완전 랜덤 모델: 27개 토큰에 균등 확률 1/27 ≈ 0.037</span>
<span class="cmt"># 랜덤 베이스라인 손실 = -log(1/27) = log(27) ≈ 3.296</span>

<span class="cmt"># 정보 이론적 해석: 27개 심볼의 엔트로피 = log₂(27) ≈ 4.75 bits</span>
<span class="cmt"># (완전히 무작위인 시스템의 불확실성)</span>

<span class="cmt"># Perplexity = exp(loss)</span>
<span class="cmt"># 랜덤 Perplexity = exp(3.30) = 27  (= 어휘 크기, 직관적!)</span>
<span class="cmt"># 학습 후 Perplexity = exp(2.37) ≈ 10.7</span>
<span class="cmt"># → 모델이 평균적으로 약 11개 후보 중 정답을 고르는 수준</span></code></pre>

  <h3 id="loss-trajectory">손실 궤적 — 3.30 → 2.37 (1000 스텝)</h3>
  <table>
    <thead><tr><th>스텝</th><th>Loss</th><th>Perplexity</th><th>의미</th></tr></thead>
    <tbody>
      <tr><td>0</td><td>3.30</td><td>27.1</td><td>완전 랜덤 (초기화 직후)</td></tr>
      <tr><td>100</td><td>~2.90</td><td>~18</td><td>기본 통계 습득 시작</td></tr>
      <tr><td>500</td><td>~2.60</td><td>~13</td><td>모음-자음 패턴 학습 중</td></tr>
      <tr><td>1000</td><td>2.37</td><td>10.7</td><td>이름 구조 패턴 학습 완료</td></tr>
    </tbody>
  </table>

  <div class="diagram-container">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 600 260"
         style="width:100%;max-width:600px;display:block;margin:1.5em auto;background:var(--diagram-fill);border-radius:8px;padding:16px;">
      <!-- 축 -->
      <line x1="60" y1="20" x2="60" y2="210" stroke="var(--diagram-arrow)" stroke-width="1.5"/>
      <line x1="60" y1="210" x2="570" y2="210" stroke="var(--diagram-arrow)" stroke-width="1.5"/>
      <!-- Y축 레이블 (scale: 2.00~3.50, y=210-((loss-2.00)/1.50)*185) -->
      <text x="42" y="54" text-anchor="end" fill="var(--diagram-text)" font-size="9">3.30</text>
      <text x="42" y="103" text-anchor="end" fill="var(--diagram-text)" font-size="9">2.90</text>
      <text x="42" y="140" text-anchor="end" fill="var(--diagram-text)" font-size="9">2.60</text>
      <text x="42" y="168" text-anchor="end" fill="var(--diagram-text)" font-size="9">2.37</text>
      <text x="28" y="116" text-anchor="middle" fill="var(--diagram-text)" font-size="9" transform="rotate(-90,28,116)">Loss</text>
      <!-- Y축 눈금선 (데이터 포인트 기준) -->
      <line x1="57" y1="50" x2="570" y2="50" stroke="var(--border-color)" stroke-width="0.5" stroke-dasharray="3,3"/>
      <line x1="57" y1="99" x2="570" y2="99" stroke="var(--border-color)" stroke-width="0.5" stroke-dasharray="3,3"/>
      <line x1="57" y1="136" x2="570" y2="136" stroke="var(--border-color)" stroke-width="0.5" stroke-dasharray="3,3"/>
      <line x1="57" y1="164" x2="570" y2="164" stroke="var(--border-color)" stroke-width="0.5" stroke-dasharray="3,3"/>
      <!-- X축 레이블 -->
      <text x="60" y="225" text-anchor="middle" fill="var(--diagram-text)" font-size="9">0</text>
      <text x="110" y="225" text-anchor="middle" fill="var(--diagram-text)" font-size="9">100</text>
      <text x="310" y="225" text-anchor="middle" fill="var(--diagram-text)" font-size="9">500</text>
      <text x="560" y="225" text-anchor="middle" fill="var(--diagram-text)" font-size="9">1000</text>
      <text x="310" y="242" text-anchor="middle" fill="var(--diagram-text)" font-size="9">Step</text>
      <!-- 랜덤 베이스라인 점선 (loss=3.30, y=50) -->
      <line x1="60" y1="50" x2="560" y2="50" stroke="var(--text-muted)" stroke-width="1.5" stroke-dasharray="6,4"/>
      <text x="565" y="54" text-anchor="start" fill="var(--text-muted)" font-size="8">랜덤</text>
      <text x="565" y="65" text-anchor="start" fill="var(--text-muted)" font-size="8">3.30</text>
      <!-- 학습 곡선 (4 포인트 연결 선) -->
      <!-- step 0 → (x=60, y=50), step 100 → (x=110, y=99), step 500 → (x=310, y=136), step 1000 → (x=560, y=164) -->
      <polyline points="60,50 110,99 310,136 560,164"
                fill="none" stroke="var(--accent-primary)" stroke-width="2.5" stroke-linejoin="round"/>
      <!-- 데이터 포인트 -->
      <circle cx="60" cy="50" r="5" fill="var(--accent-primary)" stroke="var(--diagram-fill)" stroke-width="1.5"/>
      <circle cx="110" cy="99" r="5" fill="var(--accent-primary)" stroke="var(--diagram-fill)" stroke-width="1.5"/>
      <circle cx="310" cy="136" r="5" fill="var(--accent-primary)" stroke="var(--diagram-fill)" stroke-width="1.5"/>
      <circle cx="560" cy="164" r="5" fill="var(--accent-secondary)" stroke="var(--diagram-fill)" stroke-width="1.5"/>
      <!-- 포인트 레이블 -->
      <text x="60" y="42" text-anchor="middle" fill="var(--accent-primary)" font-size="9">3.30</text>
      <text x="110" y="91" text-anchor="middle" fill="var(--accent-primary)" font-size="9">~2.90</text>
      <text x="310" y="128" text-anchor="middle" fill="var(--accent-primary)" font-size="9">~2.60</text>
      <text x="560" y="156" text-anchor="middle" fill="var(--accent-secondary)" font-size="9">2.37</text>
    </svg>
    <p class="diagram-caption">손실 궤적: 1,000 스텝 학습 — 3.30(랜덤 초기화) → 2.37(학습 완료). 점선은 랜덤 베이스라인.</p>
  </div>

  <p>손실 3.30→2.37, 개선폭 <strong>0.93</strong> — Perplexity 27→10.7으로
  <strong>약 2.5배</strong> 향상. 4,192개 파라미터만으로 이름의 통계적 패턴을 학습한 결과입니다.</p>

  <div class="info-box info">
    <strong>왜 loss=0이 되지 않는가?</strong> 이름 데이터 자체에 불확실성이 있습니다.
    예: "k" 다음에 "a", "e", "i" 등 여러 글자가 올 수 있습니다.
    모델이 할 수 있는 최선은 이 확률 분포를 정확히 학습하는 것이며,
    완벽해도 엔트로피(최솟값) 이하로는 내려갈 수 없습니다.
  </div>
</section>

<section class="content-section">
  <h2 id="training-loop">완전한 학습 루프</h2>
  <pre><code>learning_rate, beta1, beta2, eps_adam = <span class="num">0.01</span>, <span class="num">0.85</span>, <span class="num">0.99</span>, <span class="num">1e-8</span>
m = [<span class="num">0.0</span>] * <span class="fn">len</span>(params)   <span class="cmt"># 1차 모멘텀 (방향)</span>
v = [<span class="num">0.0</span>] * <span class="fn">len</span>(params)   <span class="cmt"># 2차 모멘텀 (크기)</span>

num_steps = <span class="num">1000</span>
<span class="kw">for</span> step <span class="kw">in</span> <span class="fn">range</span>(num_steps):

    <span class="cmt"># ① 문서 선택: 셔플된 docs를 순환 (step % len(docs))</span>
    doc = docs[step % <span class="fn">len</span>(docs)]
    tokens = [BOS] + [uchars.<span class="fn">index</span>(ch) <span class="kw">for</span> ch <span class="kw">in</span> doc] + [BOS]
    n = <span class="fn">min</span>(block_size, <span class="fn">len</span>(tokens) - <span class="num">1</span>)   <span class="cmt"># 최대 block_size까지만</span>

    <span class="cmt"># ② 순전파: 각 위치에서 다음 토큰 예측 + loss 누적</span>
    keys, values = [[] <span class="kw">for</span> _ <span class="kw">in</span> <span class="fn">range</span>(n_layer)], [[] <span class="kw">for</span> _ <span class="kw">in</span> <span class="fn">range</span>(n_layer)]
    losses = []
    <span class="kw">for</span> pos_id <span class="kw">in</span> <span class="fn">range</span>(n):
        token_id, target_id = tokens[pos_id], tokens[pos_id + <span class="num">1</span>]
        logits = <span class="fn">gpt</span>(token_id, pos_id, keys, values)
        probs  = <span class="fn">softmax</span>(logits)
        losses.<span class="fn">append</span>(-probs[target_id].<span class="fn">log</span>())   <span class="cmt"># cross-entropy (스칼라)</span>
    loss = (<span class="num">1</span> / n) * <span class="fn">sum</span>(losses)   <span class="cmt"># 위치별 평균 loss</span>

    <span class="cmt"># ③ 역전파: 모든 파라미터의 기울기 계산</span>
    loss.<span class="fn">backward</span>()

    <span class="cmt"># ④ Adam 파라미터 업데이트 (기울기 초기화 포함)</span>
    lr_t = learning_rate * (<span class="num">1</span> - step / num_steps)   <span class="cmt"># 선형 감쇠</span>
    <span class="kw">for</span> i, p <span class="kw">in</span> <span class="fn">enumerate</span>(params):
        m[i] = beta1 * m[i] + (<span class="num">1</span> - beta1) * p.grad
        v[i] = beta2 * v[i] + (<span class="num">1</span> - beta2) * p.grad**<span class="num">2</span>
        m_hat = m[i] / (<span class="num">1</span> - beta1**(step+<span class="num">1</span>))   <span class="cmt"># 바이어스 보정</span>
        v_hat = v[i] / (<span class="num">1</span> - beta2**(step+<span class="num">1</span>))
        p.data -= lr_t * m_hat / (v_hat**<span class="num">0.5</span> + eps_adam)
        p.grad = <span class="num">0</span>   <span class="cmt"># ← 기울기 초기화 (업데이트 직후)</span></code></pre>

  <div class="info-box tip">
    <strong>학습 루프 4단계:</strong>
    ①문서 순환 → ②순전파(loss 누적) → ③역전파(grad 계산) → ④Adam 업데이트 + grad 초기화.
    이 1,000번 반복이 microgpt의 학습 전부입니다.
    <code>p.grad = 0</code>은 업데이트 <em>후</em>에 수행됩니다
    (인덱스 기반 for 루프이므로 별도의 초기화 루프가 필요 없습니다).
  </div>

  <h3 id="lr-decay">학습률 감쇠 (Linear Learning Rate Decay)</h3>
  <pre><code><span class="cmt"># 선형 학습률 감쇠: step이 진행될수록 lr 감소</span>
lr_t = learning_rate * (<span class="num">1</span> - step / num_steps)
<span class="cmt"># step=0:    lr_t = 0.01 × (1 - 0/1000)   = 0.01000  ← 최대</span>
<span class="cmt"># step=500:  lr_t = 0.01 × (1 - 500/1000) = 0.00500  ← 중간</span>
<span class="cmt"># step=999:  lr_t = 0.01 × (1 - 999/1000) = 0.00001  ← 최소</span>

<span class="cmt"># 주의: step=1000 (num_steps)에 도달하면 lr_t=0이 되므로</span>
<span class="cmt"># 실제로는 step 0~999 (총 1000번)만 실행됩니다.</span></code></pre>

  <div class="info-box info">
    <strong>왜 학습률을 줄이는가?</strong> 학습 초반에는 파라미터가 최적값에서 많이 떨어져 있으므로
    큰 보폭(높은 lr)으로 빠르게 이동합니다. 후반에는 최적값 근처에서 세밀하게 조정해야
    수렴 포인트를 지나치지 않도록 작은 보폭을 씁니다.
    nanoGPT는 더 정교한 "Cosine decay with linear warmup" (GPT-3 방식)을 사용합니다.
  </div>
</section>

<section class="content-section">
  <h2 id="adam">Adam 옵티마이저</h2>
  <p>단순한 SGD(확률적 경사 하강법) 대신, 적응적 학습률을 갖는 <strong>Adam</strong>을 구현합니다.
  Adam = <strong>Ada</strong>ptive <strong>M</strong>oment estimation.</p>

  <table>
    <thead><tr><th>옵티마이저</th><th>업데이트 규칙</th><th>단점</th></tr></thead>
    <tbody>
      <tr><td><strong>SGD</strong></td><td>p -= lr × grad</td><td>학습률 수동 조정, 느린 수렴</td></tr>
      <tr><td><strong>Momentum SGD</strong></td><td>v = β×v + grad; p -= lr×v</td><td>학습률 여전히 전역 고정</td></tr>
      <tr><td><strong>RMSProp</strong></td><td>v = β×v + (1-β)×grad²; p -= lr×grad/√v</td><td>모멘텀 없음</td></tr>
      <tr><td><strong>Adam ✓</strong></td><td>m, v 둘 다 추적; 바이어스 보정</td><td>메모리 2배 (m, v 버퍼)</td></tr>
    </tbody>
  </table>

  <h3 id="adam-code">Adam 단계별 코드 해설</h3>
  <pre><code><span class="cmt"># ── 초기화 (학습 루프 전) ──</span>
m = [<span class="num">0.0</span>] * <span class="fn">len</span>(params)   <span class="cmt"># 1차 모멘텀 버퍼 (지수 이동 평균 of grad)</span>
v = [<span class="num">0.0</span>] * <span class="fn">len</span>(params)   <span class="cmt"># 2차 모멘텀 버퍼 (지수 이동 평균 of grad²)</span>
beta1, beta2 = <span class="num">0.85</span>, <span class="num">0.99</span>

<span class="cmt"># ── 각 스텝의 업데이트 ──</span>
<span class="kw">for</span> i, p <span class="kw">in</span> <span class="fn">enumerate</span>(params):
    g = p.grad

    <span class="cmt"># ─ 1단계: 1차 모멘텀 (방향) ─</span>
    <span class="cmt"># 현재 기울기 g와 이전 방향 m[i]의 지수 가중 평균</span>
    m[i] = beta1 * m[i] + (<span class="num">1</span> - beta1) * g
    <span class="cmt"># beta1=0.85 → 현재 기울기에 15% 가중, 이전 방향에 85% 가중</span>
    <span class="cmt"># (기본값 0.9보다 작아 더 빠르게 현재 기울기에 반응)</span>

    <span class="cmt"># ─ 2단계: 2차 모멘텀 (크기) ─</span>
    <span class="cmt"># 기울기 제곱의 지수 이동 평균 → "이 파라미터의 기울기가 얼마나 크게 변하는가"</span>
    v[i] = beta2 * v[i] + (<span class="num">1</span> - beta2) * g**<span class="num">2</span>
    <span class="cmt"># beta2=0.99 → 기울기 크기의 장기 기억</span>

    <span class="cmt"># ─ 3단계: 바이어스 보정 ─</span>
    <span class="cmt"># 초기(step 작을 때) m,v가 0으로 초기화되어 있어 편향됨을 보정</span>
    t = step + <span class="num">1</span>
    m_hat = m[i] / (<span class="num">1</span> - beta1**t)   <span class="cmt"># step=1: 1/(1-0.85)=6.67배 증폭</span>
    v_hat = v[i] / (<span class="num">1</span> - beta2**t)   <span class="cmt"># step=1: 1/(1-0.99)=100배 증폭</span>

    <span class="cmt"># ─ 4단계: 파라미터 업데이트 ─</span>
    <span class="cmt"># 효과적 학습률 = lr_t / √v_hat → 기울기가 크게 변하는 파라미터는 작게 업데이트</span>
    p.data -= lr_t * m_hat / (v_hat**<span class="num">0.5</span> + eps_adam)
    p.grad = <span class="num">0</span>   <span class="cmt"># 초기화</span></code></pre>

  <h3 id="bias-correction">바이어스 보정 상세 — 왜 필요한가?</h3>
  <pre><code><span class="cmt"># step=1, beta1=0.85, grad=1.0 인 경우:</span>
m[i] = <span class="num">0.85</span> * <span class="num">0.0</span> + <span class="num">0.15</span> * <span class="num">1.0</span> = <span class="num">0.15</span>   <span class="cmt"># 실제 gradient=1.0인데 m=0.15만 저장됨</span>
m_hat = <span class="num">0.15</span> / (<span class="num">1</span> - <span class="num">0.85</span>**<span class="num">1</span>) = <span class="num">0.15</span> / <span class="num">0.15</span> = <span class="num">1.0</span>  <span class="cmt"># 보정 후 1.0 ← 올바른 값</span>

<span class="cmt"># step=100, beta1=0.85:</span>
<span class="cmt"># 1 - 0.85**100 ≈ 1.0 (완전 포화)</span>
<span class="cmt"># → 보정 인자가 거의 1이 되어 바이어스 보정이 자동으로 무력화됨</span>
<span class="cmt"># → 초반 스텝에서만 중요하고, 충분한 스텝 후엔 영향 없음</span></code></pre>

  <table>
    <thead><tr><th>파라미터</th><th>기본값 (논문)</th><th>microgpt</th><th>효과</th></tr></thead>
    <tbody>
      <tr>
        <td>beta1</td><td>0.9</td><td><strong>0.85</strong></td>
        <td>현재 기울기에 15% (기본 10%) → 더 빠른 방향 전환</td>
      </tr>
      <tr>
        <td>beta2</td><td>0.999</td><td><strong>0.99</strong></td>
        <td>기울기 크기 기억 더 짧게 → 더 빠른 적응</td>
      </tr>
    </tbody>
  </table>

  <p>1,000 스텝 밖에 없는 짧은 학습에서는 빠른 적응이 중요합니다.
  기본값(β1=0.9, β2=0.999)은 수십만 스텝의 장기 학습을 위한 값입니다.</p>

  <div class="info-box tip">
    <strong>Adam의 적응적 학습률 직관:</strong> 평탄한 길(기울기 작음) → 큰 보폭으로 빠르게.
    가파른 경사(기울기 큼) → 작은 보폭으로 조심스럽게.
    <code>lr / √v_hat</code>에서 v_hat(기울기 제곱 평균)이 크면 분모가 커져 실제 학습률이 줄어듭니다.
    파라미터마다 각자에게 맞는 학습률을 자동 조절합니다.
  </div>
</section>

<section class="content-section">
  <h2 id="inference">추론 (Inference) &amp; 이름 생성</h2>
  <pre><code><span class="cmt"># may the model babble back to us</span>
temperature = <span class="num">0.5</span>  <span class="cmt"># (0, 1] — 낮을수록 보수적, 높을수록 다양한 출력</span>
<span class="kw">for</span> sample_idx <span class="kw">in</span> <span class="fn">range</span>(<span class="num">20</span>):   <span class="cmt"># 총 20개의 이름 생성</span>
    keys, values = [[] <span class="kw">for</span> _ <span class="kw">in</span> <span class="fn">range</span>(n_layer)], [[] <span class="kw">for</span> _ <span class="kw">in</span> <span class="fn">range</span>(n_layer)]
    token_id = BOS   <span class="cmt"># 시작 토큰</span>
    sample = []
    <span class="cmt"># BOS → 첫 글자 → 두 번째 글자 → ... → BOS(종료)</span>
    <span class="kw">for</span> pos_id <span class="kw">in</span> <span class="fn">range</span>(block_size):
        logits = <span class="fn">gpt</span>(token_id, pos_id, keys, values)
        probs = <span class="fn">softmax</span>([l / temperature <span class="kw">for</span> l <span class="kw">in</span> logits])
        token_id = random.<span class="fn">choices</span>(<span class="fn">range</span>(vocab_size), weights=[p.data <span class="kw">for</span> p <span class="kw">in</span> probs])[<span class="num">0</span>]
        <span class="kw">if</span> token_id == BOS: <span class="kw">break</span>
        sample.<span class="fn">append</span>(uchars[token_id])
    <span class="fn">print</span>(<span class="str">f"sample {sample_idx+1:2d}: {''.join(sample)}"</span>)</code></pre>

  <h3 id="autoregressive-trace">자기회귀 생성 — "kamon" 단계별 추적</h3>
  <pre><code><span class="cmt"># "kamon" 생성 과정 (temperature=0.5 적용)</span>
keys = [[] <span class="kw">for</span> _ <span class="kw">in</span> <span class="fn">range</span>(n_layer)]
values = [[] <span class="kw">for</span> _ <span class="kw">in</span> <span class="fn">range</span>(n_layer)]
token_id = BOS  <span class="cmt"># 26</span>

<span class="cmt"># ── step 0 (pos=0) ──</span>
logits = <span class="fn">gpt</span>(<span class="num">26</span>, <span class="num">0</span>, keys, values)           <span class="cmt"># BOS 입력</span>
probs  = <span class="fn">softmax</span>([l/<span class="num">0.5</span> <span class="kw">for</span> l <span class="kw">in</span> logits])   <span class="cmt"># temperature=0.5 적용</span>
<span class="cmt"># probs 중 'k'(10)의 확률이 가장 높다고 가정</span>
token_id = random.<span class="fn">choices</span>(<span class="fn">range</span>(<span class="num">27</span>), weights=[p.data <span class="kw">for</span> p <span class="kw">in</span> probs])[<span class="num">0</span>]
<span class="cmt"># → token_id = 10 ('k')</span>

<span class="cmt"># ── step 1 (pos=1) ──</span>
logits = <span class="fn">gpt</span>(<span class="num">10</span>, <span class="num">1</span>, keys, values)   <span class="cmt"># 'k' 입력, pos=1</span>
<span class="cmt"># keys[0]에는 이제 K_BOS, K_k 두 개가 있음</span>
<span class="cmt"># → 'a'(0)가 높은 확률로 샘플링됨 → token_id = 0</span>

<span class="cmt"># ── step 2 (pos=2) ── 'm'(12) 샘플링</span>
<span class="cmt"># keys[0] = [K_BOS, K_k, K_a] (3개 누적)</span>

<span class="cmt"># ── step 3 (pos=3) ── 'o'(14) 샘플링</span>
<span class="cmt"># ── step 4 (pos=4) ── 'n'(13) 샘플링</span>
<span class="cmt"># ── step 5 (pos=5) ── BOS(26)가 샘플링되면 break → "kamon" 완성</span>
<span class="cmt"># → sample = ['k', 'a', 'm', 'o', 'n'] → "kamon"</span></code></pre>

  <div class="diagram-container">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 700 200"
         style="width:100%;max-width:700px;display:block;margin:1.5em auto;background:var(--diagram-fill);border-radius:8px;padding:16px;">
      <defs>
        <marker id="microgpt-ar-arrow" viewBox="0 0 10 7" refX="10" refY="3.5"
                markerWidth="8" markerHeight="7" orient="auto">
          <polygon points="0 0, 10 3.5, 0 7" fill="var(--diagram-arrow)"/>
        </marker>
        <marker id="microgpt-ar-arrow-loop" viewBox="0 0 10 7" refX="10" refY="3.5"
                markerWidth="8" markerHeight="7" orient="auto">
          <polygon points="0 0, 10 3.5, 0 7" fill="var(--accent-secondary)"/>
        </marker>
      </defs>
      <!-- BOS 시작 -->
      <rect x="8" y="75" width="80" height="50" rx="6"
            fill="var(--bg-secondary)" stroke="var(--accent-primary)" stroke-width="2"/>
      <text x="48" y="97" text-anchor="middle" fill="var(--accent-primary)" font-size="11" font-weight="bold">BOS</text>
      <text x="48" y="114" text-anchor="middle" fill="var(--text-muted)" font-size="9">token=26</text>
      <!-- 화살표 -->
      <line x1="88" y1="100" x2="108" y2="100" stroke="var(--diagram-arrow)" stroke-width="1.5" marker-end="url(#microgpt-ar-arrow)"/>
      <!-- gpt() -->
      <rect x="110" y="65" width="100" height="70" rx="6"
            fill="var(--bg-secondary)" stroke="var(--border-color)" stroke-width="1.5"/>
      <text x="160" y="90" text-anchor="middle" fill="var(--diagram-text)" font-size="12" font-weight="bold">gpt()</text>
      <text x="160" y="107" text-anchor="middle" fill="var(--diagram-text)" font-size="9">token, pos</text>
      <text x="160" y="123" text-anchor="middle" fill="var(--text-muted)" font-size="9">keys, values</text>
      <!-- 화살표 -->
      <line x1="210" y1="100" x2="230" y2="100" stroke="var(--diagram-arrow)" stroke-width="1.5" marker-end="url(#microgpt-ar-arrow)"/>
      <!-- 확률 분포 -->
      <rect x="232" y="65" width="120" height="70" rx="6"
            fill="var(--bg-secondary)" stroke="var(--border-color)" stroke-width="1.5"/>
      <text x="292" y="90" text-anchor="middle" fill="var(--diagram-text)" font-size="11">확률 분포</text>
      <text x="292" y="107" text-anchor="middle" fill="var(--text-muted)" font-size="9">softmax(logits/T)</text>
      <text x="292" y="123" text-anchor="middle" fill="var(--text-muted)" font-size="9">T=0.5</text>
      <!-- 화살표 -->
      <line x1="352" y1="100" x2="372" y2="100" stroke="var(--diagram-arrow)" stroke-width="1.5" marker-end="url(#microgpt-ar-arrow)"/>
      <!-- 샘플링 -->
      <rect x="374" y="65" width="110" height="70" rx="6"
            fill="var(--bg-secondary)" stroke="var(--accent-secondary)" stroke-width="2"/>
      <text x="429" y="90" text-anchor="middle" fill="var(--accent-secondary)" font-size="11" font-weight="bold">샘플링</text>
      <text x="429" y="107" text-anchor="middle" fill="var(--text-muted)" font-size="9">random.choices()</text>
      <text x="429" y="123" text-anchor="middle" fill="var(--text-muted)" font-size="9">→ 토큰 ID</text>
      <!-- BOS 분기 화살표 (조건 분기) -->
      <line x1="484" y1="90" x2="510" y2="68" stroke="var(--diagram-arrow)" stroke-width="1.5" marker-end="url(#microgpt-ar-arrow)"/>
      <line x1="484" y1="110" x2="510" y2="135" stroke="var(--diagram-arrow)" stroke-width="1.5" marker-end="url(#microgpt-ar-arrow)"/>
      <!-- BOS 종료 -->
      <rect x="512" y="50" width="120" height="40" rx="6"
            fill="var(--bg-secondary)" stroke="var(--accent-primary)" stroke-width="2"/>
      <text x="572" y="68" text-anchor="middle" fill="var(--accent-primary)" font-size="10" font-weight="bold">== BOS?</text>
      <text x="572" y="84" text-anchor="middle" fill="var(--text-muted)" font-size="9">break → 완성!</text>
      <!-- 반복(loop) 박스 -->
      <rect x="512" y="118" width="120" height="40" rx="6"
            fill="var(--bg-secondary)" stroke="var(--accent-secondary)" stroke-width="1.5"/>
      <text x="572" y="134" text-anchor="middle" fill="var(--accent-secondary)" font-size="10">≠ BOS</text>
      <text x="572" y="150" text-anchor="middle" fill="var(--text-muted)" font-size="9">sample.append(ch)</text>
      <!-- 반복 화살표 (아래로 돌아가기) -->
      <line x1="512" y1="138" x2="160" y2="165" stroke="var(--accent-secondary)" stroke-width="1.5" stroke-dasharray="5,3" marker-end="url(#microgpt-ar-arrow-loop)"/>
      <line x1="160" y1="165" x2="160" y2="137" stroke="var(--accent-secondary)" stroke-width="1.5" stroke-dasharray="5,3" marker-end="url(#microgpt-ar-arrow-loop)"/>
      <text x="345" y="180" text-anchor="middle" fill="var(--accent-secondary)" font-size="9" font-style="italic">반복 (pos+1, token=새 token_id)</text>
    </svg>
    <p class="diagram-caption">자기회귀 생성 루프: BOS → gpt() → softmax/temperature → 샘플링 → BOS이면 종료, 아니면 반복</p>
  </div>

  <h3 id="block-size-limit">block_size=16 제한의 의미</h3>
  <pre><code><span class="kw">for</span> pos_id <span class="kw">in</span> <span class="fn">range</span>(block_size):   <span class="cmt"># 0~15, 최대 16 스텝</span>
    logits = <span class="fn">gpt</span>(token_id, pos_id, keys, values)
    <span class="cmt"># ...</span>
    <span class="kw">if</span> token_id == BOS: <span class="kw">break</span>   <span class="cmt"># 보통 여기서 멈춤</span>

<span class="cmt"># 만약 BOS가 16 스텝 안에 생성되지 않으면:</span>
<span class="cmt"># pos_id가 block_size에 도달해 루프 종료 → 이름이 잘림</span>
<span class="cmt"># names.txt의 최장 이름이 15글자이므로, 정상 이름은 항상 16 이내에 BOS 생성</span>
<span class="cmt"># 반면 wpe(위치 임베딩)는 0~15만 정의됨</span>
<span class="cmt"># → pos_id=16 이상을 입력하면 IndexError 발생</span></code></pre>

  <h3 id="temperature">Temperature — 창의성 조절</h3>
  <pre><code><span class="cmt"># 원본 logits (학습된 모델 출력, 일부):</span>
<span class="cmt"># 'a'=2.1, 'k'=1.8, 'e'=1.5, 나머지≈0</span>

<span class="cmt"># Temperature T=1.0 (원본 유지):</span>
<span class="cmt"># softmax([2.1, 1.8, 1.5, ...]) ≈ [0.30, 0.22, 0.17, ...]</span>
<span class="cmt">#   → 적당히 다양한 선택</span>

<span class="cmt"># Temperature T=0.5 (낮게, 로짓 2배 증폭):</span>
<span class="cmt"># softmax([4.2, 3.6, 3.0, ...]) ≈ [0.52, 0.28, 0.14, ...]</span>
<span class="cmt">#   → 'a'에 집중, 더 안정적</span>

<span class="cmt"># Temperature T=0.1 (매우 낮게, 로짓 10배 증폭):</span>
<span class="cmt"># softmax([21, 18, 15, ...]) ≈ [0.95, 0.05, ~0, ...]</span>
<span class="cmt">#   → 거의 항상 'a' 선택 (greedy에 가까움)</span>

<span class="cmt"># Temperature T=2.0 (높게, 로짓 절반):</span>
<span class="cmt"># softmax([1.05, 0.9, 0.75, ...]) ≈ [0.20, 0.18, 0.16, ...]</span>
<span class="cmt">#   → 거의 균등 → 무작위에 가까움</span></code></pre>

  <dl>
    <dt>Temperature = 0.1 — 보수적 (Greedy에 가까움)</dt>
    <dd>로짓에 10을 곱한 효과 → 가장 높은 확률 토큰에 95%+ 집중.
    반복적이지만 안정적. 동일한 BOS에서 항상 같은 이름이 나올 수 있음.</dd>

    <dt>Temperature = 0.5 (microgpt.py 기본값) — 품질과 다양성의 균형</dt>
    <dd>로짓에 2를 곱한 효과 → 상위 토큰들 간의 차이를 2배로 강조.
    발음 가능하고 다양한 이름 생성.</dd>

    <dt>Temperature = 1.0 — 원본 확률 그대로</dt>
    <dd>모델이 학습한 분포를 그대로 사용. 이론적으로 "올바른" 샘플링.
    가끔 발음 어려운 이름도 생성.</dd>

    <dt>Temperature &gt; 1.0 — 창의적 (과도한 무작위)</dt>
    <dd>로짓 차이를 줄여 균등 분포에 가까워짐. 이상한 결과 빈도 증가.</dd>
  </dl>

  <div class="info-box warning">
    <strong>⚠️ Temperature = 0 → ZeroDivisionError:</strong>
    <code>logits / 0</code>은 Python에서 ZeroDivisionError를 냅니다.
    Greedy Decoding을 원한다면 temperature 나눗셈을 제거하고
    <code>token_id = max(range(vocab_size), key=lambda i: logits[i].data)</code>를 사용하세요.
  </div>

  <h3 id="inference-results">실제 생성 결과 — 1000 스텝 학습 후 샘플 20개</h3>
  <table>
    <thead><tr><th>샘플</th><th>이름</th><th>특징</th></tr></thead>
    <tbody>
      <tr><td>1~5</td><td>kamon, ann, karai, jaire, vialan</td><td>모음-자음 교대 패턴</td></tr>
      <tr><td>6~10</td><td>karia, yeran, anna, areli, kaina</td><td>"ar-", "an-" 흔한 이름 조각</td></tr>
      <tr><td>11~15</td><td>konna, keylen, liole, alerin, earan</td><td>2~6글자 적절한 길이</td></tr>
      <tr><td>16~20</td><td>lenne, kana, lara, alela, anton</td><td>발음 가능한 이름 구조</td></tr>
    </tbody>
  </table>

  <div class="info-box info">
    <strong>4,192개 파라미터가 학습한 통계적 패턴:</strong>
    "k-", "l-", "a-" 로 시작하는 이름이 많음.
    모음(a, e, i, o) 뒤에 자음이 자주 옴.
    "-an", "-ra", "-la", "-on" 같은 접미사 패턴.
    평균 4~5글자 (훈련 데이터 분포 반영).
  </div>

  <h3 id="train-vs-infer">학습 vs 추론의 차이점</h3>
  <table>
    <thead><tr><th>측면</th><th>학습 (Training)</th><th>추론 (Inference)</th></tr></thead>
    <tbody>
      <tr><td>입력 방식</td><td>이름 전체 토큰을 위치별로 순전파</td><td>BOS부터 시작, 토큰 하나씩 생성</td></tr>
      <tr><td>역전파</td><td>O (loss.backward())</td><td>X (grad 불필요)</td></tr>
      <tr><td>KV 캐시</td><td>매 이름마다 초기화</td><td>매 샘플마다 초기화</td></tr>
      <tr><td>종료 조건</td><td>모든 위치 처리 완료</td><td>BOS 샘플링 or block_size 도달</td></tr>
      <tr><td>결정성</td><td>결정적 (seed 고정)</td><td>확률적 (temperature 샘플링)</td></tr>
    </tbody>
  </table>
</section>

<section class="content-section">
  <h2 id="comparison">MicroGPT vs 현대 LLM</h2>

  <table>
    <thead>
      <tr><th>항목</th><th>microgpt.py</th><th>Modern LLMs (GPT-4 등)</th></tr>
    </thead>
    <tbody>
      <tr><td>파라미터</td><td><strong>4,192개</strong></td><td><strong>수조(Trillions)개</strong></td></tr>
      <tr><td>데이터</td><td>32,033개의 이름</td><td>인터넷 전체 텍스트 (수조 토큰)</td></tr>
      <tr><td>하드웨어</td><td>노트북 CPU, 수 분</td><td>수천 개의 H100 GPU</td></tr>
      <tr><td>코드</td><td>~200줄 순수 파이썬</td><td>수백만 줄, 수천억 원 학습 비용</td></tr>
      <tr><td>목적</td><td>영어 이름 생성</td><td>범용 언어 이해 &amp; 생성</td></tr>
    </tbody>
  </table>

  <div class="info-box tip">
    <strong>하지만 원리는 완전히 동일합니다.</strong> MicroGPT와 GPT-4의 아키텍처적 차이는
    규모(scale)뿐입니다. 토큰 임베딩, 트랜스포머 레이어, 어텐션, MLP, 잔차 연결,
    RMSNorm, 소프트맥스, Adam 옵티마이저 — 모두 같은 원리입니다.
  </div>

  <h3 id="detail-comparison">구성요소별 상세 비교</h3>
  <table>
    <thead><tr><th>구성요소</th><th>microgpt.py</th><th>GPT-4 / LLaMA 등</th></tr></thead>
    <tbody>
      <tr><td>데이터</td><td>32,033개의 이름 (names.txt)</td><td>인터넷 전체 텍스트 (수조 토큰)</td></tr>
      <tr><td>토크나이저</td><td>26 알파벳 + BOS = 27개 토큰</td><td>BPE/SentencePiece, 50K~256K 토큰</td></tr>
      <tr><td>Autograd</td><td>스칼라 Value 클래스, 순수 Python</td><td>PyTorch/JAX 텐서 자동미분 (GPU 병렬)</td></tr>
      <tr><td>아키텍처</td><td>n_embd=16, n_head=4, n_layer=1</td><td>n_embd=12288, n_head=96, n_layer=96+</td></tr>
      <tr><td>파라미터</td><td>4,192개</td><td>수천억~수조 개</td></tr>
      <tr><td>학습</td><td>1000 스텝, 노트북 CPU, 수 분</td><td>수천억 스텝, H100 수천 개, 수개월</td></tr>
      <tr><td>정규화</td><td>RMSNorm (학습 파라미터 없음)</td><td>LayerNorm or RMSNorm (γ 파라미터 있음)</td></tr>
      <tr><td>활성화</td><td>ReLU</td><td>GELU (GPT-2/3), SwiGLU (LLaMA)</td></tr>
      <tr><td>사후 학습</td><td>없음 (순수 사전학습만)</td><td>SFT + RLHF/DPO (인간 피드백)</td></tr>
      <tr><td>추론 최적화</td><td>KV캐시 + temperature 샘플링</td><td>Flash Attention, 양자화, 분산 추론</td></tr>
    </tbody>
  </table>

  <div class="info-box info">
    <strong>Karpathy의 말:</strong>
    "The most atomic way to train and run inference for a GPT in pure, dependency-free Python.
    <em>This file is the complete algorithm. Everything else is just efficiency.</em>"
  </div>
</section>

<section class="content-section">
  <h2 id="nanogpt">nanoGPT — microgpt의 Production 확장판</h2>
  <p><strong>nanoGPT</strong>는 Karpathy가 2022년 12월 공개한 "가장 간결하고 빠른 GPT 훈련 코드베이스"입니다.
  OpenAI GPT-2(124M 파라미터)를 완전히 재현하고, 단일 GPU에서 실용적인 언어 모델을 훈련할 수 있게 설계되었습니다.
  microgpt가 "이해를 위한 원자적 구현"이라면, nanoGPT는 "실제 사용을 위한 최소 구현"입니다.</p>

  <h3 id="nanogpt-compare">microgpt vs nanoGPT 핵심 차이점</h3>
  <table>
    <thead><tr><th>항목</th><th>microgpt.py</th><th>nanoGPT</th></tr></thead>
    <tbody>
      <tr><td><strong>연산 단위</strong></td><td>스칼라 <code>Value</code> 객체</td><td>PyTorch 텐서 (GPU 병렬 연산)</td></tr>
      <tr><td><strong>자동미분</strong></td><td>수동 구현 (<code>_backward</code>, 위상 정렬)</td><td>PyTorch <code>.backward()</code> 완전 자동화</td></tr>
      <tr><td><strong>토크나이저</strong></td><td>26 알파벳 + BOS = vocab_size 27 (문자 수준)</td><td>tiktoken BPE — vocab_size 50,257 (서브워드)</td></tr>
      <tr><td><strong>아키텍처</strong></td><td>n_layer=1, n_head=4, n_embd=16</td><td>n_layer=12, n_head=12, n_embd=768 (GPT-2 small)</td></tr>
      <tr><td><strong>파라미터 수</strong></td><td>4,192개</td><td>124,439,808개 (GPT-2 small) ~ 1.5B (XL)</td></tr>
      <tr><td><strong>배치 크기</strong></td><td>1 (이름 1개/스텝)</td><td>32~512+ (gradient accumulation)</td></tr>
      <tr><td><strong>컨텍스트 길이</strong></td><td>최대 15 토큰 (가장 긴 이름)</td><td>1,024 토큰 (GPT-2 기본)</td></tr>
      <tr><td><strong>Attention 구현</strong></td><td>스칼라 루프 기반 수동 계산</td><td>Flash Attention (<code>F.scaled_dot_product_attention</code>)</td></tr>
      <tr><td><strong>정규화</strong></td><td>RMSNorm (수동 구현)</td><td>LayerNorm (<code>nn.LayerNorm</code>) + Dropout</td></tr>
      <tr><td><strong>활성화 함수</strong></td><td>ReLU</td><td>GELU (<code>F.gelu</code>) — GPT-2/3 표준</td></tr>
      <tr><td><strong>옵티마이저</strong></td><td>Adam 수동 구현 (β1=0.85, β2=0.99)</td><td>PyTorch <code>optim.AdamW</code> + weight decay (β1=0.9, β2=0.95)</td></tr>
      <tr><td><strong>LR 스케줄</strong></td><td>선형 감쇠</td><td>Cosine decay with linear warmup (GPT-3 방식)</td></tr>
      <tr><td><strong>Gradient Clipping</strong></td><td>없음</td><td><code>clip_grad_norm_</code> (max_norm=1.0)</td></tr>
      <tr><td><strong>분산학습</strong></td><td>없음 (단일 프로세스)</td><td>PyTorch DDP — 다중 GPU/노드</td></tr>
      <tr><td><strong>Checkpoint</strong></td><td>없음</td><td>자동 저장/복원, best val loss 기준</td></tr>
      <tr><td><strong>Weight Tying</strong></td><td>wte/lm_head 별도 초기화</td><td><code>lm_head.weight = transformer.wte.weight</code> 명시적 공유</td></tr>
      <tr><td><strong>외부 의존성</strong></td><td>os, math, random (표준 라이브러리만)</td><td>torch, tiktoken, numpy, datasets 등</td></tr>
    </tbody>
  </table>

  <h3 id="nanogpt-files">nanoGPT 파일 구조</h3>
  <table>
    <thead><tr><th>파일</th><th>역할</th><th>microgpt 대응</th></tr></thead>
    <tbody>
      <tr>
        <td><code>model.py</code></td>
        <td>GPT 모델 정의 — <code>CausalSelfAttention</code>, <code>MLP</code>, <code>Block</code>, <code>GPT</code> 클래스, Flash Attention 지원</td>
        <td><code>def gpt()</code> 함수 전체</td>
      </tr>
      <tr>
        <td><code>train.py</code></td>
        <td>훈련 루프 — DDP 분산학습, gradient accumulation, cosine LR, checkpoint 저장/복원</td>
        <td><code>for step in range(num_steps):</code> 루프</td>
      </tr>
      <tr>
        <td><code>data/</code></td>
        <td>데이터 준비 스크립트 — Shakespeare, OpenWebText 다운로드·토크나이징·바이너리 저장</td>
        <td>코드 내 <code>names.txt</code> 직접 로딩 3줄</td>
      </tr>
      <tr>
        <td><code>config/</code></td>
        <td>사전 정의 훈련 설정 — GPT-2 small/medium/large/XL, Shakespeare 등 하이퍼파라미터 세트</td>
        <td>코드 내 하드코딩된 <code>n_embd=16</code> 등</td>
      </tr>
      <tr>
        <td><code>sample.py</code></td>
        <td>텍스트 샘플링 — checkpoint 로드 후 <code>temperature</code>·<code>top_k</code> 기반 텍스트 생성</td>
        <td><code>for sample_idx in range(20):</code> 루프</td>
      </tr>
      <tr>
        <td><code>bench.py</code></td>
        <td>성능 벤치마크 — throughput(token/s) 측정, Flash Attention 효과 비교</td>
        <td>해당 없음</td>
      </tr>
    </tbody>
  </table>

  <h3 id="nanogpt-model-py">nanoGPT model.py — 클래스 구조</h3>
  <p>microgpt의 함수형 구현과 달리 nanoGPT는 PyTorch <code>nn.Module</code> 클래스 계층 구조를 사용합니다.
  아키텍처는 동일하지만, 텐서화·자동미분·GPU 최적화가 추가됩니다.</p>
  <pre><code><span class="cmt"># nanoGPT model.py 클래스 구조 (요약)</span>

<span class="kw">class</span> <span class="type">CausalSelfAttention</span>(nn.<span class="type">Module</span>):
    <span class="cmt"># Q, K, V 프로젝션을 단일 c_attn(3*n_embd)로 통합 — 효율적 행렬 곱</span>
    <span class="cmt"># Flash Attention: F.scaled_dot_product_attention() 자동 사용</span>
    <span class="cmt"># microgpt: 스칼라 루프로 직접 qkv 계산</span>
    <span class="cmt"># attn_dropout + resid_dropout 정규화</span>

<span class="kw">class</span> <span class="type">MLP</span>(nn.<span class="type">Module</span>):
    <span class="cmt"># Linear(n_embd → 4*n_embd) → GELU → Linear(4*n_embd → n_embd)</span>
    <span class="cmt"># microgpt: ReLU 사용, nanoGPT: GELU (GPT-2/3 표준)</span>
    <span class="cmt"># dropout으로 과적합 방지</span>

<span class="kw">class</span> <span class="type">Block</span>(nn.<span class="type">Module</span>):
    <span class="cmt"># Pre-LayerNorm 구조: ln_1 → attention → residual</span>
    <span class="cmt">#                      ln_2 → mlp       → residual</span>
    <span class="cmt"># microgpt: RMSNorm 사용, nanoGPT: LayerNorm</span>

<span class="kw">class</span> <span class="type">GPT</span>(nn.<span class="type">Module</span>):
    <span class="cmt"># transformer.wte : 토큰 임베딩  [vocab_size, n_embd]</span>
    <span class="cmt"># transformer.wpe : 위치 임베딩  [block_size, n_embd]  ← 학습 가능</span>
    <span class="cmt"># transformer.drop: Dropout</span>
    <span class="cmt"># transformer.h   : ModuleList([Block(...)] * n_layer)</span>
    <span class="cmt"># transformer.ln_f: 최종 LayerNorm</span>
    <span class="cmt"># lm_head         : Linear(n_embd → vocab_size, bias=False)</span>
    <span class="cmt">#                   → wte와 가중치 공유(weight tying)</span>
    <span class="cmt">#</span>
    <span class="cmt"># generate(): top_k 샘플링 + temperature 스케일링</span>
    <span class="cmt"># from_pretrained(): HuggingFace 없이 GPT-2 가중치 직접 로드</span></code></pre>

  <h3 id="nanogpt-roadmap">microgpt → nanoGPT 확장 경로 — 6단계 로드맵</h3>
  <ol>
    <li>
      <strong>① 스칼라 → NumPy 벡터화</strong><br>
      <code>Value</code> 스칼라 루프 → NumPy 배열 연산 → 속도 10~100배 향상.<br>
      역전파도 NumPy 브로드캐스팅으로 표현 가능. 텐서 차원 개념 도입.
    </li>
    <li>
      <strong>② 배치(Batch) 처리 도입</strong><br>
      이름 1개/스텝 → 32개/스텝 미니배치 → 그래디언트 분산 감소, 학습 안정화.<br>
      텐서 차원이 <code>[T]</code>에서 <code>[B, T]</code>로 확장됨.
    </li>
    <li>
      <strong>③ PyTorch 텐서 교체</strong><br>
      NumPy → PyTorch Tensor → <code>.backward()</code> 자동미분, GPU 가속.<br>
      수동으로 작성한 <code>_backward</code> 체인이 완전히 자동화됨.
    </li>
    <li>
      <strong>④ 표준 모듈 &amp; 정규화 적용</strong><br>
      수동 구현한 <code>linear</code>, <code>rmsnorm</code>, <code>Adam</code> →
      <code>nn.Linear</code>, <code>nn.LayerNorm</code>, <code>optim.AdamW</code>.<br>
      Dropout, gradient clipping, weight decay 정규화 기법 추가.
    </li>
    <li>
      <strong>⑤ 아키텍처 확장</strong><br>
      n_layer=1 → 12+, n_embd=16 → 768+, vocab_size=27 → 50,257 (BPE).<br>
      Flash Attention, 컨텍스트 길이 1024+ 지원, top-k 샘플링 추가.
    </li>
    <li>
      <strong>⑥ 분산학습 &amp; 생산화</strong><br>
      DDP로 다중 GPU 학습, gradient accumulation, cosine LR with warmup,<br>
      자동 checkpoint 저장/복원 → 실제 GPT-2 수준 모델 훈련 가능.
    </li>
  </ol>

  <h3 id="nanogpt-examples">nanoGPT 대표 사용 예시</h3>
  <dl>
    <dt>Shakespeare 문자 수준 언어 모델</dt>
    <dd>셰익스피어 전집(~1MB)으로 훈련 → 셰익스피어 스타일 문장 생성.
    단일 GPU로 수 분~수 시간 안에 완료. GPT의 원리를 실감하는 입문 예제.
    microgpt의 names.txt → 셰익스피어 텍스트로 도메인만 교체한 확장 버전.</dd>

    <dt>GPT-2 (124M) 완전 재현</dt>
    <dd>OpenWebText(~38GB) 데이터셋으로 훈련 →
    OpenAI GPT-2와 동등한 검증 손실(val loss ≈ 2.85) 달성.
    A100 GPU 8개로 약 4일 소요. Karpathy가 직접 재현 결과를 공개함.</dd>

    <dt>GPT-2 공식 가중치 파인튜닝</dt>
    <dd>OpenAI 공개 GPT-2 가중치를 HuggingFace 없이 직접 로드.
    <code>--init_from=gpt2</code> 플래그 하나로 gpt2/gpt2-medium/gpt2-large/gpt2-xl 중 선택.
    특정 도메인 텍스트로 파인튜닝하여 도메인 특화 모델 제작 가능.</dd>
  </dl>

  <h3 id="karpathy-projects">관련 Karpathy 프로젝트</h3>
  <dl>
    <dt>micrograd — autograd 전신</dt>
    <dd>스칼라 autograd 엔진 (Value 클래스) — microgpt Value 클래스의 직접적 원형.
    역전파 원리를 이해하기 위한 가장 작은 구현. 약 150줄.</dd>

    <dt>makemore — 데이터셋·토크나이저 전신</dt>
    <dd>names.txt 데이터셋 출처, 문자 수준 언어 모델. Bigram부터 Transformer까지
    단계적으로 발전시키는 강의 시리즈. microgpt 토크나이저 구조 계승.</dd>

    <dt>nanoGPT — 실용 확장 버전</dt>
    <dd>microgpt에서 배운 원리를 실제 규모로 구현. PyTorch + GPU 기반,
    GPT-2 완전 재현, 분산학습 지원. 연구/실험용 GPT 훈련의 현실적 최소 기준점.</dd>

    <dt>llm.c — 의존성 최소화 극한</dt>
    <dd>C/CUDA로만 LLM 훈련 — PyTorch조차 제거. microgpt의 "의존성 없이 직접 구현" 철학을
    시스템 레벨까지 밀고 나간 프로젝트.</dd>
  </dl>
</section>

<section class="content-section">
  <h2 id="summary">200줄 코드로 얻은 것들</h2>

  <table>
    <thead><tr><th>구성요소</th><th>microgpt 코드</th><th>현대 프레임워크 등가</th></tr></thead>
    <tbody>
      <tr><td><strong>Autograd 엔진</strong></td><td><code>class Value</code></td><td>PyTorch <code>torch.autograd</code></td></tr>
      <tr><td><strong>신경망 레이어</strong></td><td><code>linear, softmax, rmsnorm</code></td><td>PyTorch <code>nn.Linear, nn.Softmax, nn.RMSNorm</code></td></tr>
      <tr><td><strong>GPT 아키텍처</strong></td><td><code>def gpt(...)</code></td><td>HuggingFace <code>GPT2Model</code></td></tr>
      <tr><td><strong>Adam 옵티마이저</strong></td><td>m, v 리스트 + 4줄 업데이트</td><td>PyTorch <code>optim.Adam</code></td></tr>
    </tbody>
  </table>

  <div class="info-box tip">
    <strong>KARPATHY의 핵심 메시지:</strong><br>
    "우리는 실행 규칙이 아니라, <em>학습 규칙</em>을 정의한 것입니다."<br>
    GPT-4도, LLaMA도, 우리의 microgpt도 — 모두 손실을 줄이는 숫자들을 조정합니다.
    4,192개(또는 수조 개)의 차이일 뿐, 원리는 완전히 동일합니다.
    블랙박스는 열렸습니다.
  </div>

  <h3 id="experiments">실험 아이디어 — 파라미터를 바꿔보자</h3>
  <table>
    <thead><tr><th>실험</th><th>변경 내용</th><th>예상 효과</th></tr></thead>
    <tbody>
      <tr><td>더 깊게</td><td><code>n_layer = 4</code></td><td>파라미터 4배↑ 더 자연스러운 이름</td></tr>
      <tr><td>더 넓게</td><td><code>n_embd = 64, n_head = 8</code></td><td>파라미터 ~16배↑ 표현력↑</td></tr>
      <tr><td>더 오래</td><td><code>num_steps = 10000</code></td><td>더 낮은 loss, 더 정확한 이름 패턴</td></tr>
      <tr><td>다른 데이터</td><td>셰익스피어 텍스트</td><td>문장 수준 언어 생성</td></tr>
      <tr><td>온도 실험</td><td><code>temperature = 0.1, 0.5, 1.0, 2.0</code></td><td>창의성 vs 안정성 트레이드오프</td></tr>
      <tr><td>단일 헤드</td><td><code>n_head = 1</code></td><td>멀티헤드 이점 체감 — loss 소폭 상승 예상</td></tr>
      <tr><td>bias 추가</td><td>linear: <code>+b</code> 항 추가</td><td>파라미터 +16개, 수렴 속도 변화 관찰</td></tr>
      <tr><td>std 비교</td><td><code>std=0.01</code> vs <code>0.08</code> vs <code>0.3</code></td><td>초기화 크기와 수렴 안정성의 상관관계</td></tr>
      <tr><td>Adam → SGD</td><td>Adam 업데이트를 <code>p.data -= lr * p.grad</code>로 대체</td><td>적응적 학습률의 중요성 실감</td></tr>
    </tbody>
  </table>
</section>

<!-- ===================================================== -->
<!-- 심화 섹션: 수치 안정성                                 -->
<!-- ===================================================== -->

<section class="content-section">
  <h2 id="numerical-stability">수치 안정성 — 미묘한 함정들</h2>
  <p>순수 파이썬으로 부동소수점 연산을 구현할 때 마주치는 세 가지 핵심 문제를 다룹니다.
  실제 ML 라이브러리들도 동일한 문제를 내부에서 처리합니다.</p>

  <h3 id="log-zero">log(0) 문제 — 교차 엔트로피의 약점</h3>
  <p>Cross-Entropy 손실 <code>-log(P(target))</code>에서 <code>P(target) = 0</code>이면
  <code>math.log(0)</code>은 수학적으로 <code>-∞</code>이고 Python에서는 <code>ValueError</code>를 냅니다.</p>
  <pre><code><span class="cmt"># 언제 발생하는가?</span>
<span class="cmt"># 학습 초기에는 거의 없지만, 극단적 상황에서 발생 가능:</span>
<span class="cmt">#</span>
<span class="cmt"># 1. Softmax 포화(saturation): logit 차이가 매우 크면</span>
<span class="cmt">#    softmax([100, 0, 0, ...]) → [1.0, 0.0, 0.0, ...]</span>
<span class="cmt">#    target이 index 1이라면: probs[1] = 0.0 → log(0) → ValueError!</span>
<span class="cmt">#</span>
<span class="cmt"># 2. Temperature가 매우 작으면:</span>
<span class="cmt">#    softmax([10, 1, 0] / 0.001) = softmax([10000, 1000, 0])</span>
<span class="cmt">#    → probs ≈ [1.0, 0.0, 0.0] → 같은 문제 발생</span>

<span class="cmt"># microgpt의 자연스러운 방어:</span>
<span class="cmt"># 1) softmax 내 max 차감 → overflow 방지 (exp(큰 수) 방지)</span>
<span class="cmt"># 2) std=0.08 작은 초기값 → 초기 logit 포화 방지</span>
<span class="cmt"># 3) Adam 학습률 감쇠 → 학습 후반 포화 방지</span>

<span class="cmt"># 명시적 방어가 필요한 경우 (microgpt에는 없지만 추가 가능):</span>
<span class="kw">def</span> <span class="fn">safe_log</span>(x, eps=<span class="num">1e-8</span>):
    <span class="kw">return</span> (x + eps).<span class="fn">log</span>()   <span class="cmt"># p가 0이어도 log(ε) ≈ -18 (유한)</span>

<span class="cmt"># 또는 log-softmax 직접 계산 (수치적으로 더 안정적):</span>
<span class="kw">def</span> <span class="fn">log_softmax</span>(logits):
    max_val = <span class="fn">max</span>(val.data <span class="kw">for</span> val <span class="kw">in</span> logits)
    log_sum_exp = (<span class="fn">sum</span>([(v - max_val).<span class="fn">exp</span>() <span class="kw">for</span> v <span class="kw">in</span> logits])).<span class="fn">log</span>()
    <span class="kw">return</span> [(v - max_val) - log_sum_exp <span class="kw">for</span> v <span class="kw">in</span> logits]
<span class="cmt"># log_softmax(logits)[target] 를 직접 손실로 사용하면 log(prob) 보다 안정적</span></code></pre>

  <h3 id="softmax-stable">Softmax Max 차감 — 수학적 동치 증명</h3>
  <p>microgpt의 <code>softmax()</code>는 <code>max_val</code>을 먼저 뺍니다. 왜 결과가 같은지 수학적으로 보입니다.</p>
  <pre><code><span class="cmt"># 원본 Softmax: σ(z)ᵢ = exp(zᵢ) / Σⱼ exp(zⱼ)</span>
<span class="cmt">#</span>
<span class="cmt"># 임의의 상수 c를 빼도 결과가 같음을 증명:</span>
<span class="cmt"># σ(z - c)ᵢ = exp(zᵢ - c) / Σⱼ exp(zⱼ - c)</span>
<span class="cmt">#            = exp(zᵢ) × exp(-c) / [Σⱼ exp(zⱼ) × exp(-c)]</span>
<span class="cmt">#            = exp(zᵢ) / Σⱼ exp(zⱼ)   ← exp(-c)가 약분됨!</span>
<span class="cmt">#            = σ(z)ᵢ  ✓</span>
<span class="cmt">#</span>
<span class="cmt"># 실용적 의미:</span>
<span class="cmt"># z = [1000, 999, 998]  (큰 수)</span>
<span class="cmt"># 직접 계산: exp(1000) = 5×10^434 → Python float overflow → inf</span>
<span class="cmt">#            inf / inf = nan → 완전히 망가짐</span>
<span class="cmt">#</span>
<span class="cmt"># max 차감: c = 1000</span>
<span class="cmt"># z - c = [0, -1, -2]</span>
<span class="cmt"># exp([0, -1, -2]) = [1.0, 0.368, 0.135] → 완전 안전!</span></code></pre>

  <div class="info-box tip">
    <strong>언제 오버플로가 발생하는가?</strong>
    Python <code>float</code>은 IEEE 754 배정밀도 (64비트). 최대 표현 값은 약 1.8 × 10^308.
    <code>math.exp(710)</code> 이상이면 <code>OverflowError</code> 발생.
    logit이 710을 넘으면 위험합니다 — max 차감이 이를 방어합니다.
  </div>

  <h3 id="recursion-limit">Python 재귀 깊이 한계 — build_topo 주의</h3>
  <pre><code><span class="cmt"># Python 기본 재귀 한계: sys.getrecursionlimit() = 1000</span>
<span class="cmt">#</span>
<span class="cmt"># microgpt에서 build_topo 재귀 깊이:</span>
<span class="cmt"># loss = (1/n) × sum(losses)</span>
<span class="cmt"># losses = n개 cross-entropy 값</span>
<span class="cmt"># 각 loss = softmax + log 연산 체인</span>
<span class="cmt">#</span>
<span class="cmt"># 한 스텝 처리 시 생성되는 Value 노드 수:</span>
<span class="cmt"># - 임베딩: 16 nodes</span>
<span class="cmt"># - Attention (n_head=4, T 토큰): T × (4×head_dim + softmax) nodes</span>
<span class="cmt"># - MLP: 16→64→16, relu 포함 → ~200 nodes</span>
<span class="cmt"># - loss: n개 토큰의 chain</span>
<span class="cmt">#</span>
<span class="cmt"># "isabella" (8글자) 처리 시:</span>
<span class="cmt"># n = 9 위치, 각 위치당 ~500 nodes → 총 ~4,500 nodes</span>
<span class="cmt"># 재귀 깊이는 그래프 최대 깊이 (체인 길이) ≈ 수백</span>
<span class="cmt"># → 1,000 한계에 걸릴 가능성 있음</span>
<span class="cmt">#</span>
<span class="cmt"># 더 깊은 네트워크(n_layer=4+) 사용 시 재귀 한계 초과 가능:</span>
<span class="kw">import</span> sys
sys.<span class="fn">setrecursionlimit</span>(<span class="num">10000</span>)  <span class="cmt"># 안전하게 늘리기</span></code></pre>

  <h3 id="float-precision">부동소수점 누적 오차</h3>
  <pre><code><span class="cmt"># 스칼라 연산은 텐서 연산보다 누적 오차가 클 수 있음</span>
<span class="cmt"># 예: 1000번의 덧셈에서 오차 누적</span>
<span class="cmt">#</span>
<span class="cmt"># sum([0.1] * 10) == 1.0  → False (≈ 0.9999999999999999)</span>
<span class="cmt"># Python의 부동소수점 특성 — 이진수로 표현할 수 없는 소수들</span>
<span class="cmt">#</span>
<span class="cmt"># microgpt에서는 이 오차가 학습에 무시할 수 있는 수준:</span>
<span class="cmt"># - 4,192개 파라미터 — 매우 작은 모델</span>
<span class="cmt"># - 1,000 스텝 — 짧은 학습</span>
<span class="cmt"># - Adam이 자동으로 보상: v_hat 분모가 적응적으로 조정</span>
<span class="cmt">#</span>
<span class="cmt"># 대규모 모델에서는 bfloat16 또는 float32 + gradient scaling 사용</span></code></pre>

  <table>
    <thead><tr><th>문제</th><th>원인</th><th>microgpt 방어책</th><th>프로덕션 방어책</th></tr></thead>
    <tbody>
      <tr><td>exp() 오버플로</td><td>logit 값이 710 초과</td><td>softmax max 차감</td><td>동일 + bfloat16</td></tr>
      <tr><td>log(0) 오류</td><td>softmax 포화 후 타겟 확률 0</td><td>작은 초기값 + LR 감쇠</td><td>log-softmax + label smoothing</td></tr>
      <tr><td>재귀 한계 초과</td><td>build_topo 재귀 깊이</td><td>단순 구조(n_layer=1)</td><td>iterative DFS 또는 PyTorch</td></tr>
      <tr><td>기울기 소실</td><td>역전파 시 기울기 < 0.01</td><td>잔차 연결</td><td>gradient clipping + warmup</td></tr>
      <tr><td>기울기 폭발</td><td>역전파 시 기울기 > 1e6</td><td>작은 lr + 감쇠</td><td>clip_grad_norm_(max=1.0)</td></tr>
    </tbody>
  </table>
</section>

<!-- ===================================================== -->
<!-- 심화 섹션: 순전파 수치 추적                             -->
<!-- ===================================================== -->

<section class="content-section">
  <h2 id="forward-trace">순전파 수치 추적 — "ab" 이름의 첫 토큰 처리</h2>
  <p>학습 완료 후 <strong>"ab" 이름을 학습하는 한 스텝</strong>의 순전파를 개념적으로 추적합니다.
  실제 수치는 가중치 초기화에 따라 다르지만, 각 단계에서 <em>무슨 일이 일어나는지</em>를 구체적으로 이해합니다.</p>

  <h3 id="forward-step1">Step 0 — BOS(26) 입력, pos=0, 타겟='a'(0)</h3>
  <pre><code><span class="cmt"># ── 임베딩 ──────────────────────────────────────────────</span>
token_id = <span class="num">26</span>  <span class="cmt"># BOS</span>
pos_id   = <span class="num">0</span>

tok_emb = state_dict[<span class="str">'wte'</span>][<span class="num">26</span>]
<span class="cmt"># → [0.031, -0.072, 0.085, -0.041, 0.097, -0.063, ...]  (16개 Value)</span>
<span class="cmt">#    각 값은 N(0, 0.08²)로 초기화된 후 학습으로 조정됨</span>

pos_emb = state_dict[<span class="str">'wpe'</span>][<span class="num">0</span>]
<span class="cmt"># → [0.055, 0.021, -0.088, 0.067, -0.034, 0.092, ...]  (16개 Value)</span>
<span class="cmt">#    위치 0: 이름의 시작 위치 특성 인코딩</span>

x = [t + p <span class="kw">for</span> t, p <span class="kw">in</span> <span class="fn">zip</span>(tok_emb, pos_emb)]
<span class="cmt"># → [0.086, -0.051, -0.003, 0.026, ...]  (BOS + 위치0 합산)</span>

x = <span class="fn">rmsnorm</span>(x)
<span class="cmt"># RMS = sqrt(mean(x²)) ≈ sqrt(0.086²+0.051²+... / 16)</span>
<span class="cmt"># scale = 1/RMS → 각 원소가 단위 RMS로 정규화됨</span>
<span class="cmt"># → [-0.89, 0.52, 0.03, -0.27, ...]  (크기 정규화, 방향 유지)</span>

<span class="cmt"># ── Attention (pos=0: keys가 비어있어 자기 자신만 봄) ──</span>
q = <span class="fn">linear</span>(x, state_dict[<span class="str">'layer0.attn_wq'</span>])  <span class="cmt"># 16×16 행렬 곱 → 16차원</span>
k = <span class="fn">linear</span>(x, state_dict[<span class="str">'layer0.attn_wk'</span>])  <span class="cmt"># 16차원</span>
v = <span class="fn">linear</span>(x, state_dict[<span class="str">'layer0.attn_wv'</span>])  <span class="cmt"># 16차원</span>

keys[<span class="num">0</span>].<span class="fn">append</span>(k)    <span class="cmt"># KV 캐시: 첫 번째 항목 추가</span>
values[<span class="num">0</span>].<span class="fn">append</span>(v)

<span class="cmt"># 각 헤드 (head_dim=4):</span>
<span class="cmt"># 헤드 0: q[0:4] 와 keys[0][0][0:4] 내적 → attn_logit 1개</span>
<span class="cmt"># → attn_logit / 2.0 (= /√4)</span>
<span class="cmt"># → softmax([single_value]) = [1.0]  ← 자기 자신 100% 집중</span>
<span class="cmt"># → head_out = 1.0 × v[0:4]  (value 그대로)</span>
<span class="cmt">#</span>
<span class="cmt"># pos=0에서는 어텐션이 자기 자신에 100% 집중 (다른 선택지 없음!)</span>

x_attn = q  <span class="cmt"># 개념적으로: pos=0에서 attention은 거의 항등 함수</span>

<span class="cmt"># 출력 프로젝션 + 잔차 연결</span>
x = <span class="fn">linear</span>(x_attn, state_dict[<span class="str">'layer0.attn_wo'</span>])
x = [a + b <span class="kw">for</span> a, b <span class="kw">in</span> <span class="fn">zip</span>(x, x_residual)]  <span class="cmt"># 잔차 더하기</span>

<span class="cmt"># ── MLP ────────────────────────────────────────────────</span>
x_residual = x
x = <span class="fn">rmsnorm</span>(x)
x = <span class="fn">linear</span>(x, state_dict[<span class="str">'layer0.mlp_fc1'</span>])   <span class="cmt"># 16 → 64</span>
<span class="cmt"># 64개 중 약 절반이 ReLU로 0이 됨 (죽은 뉴런)</span>
x = [xi.<span class="fn">relu</span>() <span class="kw">for</span> xi <span class="kw">in</span> x]                   <span class="cmt"># 음수 → 0</span>
x = <span class="fn">linear</span>(x, state_dict[<span class="str">'layer0.mlp_fc2'</span>])   <span class="cmt"># 64 → 16</span>
x = [a + b <span class="kw">for</span> a, b <span class="kw">in</span> <span class="fn">zip</span>(x, x_residual)]

<span class="cmt"># ── 언임베딩 ────────────────────────────────────────────</span>
logits = <span class="fn">linear</span>(x, state_dict[<span class="str">'lm_head'</span>])     <span class="cmt"># 16 → 27</span>
<span class="cmt"># 학습 후: 'a','e','i','m','l' 등의 logit이 높아야 함</span>
<span class="cmt"># (이름은 모음이나 흔한 자음으로 시작하는 경우 많음)</span>
<span class="cmt">#</span>
<span class="cmt"># 예시 (학습 완료 후):</span>
<span class="cmt"># logits ≈ {'a': 2.3, 'l': 2.1, 'k': 1.9, 'e': 1.8, ..., 'x': 0.2, 'z': 0.1}</span>
<span class="cmt"># BOS 타겟이 아닌 이름 시작 글자들이 높은 점수</span>

probs = <span class="fn">softmax</span>(logits)
<span class="cmt"># 'a' 확률: exp(2.3) / Σ exp(logits) ≈ 0.12  (12% 확률)</span>
<span class="cmt"># 타겟이 'a'(0)이므로: loss = -log(0.12) ≈ 2.12</span></code></pre>

  <div class="info-box info">
    <strong>pos=0의 특수성:</strong> 첫 번째 토큰(BOS)을 처리할 때 KV 캐시가 비어있습니다.
    어텐션은 자기 자신에게만 100% 집중할 수밖에 없습니다 — softmax([x]) = [1.0].
    이 때의 어텐션 연산은 사실상 항등 함수에 가깝습니다.
    실질적인 어텐션의 힘은 <strong>pos ≥ 1 부터</strong> 발휘됩니다.
  </div>

  <h3 id="forward-step2">Step 1 — 'a'(0) 입력, pos=1, 타겟='b'(1)</h3>
  <pre><code><span class="cmt"># keys[0]에는 이제 [K_BOS] 1개 → pos=1에서는 2개를 봄</span>
<span class="cmt"># 'a' + 위치1 임베딩 → x 계산 (이전과 동일 과정)</span>
<span class="cmt"># ...</span>
<span class="cmt"># Attention에서:</span>
<span class="cmt"># 각 헤드: q_h[현재] × K_BOS / 2  AND  q_h[현재] × K_a / 2</span>
<span class="cmt"># attn_logits = [score_BOS, score_a]  (2개)</span>
<span class="cmt"># weights = softmax([score_BOS, score_a])  → 합이 1</span>
<span class="cmt">#</span>
<span class="cmt"># 예: weights ≈ [0.3, 0.7]  ("a가 BOS보다 더 관련있다")</span>
<span class="cmt"># head_out = 0.3 × V_BOS[0:4] + 0.7 × V_a[0:4]</span>
<span class="cmt">#</span>
<span class="cmt"># 이 어텐션이 "a 다음엔 무엇이 오는가"를 학습합니다.</span>
<span class="cmt"># names.txt에서 'a' 다음에 자주 오는 글자들이 높은 확률로 학습됨</span>
keys[<span class="num">0</span>].<span class="fn">append</span>(k)   <span class="cmt"># K_a 추가, 이제 [K_BOS, K_a]</span>
values[<span class="num">0</span>].<span class="fn">append</span>(v)

<span class="cmt"># 최종 logits:</span>
<span class="cmt"># 학습 후 'b','d','l','n','r','s' 등이 높은 점수</span>
<span class="cmt"># (영어 이름에서 'a' 다음에 자주 오는 글자들)</span>
<span class="cmt"># 타겟='b'(1): loss = -log(P('b'))</span></code></pre>

  <h3 id="forward-dimensions">차원 변환 요약표</h3>
  <table>
    <thead><tr><th>단계</th><th>입력 형태</th><th>출력 형태</th><th>연산</th><th>파라미터</th></tr></thead>
    <tbody>
      <tr><td>토큰 임베딩</td><td>scalar (token_id)</td><td>[16]</td><td>table lookup</td><td>wte[27×16]</td></tr>
      <tr><td>위치 임베딩</td><td>scalar (pos_id)</td><td>[16]</td><td>table lookup</td><td>wpe[16×16]</td></tr>
      <tr><td>합산 + RMSNorm</td><td>[16] + [16]</td><td>[16]</td><td>add + scale</td><td>없음</td></tr>
      <tr><td>Q/K/V 프로젝션</td><td>[16]</td><td>[16] × 3</td><td>linear</td><td>3 × [16×16]</td></tr>
      <tr><td>어텐션 (T토큰)</td><td>[16] + T×[16]</td><td>[16]</td><td>scaled dot-product</td><td>없음(캐시 사용)</td></tr>
      <tr><td>출력 프로젝션</td><td>[16]</td><td>[16]</td><td>linear</td><td>[16×16]</td></tr>
      <tr><td>MLP fc1</td><td>[16]</td><td>[64]</td><td>linear + ReLU</td><td>[64×16]</td></tr>
      <tr><td>MLP fc2</td><td>[64]</td><td>[16]</td><td>linear</td><td>[16×64]</td></tr>
      <tr><td>lm_head</td><td>[16]</td><td>[27]</td><td>linear</td><td>[27×16]</td></tr>
      <tr><td>softmax</td><td>[27]</td><td>[27]</td><td>exp / sum</td><td>없음</td></tr>
      <tr><td>cross-entropy</td><td>[27] probs</td><td>scalar</td><td>-log(probs[target])</td><td>없음</td></tr>
    </tbody>
  </table>
</section>

<!-- ===================================================== -->
<!-- 심화 섹션: AdamW & 옵티마이저 변형                     -->
<!-- ===================================================== -->

<section class="content-section">
  <h2 id="adamw">AdamW &amp; 최신 옵티마이저 변형</h2>
  <p>microgpt의 Adam 구현을 기준으로 프로덕션 옵티마이저들과의 차이를 비교합니다.</p>

  <h3 id="adam-vs-adamw">Adam vs AdamW — Weight Decay의 위치가 결정적</h3>
  <pre><code><span class="cmt"># ── Adam + L2 Regularization (잘못된 방식) ──</span>
<span class="cmt"># L2: loss에 λ×‖W‖² 추가 → gradient에 λ×W 추가</span>
g_l2 = p.grad + lambda_wd * p.data   <span class="cmt"># L2 포함 gradient</span>
m[i] = beta1 * m[i] + (<span class="num">1</span> - beta1) * g_l2
v[i] = beta2 * v[i] + (<span class="num">1</span> - beta2) * g_l2**<span class="num">2</span>
<span class="cmt"># 문제: Adam이 g_l2의 크기를 정규화해버림</span>
<span class="cmt"># v_hat에 의한 나눗셈이 weight decay 효과를 희석시킴</span>
<span class="cmt"># → weight decay가 실제로는 잘 작동하지 않음!</span>

<span class="cmt"># ── AdamW (올바른 방식, Loshchilov & Hutter 2017) ──</span>
m[i] = beta1 * m[i] + (<span class="num">1</span> - beta1) * p.grad     <span class="cmt"># 순수 gradient만</span>
v[i] = beta2 * v[i] + (<span class="num">1</span> - beta2) * p.grad**<span class="num">2</span>  <span class="cmt"># 순수 gradient만</span>
m_hat = m[i] / (<span class="num">1</span> - beta1**t)
v_hat = v[i] / (<span class="num">1</span> - beta2**t)
p.data -= lr_t * m_hat / (v_hat**<span class="num">0.5</span> + eps_adam)  <span class="cmt"># Adam 업데이트</span>
p.data -= lr_t * lambda_wd * p.data                <span class="cmt"># weight decay 별도 적용!</span>
<span class="cmt"># → weight decay가 Adam 적응 학습률에 희석되지 않고 독립적으로 작용</span>
<span class="cmt"># → GPT-3, LLaMA, nanoGPT가 사용하는 방식</span></code></pre>

  <div class="info-box info">
    <strong>microgpt에 weight decay가 없는 이유:</strong> 4,192개 파라미터, 32,033개 훈련 예제 — 모델이 너무 작아 과적합 위험이 낮습니다.
    weight decay는 파라미터가 많고 데이터가 적을 때 (과적합 방지) 중요합니다.
    <strong>nanoGPT</strong>는 <code>weight_decay=0.1</code>을 사용하며 임베딩/bias에는 적용하지 않습니다.
  </div>

  <h3 id="lion-optimizer">Lion 옵티마이저 (Evolved Sign Momentum, 2023)</h3>
  <pre><code><span class="cmt"># Google Brain의 프로그래밍으로 진화된 옵티마이저</span>
<span class="cmt"># Adam보다 메모리 효율적: m 버퍼 1개만 필요 (v 버퍼 불필요)</span>
<span class="cmt">#</span>
<span class="cmt"># Lion 업데이트 규칙:</span>
<span class="kw">for</span> i, p <span class="kw">in</span> <span class="fn">enumerate</span>(params):
    <span class="cmt"># 업데이트 방향: m과 gradient의 선형 조합</span>
    update = <span class="fn">sign</span>(beta1 * m[i] + (<span class="num">1</span> - beta1) * p.grad)
    p.data -= lr_t * (update + lambda_wd * p.data)
    <span class="cmt"># 모멘텀 업데이트 (업데이트 후)</span>
    m[i] = beta2 * m[i] + (<span class="num">1</span> - beta2) * p.grad

<span class="cmt"># 핵심 차이: sign() 사용 → 업데이트 크기가 항상 ±lr로 고정</span>
<span class="cmt"># Adam: lr × m_hat / √v_hat  (크기 가변)</span>
<span class="cmt"># Lion: lr × sign(...)       (크기 고정)</span>
<span class="cmt">#</span>
<span class="cmt"># 장점: v 버퍼 불필요 → 메모리 33% 절약 (m+v → m만)</span>
<span class="cmt"># 단점: 적절한 lr 조정 필요 (보통 Adam의 10배 작게)</span></code></pre>

  <h3 id="lr-schedulers">학습률 스케줄러 완전 비교</h3>
  <table>
    <thead><tr><th>스케줄러</th><th>수식</th><th>사용처</th><th>특징</th></tr></thead>
    <tbody>
      <tr>
        <td><strong>Constant</strong></td>
        <td>lr_t = lr</td>
        <td>단순 실험</td>
        <td>후반 수렴 불안정</td>
      </tr>
      <tr>
        <td><strong>Linear Decay</strong><br>(microgpt)</td>
        <td>lr_t = lr × (1 - t/T)</td>
        <td>짧은 학습 (1K 스텝)</td>
        <td>구현 단순, 후반 급격히 감소</td>
      </tr>
      <tr>
        <td><strong>Step Decay</strong></td>
        <td>lr_t = lr × γ^⌊t/s⌋</td>
        <td>전통적 CNN 학습</td>
        <td>단계적 감소, 불연속</td>
      </tr>
      <tr>
        <td><strong>Cosine Decay</strong></td>
        <td>lr_t = lr_min + ½(lr-lr_min)(1 + cos(πt/T))</td>
        <td>GPT-2/3, nanoGPT</td>
        <td>부드러운 감소, 최솟값 보장</td>
      </tr>
      <tr>
        <td><strong>Warmup + Cosine</strong><br>(GPT-3 방식)</td>
        <td>선형 warmup → cosine decay</td>
        <td>대규모 LLM 학습 표준</td>
        <td>초반 불안정 방지 + 부드러운 수렴</td>
      </tr>
      <tr>
        <td><strong>1-Cycle</strong></td>
        <td>lr 상승 → 감소</td>
        <td>FastAI 방식</td>
        <td>빠른 수렴, 소규모 실험</td>
      </tr>
    </tbody>
  </table>

  <pre><code><span class="cmt"># Cosine Decay with Linear Warmup (nanoGPT 방식)</span>
warmup_steps = <span class="num">100</span>   <span class="cmt"># 전체 스텝의 10%</span>
lr_min = <span class="num">0.0001</span>      <span class="cmt"># 최솟값 (lr의 10%)</span>

<span class="kw">def</span> <span class="fn">get_lr</span>(t):
    <span class="kw">if</span> t < warmup_steps:
        <span class="kw">return</span> learning_rate * t / warmup_steps  <span class="cmt"># 선형 증가</span>
    <span class="cmt"># Cosine decay</span>
    progress = (t - warmup_steps) / (num_steps - warmup_steps)
    <span class="kw">return</span> lr_min + <span class="num">0.5</span> * (learning_rate - lr_min) * (<span class="num">1</span> + math.<span class="fn">cos</span>(math.pi * progress))

<span class="cmt"># microgpt 선형 감쇠와 비교:</span>
<span class="cmt"># step=0:    linear=0.01000, cosine=0.0000 (warmup 시작)</span>
<span class="cmt"># step=100:  linear=0.00900, cosine=0.0100 (warmup 완료)</span>
<span class="cmt"># step=500:  linear=0.00500, cosine=0.0055 (cosine 중반)</span>
<span class="cmt"># step=999:  linear=0.00001, cosine=0.0001 (cosine 끝)</span></code></pre>

  <div class="info-box tip">
    <strong>왜 Warmup이 필요한가?</strong> 학습 초반에는 m, v 버퍼가 0으로 초기화되어 있어
    바이어스 보정 후에도 불안정합니다. Warmup 동안 lr을 서서히 높이면
    파라미터들이 먼저 "적응"한 뒤 큰 lr로 빠르게 학습합니다.
    microgpt처럼 1,000 스텝의 짧은 학습에는 warmup이 불필요하지만,
    수십만 스텝의 LLM 학습에서는 필수입니다.
  </div>
</section>

<!-- ===================================================== -->
<!-- 심화 섹션: 고급 샘플링 전략                             -->
<!-- ===================================================== -->

<section class="content-section">
  <h2 id="sampling-strategies">고급 샘플링 전략</h2>
  <p>microgpt의 temperature 샘플링 외에, 프로덕션 LLM에서 사용하는 다양한 샘플링 전략을 구현합니다.</p>

  <h3 id="greedy-decoding">Greedy Decoding — 항상 가장 확률 높은 토큰</h3>
  <pre><code><span class="cmt"># Temperature 없이 argmax 선택</span>
<span class="cmt"># 장점: 결정적(deterministic), 항상 같은 출력</span>
<span class="cmt"># 단점: 반복 루프에 빠질 수 있음, 다양성 없음</span>

<span class="kw">def</span> <span class="fn">greedy_decode</span>(logits):
    <span class="kw">return</span> <span class="fn">max</span>(<span class="fn">range</span>(<span class="fn">len</span>(logits)), key=<span class="kw">lambda</span> i: logits[i].data)

<span class="cmt"># microgpt 추론에서 사용 예:</span>
<span class="cmt"># token_id = greedy_decode(logits)  # temperature 없이</span>

<span class="cmt"># 주의: temperature=0은 ZeroDivisionError → greedy_decode()를 대신 사용</span></code></pre>

  <h3 id="top-k">Top-K 샘플링</h3>
  <pre><code><span class="cmt"># 확률 상위 K개 토큰만 남기고 나머지는 -∞ 처리 후 샘플링</span>
<span class="cmt"># 장점: 낮은 확률의 이상한 토큰을 완전 차단</span>
<span class="cmt"># 단점: K 선택이 어려움 (K=1은 greedy, K=vocab_size는 pure sampling)</span>

<span class="kw">def</span> <span class="fn">top_k_sample</span>(logits, k=<span class="num">10</span>, temperature=<span class="num">1.0</span>):
    <span class="cmt"># 1. 상위 K개 인덱스 찾기</span>
    sorted_indices = <span class="fn">sorted</span>(<span class="fn">range</span>(<span class="fn">len</span>(logits)),
                              key=<span class="kw">lambda</span> i: logits[i].data, reverse=<span class="kw">True</span>)
    top_k_indices = sorted_indices[:k]

    <span class="cmt"># 2. 상위 K개 logit만 추출</span>
    top_k_logits = [logits[i] / temperature <span class="kw">for</span> i <span class="kw">in</span> top_k_indices]

    <span class="cmt"># 3. softmax → 샘플링</span>
    top_k_probs = <span class="fn">softmax</span>(top_k_logits)
    local_idx = random.<span class="fn">choices</span>(<span class="fn">range</span>(k), weights=[p.data <span class="kw">for</span> p <span class="kw">in</span> top_k_probs])[<span class="num">0</span>]
    <span class="kw">return</span> top_k_indices[local_idx]

<span class="cmt"># k=5 예시: 상위 5개 ['a', 'e', 'l', 'k', 'm'] 만 고려</span>
<span class="cmt"># 나머지 22개 글자는 아무리 작은 확률이어도 완전히 제외</span></code></pre>

  <h3 id="top-p">Top-P (Nucleus) 샘플링</h3>
  <pre><code><span class="cmt"># 확률의 누적 합이 p를 넘을 때까지 상위 토큰을 포함</span>
<span class="cmt"># 장점: 확률 분포가 넓을 때(불확실할 때)는 더 많은 토큰 포함</span>
<span class="cmt">#       확률 분포가 좁을 때(확실할 때)는 적은 토큰만 포함</span>
<span class="cmt"># Top-K보다 적응적 → 더 자연스러운 텍스트 생성</span>

<span class="kw">def</span> <span class="fn">top_p_sample</span>(logits, p=<span class="num">0.9</span>, temperature=<span class="num">1.0</span>):
    <span class="cmt"># 1. 내림차순 정렬</span>
    sorted_indices = <span class="fn">sorted</span>(<span class="fn">range</span>(<span class="fn">len</span>(logits)),
                              key=<span class="kw">lambda</span> i: logits[i].data, reverse=<span class="kw">True</span>)
    sorted_logits = [logits[i] / temperature <span class="kw">for</span> i <span class="kw">in</span> sorted_indices]
    sorted_probs = <span class="fn">softmax</span>(sorted_logits)

    <span class="cmt"># 2. 누적 확률이 p를 초과하는 최소 집합 찾기</span>
    nucleus = []
    cumulative = <span class="num">0.0</span>
    <span class="kw">for</span> idx, prob <span class="kw">in</span> <span class="fn">zip</span>(sorted_indices, sorted_probs):
        nucleus.<span class="fn">append</span>((idx, prob))
        cumulative += prob.data
        <span class="kw">if</span> cumulative >= p:
            <span class="kw">break</span>

    <span class="cmt"># 3. nucleus에서 샘플링</span>
    indices = [item[<span class="num">0</span>] <span class="kw">for</span> item <span class="kw">in</span> nucleus]
    weights = [item[<span class="num">1</span>].data <span class="kw">for</span> item <span class="kw">in</span> nucleus]
    <span class="kw">return</span> random.<span class="fn">choices</span>(indices, weights=weights)[<span class="num">0</span>]

<span class="cmt"># p=0.9 예시:</span>
<span class="cmt"># 확률이 고른 경우: [0.1, 0.09, 0.08, ...] → 누적 0.9 위해 10개+ 포함</span>
<span class="cmt"># 확률이 집중된 경우: [0.85, 0.08, ...] → 누적 0.9 위해 2개만 포함</span></code></pre>

  <h3 id="repetition-penalty">Repetition Penalty</h3>
  <pre><code><span class="cmt"># 이미 생성된 토큰의 logit을 낮춰 반복 방지</span>
<span class="cmt"># GPT-2 공식 생성에서 사용</span>

<span class="kw">def</span> <span class="fn">apply_repetition_penalty</span>(logits, generated, penalty=<span class="num">1.3</span>):
    <span class="kw">for</span> token_id <span class="kw">in</span> <span class="fn">set</span>(generated):   <span class="cmt"># 중복 처리 방지</span>
        <span class="kw">if</span> logits[token_id].data > <span class="num">0</span>:
            logits[token_id] = logits[token_id] / <span class="type">Value</span>(penalty)  <span class="cmt"># 양수 logit 낮추기</span>
        <span class="kw">else</span>:
            logits[token_id] = logits[token_id] * <span class="type">Value</span>(penalty)  <span class="cmt"># 음수 logit 더 낮추기</span>
    <span class="kw">return</span> logits

<span class="cmt"># penalty=1.0: 효과 없음</span>
<span class="cmt"># penalty=1.3: 이미 나온 글자의 확률 약 23% 감소</span>
<span class="cmt"># penalty=2.0: 이미 나온 글자의 확률 50% 감소 (과도하면 이상한 이름)</span></code></pre>

  <table>
    <thead><tr><th>전략</th><th>적합 상황</th><th>파라미터</th><th>특징</th></tr></thead>
    <tbody>
      <tr><td><strong>Greedy</strong></td><td>테스트, 재현성 필요</td><td>없음</td><td>항상 같은 결과, 반복 위험</td></tr>
      <tr><td><strong>Temperature</strong><br>(microgpt 기본)</td><td>일반 이름 생성</td><td>T=0.5~1.0</td><td>균형 잡힌 다양성</td></tr>
      <tr><td><strong>Top-K</strong></td><td>특정 품질 기준 유지</td><td>K=10~50</td><td>이상한 토큰 완전 차단</td></tr>
      <tr><td><strong>Top-P (Nucleus)</strong></td><td>LLM 프로덕션 표준</td><td>p=0.9~0.95</td><td>적응적 후보 집합</td></tr>
      <tr><td><strong>Temperature + Top-P</strong></td><td>GPT-4 기본값</td><td>T=1.0, p=0.9</td><td>두 방법의 조합</td></tr>
      <tr><td><strong>Repetition Penalty</strong></td><td>반복 방지 필요 시</td><td>penalty=1.2~1.5</td><td>이미 생성된 토큰 억제</td></tr>
    </tbody>
  </table>
</section>

<!-- ===================================================== -->
<!-- 심화 섹션: PyTorch 마이그레이션                         -->
<!-- ===================================================== -->

<section class="content-section">
  <h2 id="pytorch-migration">PyTorch 마이그레이션 가이드 — microgpt → nanoGPT</h2>
  <p>microgpt의 각 구성요소를 PyTorch로 1:1 변환하는 방법을 코드와 함께 보여줍니다.
  이 과정에서 수동 구현의 의미가 더욱 명확해집니다.</p>

  <h3 id="pytorch-value">① Value 클래스 → torch.Tensor</h3>
  <pre><code><span class="cmt"># ── microgpt ──────────────────────────────────────────</span>
a = <span class="type">Value</span>(<span class="num">2.0</span>)
b = <span class="type">Value</span>(<span class="num">3.0</span>)
c = a * b + a   <span class="cmt"># 계산 그래프 자동 구성</span>
c.<span class="fn">backward</span>()   <span class="cmt"># 위상 정렬 + 연쇄 법칙</span>
<span class="fn">print</span>(a.grad)   <span class="cmt"># 4.0</span>
<span class="fn">print</span>(b.grad)   <span class="cmt"># 2.0</span>

<span class="cmt"># ── PyTorch 동치 ──────────────────────────────────────</span>
<span class="kw">import</span> torch
a = torch.<span class="fn">tensor</span>(<span class="num">2.0</span>, requires_grad=<span class="kw">True</span>)  <span class="cmt"># requires_grad=True ← 역전파 대상</span>
b = torch.<span class="fn">tensor</span>(<span class="num">3.0</span>, requires_grad=<span class="kw">True</span>)
c = a * b + a
c.<span class="fn">backward</span>()   <span class="cmt"># PyTorch 내부에서 동일한 위상 정렬 + 연쇄 법칙</span>
<span class="fn">print</span>(a.grad)   <span class="cmt"># tensor(4.)</span>
<span class="fn">print</span>(b.grad)   <span class="cmt"># tensor(2.)</span>

<span class="cmt"># 차이점:</span>
<span class="cmt"># - Value: 스칼라 하나 = 객체 1개  (느림, 파이썬 오버헤드)</span>
<span class="cmt"># - Tensor: 배열 전체 = 객체 1개  (빠름, C++/CUDA 연산)</span></code></pre>

  <h3 id="pytorch-nn">② 헬퍼 함수 → torch.nn 모듈</h3>
  <pre><code><span class="kw">import</span> torch
<span class="kw">import</span> torch.nn <span class="kw">as</span> nn
<span class="kw">import</span> torch.nn.functional <span class="kw">as</span> F

<span class="cmt"># ── linear(x, w) → nn.Linear ──</span>
<span class="cmt"># microgpt:</span>
<span class="fn">linear</span>(x, state_dict[<span class="str">'layer0.attn_wq'</span>])  <span class="cmt"># list 내포, O(n³) 스칼라 루프</span>

<span class="cmt"># PyTorch:</span>
attn_wq = nn.<span class="fn">Linear</span>(n_embd, n_embd, bias=<span class="kw">False</span>)
attn_wq(x_tensor)  <span class="cmt"># BLAS/cuBLAS 행렬 곱 → GPU에서 수천 배 빠름</span>

<span class="cmt"># ── softmax(logits) → F.softmax ──</span>
<span class="cmt"># microgpt:</span>
<span class="fn">softmax</span>(logits)  <span class="cmt"># max 차감 + exp + sum + div, 스칼라</span>

<span class="cmt"># PyTorch:</span>
F.<span class="fn">softmax</span>(logits_tensor, dim=-<span class="num">1</span>)  <span class="cmt"># 내부에서 동일한 max 차감 적용, 텐서 연산</span>

<span class="cmt"># ── rmsnorm(x) → nn.RMSNorm ──</span>
<span class="cmt"># microgpt:</span>
<span class="fn">rmsnorm</span>(x)  <span class="cmt"># 학습 가능 γ 없음</span>

<span class="cmt"># PyTorch (≥2.4):</span>
rms_norm = nn.<span class="fn">RMSNorm</span>(n_embd, eps=<span class="num">1e-5</span>)  <span class="cmt"># 학습 가능 γ 있음 (기본 1로 초기화)</span>
rms_norm(x_tensor)

<span class="cmt"># 또는 직접 구현:</span>
<span class="kw">def</span> <span class="fn">rmsnorm_torch</span>(x, eps=<span class="num">1e-5</span>):
    ms = (x ** <span class="num">2</span>).<span class="fn">mean</span>(dim=-<span class="num">1</span>, keepdim=<span class="kw">True</span>)
    <span class="kw">return</span> x * (ms + eps) ** -<span class="num">0.5</span></code></pre>

  <h3 id="pytorch-gpt">③ gpt() 함수 → nn.Module</h3>
  <pre><code><span class="kw">import</span> torch
<span class="kw">import</span> torch.nn <span class="kw">as</span> nn
<span class="kw">import</span> torch.nn.functional <span class="kw">as</span> F

<span class="kw">class</span> <span class="type">MicroGPT</span>(nn.<span class="type">Module</span>):
    <span class="kw">def</span> <span class="fn">__init__</span>(self, vocab_size=<span class="num">27</span>, n_embd=<span class="num">16</span>, n_head=<span class="num">4</span>, n_layer=<span class="num">1</span>, block_size=<span class="num">16</span>):
        <span class="fn">super</span>().<span class="fn">__init__</span>()
        self.wte = nn.<span class="fn">Embedding</span>(vocab_size, n_embd)   <span class="cmt"># microgpt: state_dict['wte']</span>
        self.wpe = nn.<span class="fn">Embedding</span>(block_size, n_embd)   <span class="cmt"># microgpt: state_dict['wpe']</span>
        self.layers = nn.<span class="fn">ModuleList</span>([<span class="fn">Block</span>(n_embd, n_head) <span class="kw">for</span> _ <span class="kw">in</span> <span class="fn">range</span>(n_layer)])
        self.lm_head = nn.<span class="fn">Linear</span>(n_embd, vocab_size, bias=<span class="kw">False</span>)
        <span class="cmt"># Weight Tying: wte와 lm_head 공유 (nanoGPT 방식)</span>
        self.lm_head.weight = self.wte.weight           <span class="cmt"># 파라미터 432개 절약!</span>

    <span class="kw">def</span> <span class="fn">forward</span>(self, idx):  <span class="cmt"># idx: [B, T] 토큰 인덱스</span>
        B, T = idx.shape
        pos = torch.<span class="fn">arange</span>(<span class="num">0</span>, T, device=idx.device)    <span class="cmt"># [T]</span>

        x = self.<span class="fn">wte</span>(idx) + self.<span class="fn">wpe</span>(pos)             <span class="cmt"># [B, T, n_embd]</span>
        x = F.<span class="fn">rms_norm</span>(x, [n_embd])                    <span class="cmt"># RMSNorm</span>

        <span class="kw">for</span> layer <span class="kw">in</span> self.layers:
            x = layer(x)                                <span class="cmt"># [B, T, n_embd]</span>

        logits = self.<span class="fn">lm_head</span>(x)                       <span class="cmt"># [B, T, vocab_size]</span>
        <span class="kw">return</span> logits

<span class="cmt"># 학습 루프 (PyTorch 버전)</span>
model = <span class="type">MicroGPT</span>()
optimizer = torch.optim.<span class="fn">Adam</span>(model.<span class="fn">parameters</span>(), lr=<span class="num">0.01</span>,
                              betas=(<span class="num">0.85</span>, <span class="num">0.99</span>), eps=<span class="num">1e-8</span>)

<span class="kw">for</span> step <span class="kw">in</span> <span class="fn">range</span>(num_steps):
    <span class="cmt"># 토큰 텐서: [1, T] → [T-1] input, [T-1] target</span>
    tokens = torch.<span class="fn">tensor</span>(token_sequence).<span class="fn">unsqueeze</span>(<span class="num">0</span>)  <span class="cmt"># [1, T]</span>
    logits = <span class="fn">model</span>(tokens[:, :-<span class="num">1</span>])                      <span class="cmt"># [1, T-1, 27]</span>
    loss = F.<span class="fn">cross_entropy</span>(logits.<span class="fn">view</span>(-<span class="num">1</span>, <span class="num">27</span>),
                            tokens[:, <span class="num">1</span>:].<span class="fn">view</span>(-<span class="num">1</span>))       <span class="cmt"># scalar</span>

    optimizer.<span class="fn">zero_grad</span>()   <span class="cmt"># grad 초기화 (microgpt: p.grad = 0)</span>
    loss.<span class="fn">backward</span>()         <span class="cmt"># 역전파</span>
    optimizer.<span class="fn">step</span>()         <span class="cmt"># Adam 업데이트</span></code></pre>

  <h3 id="migration-speed">예상 속도 향상</h3>
  <table>
    <thead><tr><th>구현 방식</th><th>1000 스텝 소요 시간</th><th>속도 비율</th><th>특징</th></tr></thead>
    <tbody>
      <tr><td>microgpt (순수 파이썬)</td><td>~5~10분</td><td>1×</td><td>의존성 0, 교육 목적</td></tr>
      <tr><td>NumPy 벡터화</td><td>~10~30초</td><td>~20×</td><td>스칼라 → 배열 연산</td></tr>
      <tr><td>PyTorch CPU</td><td>~1~5초</td><td>~100×</td><td>자동 최적화, BLAS 활용</td></tr>
      <tr><td>PyTorch GPU (RTX 3090)</td><td>~0.1초</td><td>~1000×</td><td>병렬 텐서 연산, CUDA</td></tr>
    </tbody>
  </table>

  <div class="info-box info">
    <strong>어떤 것을 먼저 배워야 하는가?</strong>
    microgpt → NumPy → PyTorch CPU → PyTorch GPU 순서로 발전하면,
    각 단계에서 <em>왜 더 빠른지</em>를 이해하면서 배울 수 있습니다.
    PyTorch만 배우면 <code>loss.backward()</code>가 마법처럼 느껴지지만,
    microgpt를 먼저 구현하면 "아, 이게 위상 정렬 + 연쇄 법칙이구나"를 알게 됩니다.
  </div>
</section>

<!-- ===================================================== -->
<!-- 심화 섹션: 디버깅 가이드                               -->
<!-- ===================================================== -->

<section class="content-section">
  <h2 id="debugging">디버깅 가이드 &amp; 흔한 실수</h2>
  <p>microgpt.py를 직접 실행하거나 변형할 때 마주치는 오류들을 유형별로 정리합니다.</p>

  <h3 id="runtime-errors">런타임 오류 — 즉시 크래시</h3>
  <table>
    <thead><tr><th>오류</th><th>원인</th><th>해결</th></tr></thead>
    <tbody>
      <tr>
        <td><code>ValueError: math domain error</code></td>
        <td><code>math.log(0)</code> 또는 음수 입력</td>
        <td>softmax 결과 확인, <code>eps=1e-8</code> 추가, 또는 log-softmax 사용</td>
      </tr>
      <tr>
        <td><code>ValueError: 'E' is not in list</code></td>
        <td><code>uchars.index('E')</code> — 대문자 입력</td>
        <td>입력 텍스트 소문자 변환: <code>doc.lower()</code></td>
      </tr>
      <tr>
        <td><code>ZeroDivisionError</code></td>
        <td><code>temperature = 0</code> 설정</td>
        <td>temperature &gt; 0 유지, 또는 <code>greedy_decode()</code> 사용</td>
      </tr>
      <tr>
        <td><code>RecursionError: maximum recursion depth exceeded</code></td>
        <td><code>build_topo</code> 재귀가 Python 기본 한계 1000 초과</td>
        <td><code>sys.setrecursionlimit(10000)</code> 또는 iterative DFS 구현</td>
      </tr>
      <tr>
        <td><code>IndexError: list index out of range</code></td>
        <td><code>vocab_size=26</code>으로 잘못 지정, BOS(26)가 범위 밖</td>
        <td><code>vocab_size = len(uchars) + 1 = 27</code> 확인</td>
      </tr>
      <tr>
        <td><code>OverflowError: math range error</code></td>
        <td><code>math.exp()</code>에 710 이상 입력</td>
        <td>softmax max 차감 확인, 또는 lr 줄이기 (logit 폭발 가능성)</td>
      </tr>
    </tbody>
  </table>

  <h3 id="convergence-issues">학습 수렴 문제</h3>
  <dl>
    <dt>❌ Loss가 3.30 (랜덤 베이스라인)에서 전혀 내려가지 않는 경우</dt>
    <dd>
      <ul>
        <li><code>loss.backward()</code>가 실행되지 않거나, <code>p.grad</code>가 모두 0인지 확인</li>
        <li>파라미터 업데이트 후 <code>p.grad = 0</code>이 아닌 <code>p.grad = 0.0</code>으로 해야 함 (실수형)</li>
        <li><code>params</code> 리스트에 일부 파라미터가 빠졌는지 확인</li>
        <li>Adam 버퍼 <code>m</code>, <code>v</code>가 params와 길이가 같은지 확인</li>
      </ul>
    </dd>
    <dt>❌ Loss가 발산하는 경우 (NaN 또는 무한대)</dt>
    <dd>
      <ul>
        <li>학습률이 너무 큼 → <code>learning_rate = 0.001</code>로 줄이기</li>
        <li>초기화 std가 너무 큼 → <code>std = 0.02</code>로 줄이기</li>
        <li>softmax 오버플로 → max 차감 확인</li>
        <li><code>eps_adam = 1e-8</code>이 너무 작은 경우 드물게 발생 → <code>1e-6</code>으로 늘리기</li>
      </ul>
    </dd>
    <dt>❌ Loss가 2.7~2.8에서 멈추는 경우</dt>
    <dd>
      <ul>
        <li>정상 범위: 1,000 스텝으로는 2.37이 목표 (데이터와 모델 크기 한계)</li>
        <li><code>num_steps = 5000</code>으로 늘리면 더 낮아질 수 있음</li>
        <li><code>n_layer = 2, n_embd = 32</code>로 모델 크기 증가 → 더 빠른 수렴</li>
      </ul>
    </dd>
    <dt>❌ 생성된 이름이 모두 동일한 경우</dt>
    <dd>
      <ul>
        <li>temperature가 너무 낮음 → <code>temperature = 0.8~1.0</code>으로 높이기</li>
        <li><code>random.seed()</code>가 반복 설정되어 있는지 확인 (추론 루프 안에 있으면 안 됨)</li>
      </ul>
    </dd>
    <dt>❌ 생성된 이름이 모두 단 1~2글자인 경우</dt>
    <dd>
      <ul>
        <li>모델이 BOS를 너무 쉽게 생성 → 학습이 부족하거나 temperature가 너무 낮음</li>
        <li>더 많은 학습 스텝 또는 높은 temperature 시도</li>
      </ul>
    </dd>
  </dl>

  <h3 id="silent-bugs">조용한 버그 — 오류 없이 잘못된 결과</h3>
  <pre><code><span class="cmt"># 버그 1: grad 초기화를 업데이트 전에 하는 경우</span>
<span class="kw">for</span> i, p <span class="kw">in</span> <span class="fn">enumerate</span>(params):
    p.grad = <span class="num">0</span>              <span class="cmt"># ❌ 잘못됨: grad를 먼저 초기화하면 m, v 계산이 0 기반</span>
    m[i] = beta1 * m[i] + (<span class="num">1</span>-beta1) * p.grad  <span class="cmt"># ← 항상 0!</span>
    <span class="cmt"># ...</span>

<span class="cmt"># 올바른 순서: 업데이트 후 초기화</span>
<span class="kw">for</span> i, p <span class="kw">in</span> <span class="fn">enumerate</span>(params):
    g = p.grad              <span class="cmt"># ✓ 먼저 grad 읽기</span>
    m[i] = beta1 * m[i] + (<span class="num">1</span>-beta1) * g
    v[i] = beta2 * v[i] + (<span class="num">1</span>-beta2) * g**<span class="num">2</span>
    <span class="cmt"># ... 업데이트 ...</span>
    p.grad = <span class="num">0</span>              <span class="cmt"># ✓ 마지막에 초기화</span>

<span class="cmt"># 버그 2: keys/values를 초기화하지 않고 재사용</span>
<span class="kw">for</span> step <span class="kw">in</span> <span class="fn">range</span>(num_steps):
    <span class="cmt"># ❌ 잘못됨: 이전 이름의 K, V가 누적되어 있음</span>
    <span class="kw">for</span> pos_id <span class="kw">in</span> <span class="fn">range</span>(n):
        logits = <span class="fn">gpt</span>(token_id, pos_id, keys, values)

<span class="cmt"># ✓ 올바름: 각 이름 처리 전 KV 캐시 초기화</span>
<span class="kw">for</span> step <span class="kw">in</span> <span class="fn">range</span>(num_steps):
    keys, values = [[] <span class="kw">for</span> _ <span class="kw">in</span> <span class="fn">range</span>(n_layer)], [[] <span class="kw">for</span> _ <span class="kw">in</span> <span class="fn">range</span>(n_layer)]
    <span class="kw">for</span> pos_id <span class="kw">in</span> <span class="fn">range</span>(n):
        logits = <span class="fn">gpt</span>(token_id, pos_id, keys, values)

<span class="cmt"># 버그 3: state_dict를 함수 내부에서 선언하는 경우</span>
<span class="kw">def</span> <span class="fn">gpt</span>(token_id, pos_id, keys, values):
    state_dict = { ... }  <span class="cmt"># ❌ 매 호출마다 재초기화! 학습이 전혀 안 됨</span>
<span class="cmt"># ✓ state_dict는 함수 밖 전역 스코프에 선언해야 함</span></code></pre>

  <h3 id="debug-checklist">디버깅 체크리스트</h3>
  <ol>
    <li>학습 시작 전 <code>loss.data</code>가 <code>log(27) ≈ 3.296</code> 근처인지 확인 (랜덤 베이스라인)</li>
    <li>10 스텝 후 loss가 감소하고 있는지 확인 (그래프나 print)</li>
    <li><code>len(params)</code>가 정확히 4,192인지 확인</li>
    <li>추론 전 <code>keys, values</code>가 새로 초기화되었는지 확인</li>
    <li>vocab_size가 <code>len(uchars) + 1 = 27</code>인지 확인</li>
    <li>모든 입력 문자가 소문자인지 확인</li>
  </ol>
</section>

<!-- ===================================================== -->
<!-- 심화 섹션: 성능 분석                                   -->
<!-- ===================================================== -->

<section class="content-section">
  <h2 id="performance">성능 분석 &amp; 시간 복잡도</h2>
  <p>microgpt의 각 단계별 계산 비용을 분석하고, 왜 느린지, 어디서 병목이 발생하는지를 이해합니다.</p>

  <h3 id="complexity">시간 복잡도 — 기호 정의</h3>
  <ul>
    <li><strong>B</strong> = batch size (microgpt: 1)</li>
    <li><strong>T</strong> = 시퀀스 길이 (이름 길이, 최대 16)</li>
    <li><strong>d</strong> = n_embd = 16 (임베딩 차원)</li>
    <li><strong>H</strong> = n_head = 4 (어텐션 헤드 수)</li>
    <li><strong>d_h</strong> = head_dim = d/H = 4 (헤드당 차원)</li>
    <li><strong>V</strong> = vocab_size = 27</li>
    <li><strong>4d</strong> = MLP 확장 차원 = 64</li>
  </ul>

  <table>
    <thead><tr><th>연산</th><th>순전파 복잡도</th><th>실제 연산 수 (T=5, d=16)</th></tr></thead>
    <tbody>
      <tr><td>임베딩 조회 (wte + wpe)</td><td>O(d)</td><td>32 덧셈</td></tr>
      <tr><td>RMSNorm</td><td>O(d)</td><td>~64 연산</td></tr>
      <tr><td>Q, K, V 프로젝션 (각각)</td><td>O(d²)</td><td>16×16 = 256 (×3)</td></tr>
      <tr><td>어텐션 점수 (T 토큰, H 헤드)</td><td>O(T × d)</td><td>5 × 4 × 4 = 80 내적</td></tr>
      <tr><td>Softmax (T개)</td><td>O(T)</td><td>~50 연산</td></tr>
      <tr><td>Value 가중합</td><td>O(T × d)</td><td>5 × 4 × 4 = 80</td></tr>
      <tr><td>출력 프로젝션</td><td>O(d²)</td><td>16×16 = 256</td></tr>
      <tr><td>MLP (fc1 + relu + fc2)</td><td>O(d × 4d)</td><td>16×64 + 64×16 = 2,048</td></tr>
      <tr><td>lm_head</td><td>O(V × d)</td><td>27×16 = 432</td></tr>
      <tr><th>총 순전파</th><th>O(T×d² + T²×d)</th><th>~4,000 스칼라 연산 (T=5 기준)</th></tr>
    </tbody>
  </table>

  <pre><code><span class="cmt"># 전체 학습 1000 스텝 연산 추정:</span>
<span class="cmt">#</span>
<span class="cmt"># 이름당 평균 길이 ≈ 5.5글자 → 평균 T ≈ 7 (BOS 포함)</span>
<span class="cmt"># 순전파당 스칼라 곱셈: ~6,000개</span>
<span class="cmt"># 역전파: 순전파의 약 2배 → ~12,000개</span>
<span class="cmt"># Value 객체당 Python 오버헤드: ~1μs</span>
<span class="cmt">#</span>
<span class="cmt"># 한 스텝: ~18,000 스칼라 연산 × 1μs = 18ms</span>
<span class="cmt"># 1,000 스텝: ~18초  (실제: ~5~10분 — Python 객체 오버헤드가 큼)</span>
<span class="cmt">#</span>
<span class="cmt"># NumPy 벡터화 시:</span>
<span class="cmt"># 18,000 스칼라 곱 → 1번의 16×16 행렬 곱 (BLAS: ~1μs)</span>
<span class="cmt"># 예상 속도 향상: 18ms → 0.1ms = 180배</span></code></pre>

  <h3 id="bottleneck">Python 구현 병목 분석</h3>
  <table>
    <thead><tr><th>병목</th><th>microgpt 속도</th><th>NumPy 등가</th><th>속도 차이</th></tr></thead>
    <tbody>
      <tr>
        <td>행렬-벡터 곱 (linear)</td>
        <td>O(n²) 중첩 for 루프<br>스칼라 Value 곱셈</td>
        <td><code>np.dot(W, x)</code><br>BLAS 최적화</td>
        <td>100~1000×</td>
      </tr>
      <tr>
        <td>softmax 분모 합산</td>
        <td><code>sum(exps)</code> Python 리스트<br>__radd__ 체인</td>
        <td><code>np.sum(exps)</code><br>벡터 연산</td>
        <td>50~200×</td>
      </tr>
      <tr>
        <td>역전파 위상 정렬</td>
        <td>DFS 재귀<br>Python 함수 호출 오버헤드</td>
        <td>PyTorch autograd<br>C++ 연산 그래프</td>
        <td>1000×+</td>
      </tr>
      <tr>
        <td>Value 객체 생성</td>
        <td>한 스텝당 ~수만 개<br>Python 힙 할당</td>
        <td>텐서 뷰 (zero-copy)</td>
        <td>100×+</td>
      </tr>
    </tbody>
  </table>

  <h3 id="complexity-attention">어텐션의 O(T²) 복잡도 — GPT의 근본 한계</h3>
  <pre><code><span class="cmt"># Attention 행렬: T × T (모든 토큰 쌍의 점수)</span>
<span class="cmt"># T=16  → 16×16 = 256 점수</span>
<span class="cmt"># T=512 → 512×512 = 262,144 점수 (1,024배!)</span>
<span class="cmt"># T=128K (Claude 3.5) → 128K×128K ≈ 16.4억 점수</span>
<span class="cmt">#</span>
<span class="cmt"># 이 O(T²) 문제를 해결하는 기술들:</span>
<span class="cmt">#</span>
<span class="cmt"># 1. Flash Attention (Dao et al., 2022):</span>
<span class="cmt">#    동일한 O(T²) 연산이지만 메모리 접근 패턴 최적화</span>
<span class="cmt">#    GPU 메모리 O(T²) → O(T) (행렬 저장 없이 on-the-fly 계산)</span>
<span class="cmt">#    nanoGPT: F.scaled_dot_product_attention() 으로 자동 적용</span>
<span class="cmt">#</span>
<span class="cmt"># 2. Sliding Window Attention (Mistral):</span>
<span class="cmt">#    각 토큰이 이전 W개만 봄 → O(T × W)</span>
<span class="cmt">#    W=4096이면 긴 시퀀스에서도 효율적</span>
<span class="cmt">#</span>
<span class="cmt"># 3. GQA (Grouped Query Attention, LLaMA-2/3):</span>
<span class="cmt">#    여러 Q 헤드가 K, V를 공유 → KV 캐시 크기 감소</span>
<span class="cmt">#    n_head=32, n_kv_head=8 → K, V 메모리 4배 절약</span></code></pre>

  <h3 id="kv-cache-memory">KV 캐시 메모리 사용량</h3>
  <pre><code><span class="cmt"># microgpt KV 캐시 분석:</span>
<span class="cmt"># 레이어당: T개 키 + T개 값, 각각 n_embd=16 차원</span>
<span class="cmt"># T=16 (최대), n_layer=1:</span>
<span class="cmt"># 메모리 = 2 × 16 × 16 = 512 Value 객체</span>
<span class="cmt">#        × ~150 bytes/객체 ≈ 76.8 KB (매우 작음)</span>
<span class="cmt">#</span>
<span class="cmt"># GPT-4 KV 캐시 비교 (n_embd=12288, n_layer=96, T=128K):</span>
<span class="cmt"># 2 × 128K × 12288 × 96 × 4 bytes (float32)</span>
<span class="cmt"># ≈ 2 × 128K × 12288 × 96 × 4 ≈ 300 GB (!)</span>
<span class="cmt"># 실제로는 fp16 + GQA 로 대폭 줄임</span></code></pre>

  <div class="info-box tip">
    <strong>성능 개선 로드맵 요약:</strong><br>
    microgpt (5~10분) → NumPy 벡터화 (~30초) → PyTorch CPU (~5초) →
    PyTorch GPU (0.1초) → Flash Attention + 배치 처리 (0.01초).
    각 단계마다 10~100배 개선이 가능하며, 원리는 모두 동일합니다.
  </div>
</section>

<section class="content-section">
  <h2 id="references">참고 자료</h2>

  <h3>원본 소스</h3>
  <ul>
    <li><a href="https://karpathy.github.io/2026/02/12/microgpt/" target="_blank" rel="noopener">Andrej Karpathy — microgpt 블로그 포스트 (2026.02.12)</a> — 저자 직접 작성 공식 가이드. 데이터셋~추론 전체 해설</li>
    <li><a href="https://gist.github.com/karpathy/8627fe009c40f57531cb18360106ce95" target="_blank" rel="noopener">microgpt.py 원본 소스 코드 (GitHub Gist)</a> — ~200줄 순수 파이썬. 토크나이저·Autograd·GPT·Adam·추론 전체</li>
    <li><a href="https://github.com/karpathy" target="_blank" rel="noopener">@karpathy GitHub</a> — 저자의 모든 오픈소스 교육용 AI 프로젝트</li>
    <li><a href="https://x.com/karpathy" target="_blank" rel="noopener">@karpathy X(Twitter)</a> — 최신 연구·강의 업데이트</li>
  </ul>

  <h3>관련 Karpathy 프로젝트</h3>
  <table>
    <thead><tr><th>프로젝트</th><th>설명</th><th>관계</th></tr></thead>
    <tbody>
      <tr>
        <td><a href="https://github.com/karpathy/micrograd" target="_blank" rel="noopener">micrograd</a></td>
        <td>스칼라 autograd 엔진 — Value 클래스, 위상 정렬 역전파 (~150줄)</td>
        <td>microgpt Value 클래스의 직접적 원형</td>
      </tr>
      <tr>
        <td><a href="https://github.com/karpathy/makemore" target="_blank" rel="noopener">makemore</a></td>
        <td>문자 수준 언어 모델 — Bigram → MLP → Transformer 단계별 강의</td>
        <td>names.txt 데이터셋 출처, 토크나이저 구조 계승</td>
      </tr>
      <tr>
        <td><a href="https://github.com/karpathy/nanoGPT" target="_blank" rel="noopener">nanoGPT</a></td>
        <td>실용적 GPT 훈련 — PyTorch, Flash Attention, DDP, GPT-2 재현, Shakespeare 예제</td>
        <td>microgpt의 "Production 확장" — 같은 원리, 실용적 규모</td>
      </tr>
      <tr>
        <td><a href="https://github.com/karpathy/llm.c" target="_blank" rel="noopener">llm.c</a></td>
        <td>C/CUDA로 LLM 훈련 — PyTorch조차 없이 구현</td>
        <td>의존성 제거 철학의 극한 — microgpt 방향의 시스템 레벨 확장</td>
      </tr>
    </tbody>
  </table>

  <h3>학습 영상 (Karpathy YouTube)</h3>
  <ul>
    <li><a href="https://www.youtube.com/watch?v=5NQ2NBRfctU" target="_blank" rel="noopener">안드레 카파시가 들려주는 GPT 이야기 | microgpt.py</a> — microgpt.py 전체 코드를 직접 설명하는 한국어 해설 영상</li>
    <li><a href="https://www.youtube.com/watch?v=kCc8FmEb1nY" target="_blank" rel="noopener">Let's build GPT: from scratch, in code, spelled out</a> — nanoGPT 라이브 코딩 (2시간). 학습 루프·Adam·추론을 실시간으로 구현</li>
    <li><a href="https://www.youtube.com/watch?v=VMj-3S1tku0" target="_blank" rel="noopener">The spelled-out intro to neural networks and backpropagation: building micrograd</a> — micrograd + 역전파 강의 (2시간25분)</li>
    <li><a href="https://www.youtube.com/watch?v=PaCmpygFfXo" target="_blank" rel="noopener">The spelled-out intro to language modeling: building makemore</a> — names.txt + 문자 수준 LM</li>
    <li><a href="https://www.youtube.com/watch?v=TCH_1BHY58I" target="_blank" rel="noopener">Let's build a GPT Tokenizer</a> — BPE 토크나이저 (2시간). microgpt 문자 수준 토크나이저와 비교</li>
    <li><a href="https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ" target="_blank" rel="noopener">Neural Networks: Zero to Hero (재생목록)</a> — Karpathy 전체 강의 시리즈 (micrograd → makemore → GPT)</li>
  </ul>

  <h3>핵심 논문</h3>
  <ul>
    <li><a href="https://arxiv.org/abs/1412.6980" target="_blank" rel="noopener">Adam: A Method for Stochastic Optimization (Kingma &amp; Ba, 2014)</a> — microgpt가 구현한 Adam 옵티마이저 원논문. β1=0.9, β2=0.999, ε=1e-8 기본값 출처</li>
    <li><a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">Attention Is All You Need (Vaswani et al., 2017)</a> — Transformer 원논문. Q·K·V·Scaled Dot-Product·Multi-head·Cross-Entropy 수식 출처</li>
    <li><a href="https://openai.com/research/language-unsupervised" target="_blank" rel="noopener">GPT-2: Language Models are Unsupervised Multitask Learners (OpenAI, 2019)</a> — microgpt가 "follow GPT-2"라고 명시한 기준 아키텍처</li>
    <li><a href="https://arxiv.org/abs/2005.14165" target="_blank" rel="noopener">GPT-3: Language Models are Few-Shot Learners (Brown et al., 2020)</a> — nanoGPT의 Cosine LR with warmup 스케줄 출처</li>
    <li><a href="https://arxiv.org/abs/1910.07467" target="_blank" rel="noopener">Root Mean Square Layer Normalization (Zhang &amp; Sennrich, 2019)</a> — microgpt의 <code>rmsnorm</code> 수식 근거</li>
    <li><a href="https://arxiv.org/abs/2205.14135" target="_blank" rel="noopener">FlashAttention: Fast and Memory-Efficient Exact Attention (Dao et al., 2022)</a> — nanoGPT가 채택한 IO-aware 어텐션 알고리즘</li>
    <li><a href="https://arxiv.org/abs/2002.05202" target="_blank" rel="noopener">Press &amp; Wolf (2016) — Using the Output Embedding to Improve Language Models</a> — Weight Tying 원논문</li>
    <li><a href="https://arxiv.org/abs/1603.05027" target="_blank" rel="noopener">Identity Mappings in Deep Residual Networks (He et al., 2016)</a> — Pre-LN vs Post-LN 잔차 연결 연구</li>
  </ul>

  <h3>관련 vibecoding 문서</h3>
  <ul>
    <li><a href="llm-theory-foundations.html">LLM 이론 기초</a></li>
    <li><a href="llm-theory-advanced.html">LLM 이론 심화 (Transformer, Attention 상세)</a></li>
    <li><a href="llm-theory-math-appendix.html">LLM 이론 수학 부록 (어텐션 수식 유도)</a></li>
    <li><a href="llm-handbook-training.html">LLM 핸드북: 학습·정렬·추론</a></li>
    <li><a href="rag-guide.html">RAG 완전 가이드</a></li>
  </ul>
</section>

<div class="page-nav"></div>
</main>

<aside class="inline-toc">
  <div class="toc-title">목차</div>
  <div class="toc-nav"></div>
</aside>
<footer class="site-footer"></footer>
</div>
<script src="../js/main.js"></script>
</body>
</html>
