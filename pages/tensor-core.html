<!DOCTYPE html>
<html lang="ko" data-theme="dark-kernel">
<head>
<!-- BEGIN: Google adsense -->
<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-2110881342960271" crossorigin="anonymous"></script>
<!-- END: Google adsense -->
<!-- BEGIN: Google adsense repair -->
<script async src="https://fundingchoicesmessages.google.com/i/pub-2110881342960271?ers=1" nonce="laI6FT8gpRxugDJv5AGJRA"></script><script nonce="laI6FT8gpRxugDJv5AGJRA">(function() {function signalGooglefcPresent() {if (!window.frames['googlefcPresent']) {if (document.body) {const iframe = document.createElement('iframe'); iframe.style = 'width: 0; height: 0; border: none; z-index: -1000; left: -1000px; top: -1000px;'; iframe.style.display = 'none'; iframe.name = 'googlefcPresent'; document.body.appendChild(iframe);} else {setTimeout(signalGooglefcPresent, 0);}}}signalGooglefcPresent();})();</script>
<!-- END: Google adsense repair -->
<!-- BEGIN: Google analytics -->
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-1VWQF060SX"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-1VWQF060SX');
</script>
<!-- END: Google analytics -->

<!-- Flash 방지: 쿠키에서 테마 즉시 적용 -->
<script>
(function(){var m=document.cookie.match(/claude_theme=([^;]+)/);if(m)document.documentElement.setAttribute('data-theme',m[1]);})();
</script>
<meta charset="UTF-8">
<meta property="og:type" content="article">
<meta property="og:site_name" content="AI Vibe Coding 가이드 /with MINZKN">
<meta property="og:title" content="NVIDIA Tensor Core 완전 가이드">
<meta property="og:description" content="NVIDIA Tensor Core의 원리, 세대별 발전, LLM 추론·학습 최적화 기법을 상세히 소개합니다">
<meta property="og:url" content="https://minzkn.com/claude/pages/tensor-core.html">
<meta property="og:image" content="https://minzkn.com/claude/images/og-image.png">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="NVIDIA Tensor Core의 원리, 세대별 발전, LLM 추론·학습 최적화 기법을 상세히 소개합니다">
<meta name="keywords" content="tensor core nvidia gpu deep learning matrix multiplicationwmma cuda llm 추론 학습 최적화 volta ampere hopper">
<meta name="author" content="MINZKN">
<title>Tensor Core 완전 가이드 - AI Vibe Coding 가이드 /with MINZKN</title>
<link rel="icon" type="image/svg+xml" href="../images/favicon.svg">
<link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/joungkyun/font-d2coding/d2coding.css">
<link rel="stylesheet" href="../css/themes.css">
<link rel="stylesheet" href="../css/style.css">
<link rel="stylesheet" href="../css/responsive.css">
</head>
<body>
<div class="page-wrapper">

<!-- ===== Header ===== -->
<header class="site-header">
</header>

<!-- ===== Side Navigation ===== -->
<nav class="side-nav" aria-label="사이트 내비게이션">
</nav>

<!-- ===== Main Content ===== -->
<main class="main-content">
<div class="breadcrumb"></div>

<h1 id="top">NVIDIA Tensor Core 완전 가이드</h1>
<p class="page-description">NVIDIA Tensor Core의 기본 원리부터 LLM 최적화까지, 딥러닝 추론과 학습을 가속화하는 GPU 핵심 기술의 모든 것</p>

<section class="content-section">
  <h2 id="what-is-tensor-core">Tensor Core란?</h2>
  <p>Tensor Core는 NVIDIA가 2017년 Volta 아키텍처에서 처음 도입한 전용 행렬 곱셈 가속 하드웨어입니다. 전통적인 CUDA 코어 대비 행렬 연산에서 최대 12배의 성능 향상을 제공하며, 특히 딥러닝의 핵심 연산인 행렬 곱셈(Matrix Multiplication)을 하드웨어 수준에서 가속화합니다.</p>

  <div class="info-box info">
    <strong>핵심 개념:</strong> Tensor Core는 FP16, BF16, FP32, TF32, INT8, INT4 등 다양한 정밀도를 지원하는 행렬 누산(Matrix Accumulate) 연산을 단일 클럭 사이클에서 수행합니다.
  </div>

  <h3 id="cpu-vs-cuda-vs-tensor">CPU, CUDA Core, Tensor Core 비교</h3>
  <table>
    <thead>
      <tr>
        <th>특성</th>
        <th>CPU</th>
        <th>CUDA Core</th>
        <th>Tensor Core</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>행렬 곱셈 방식</td>
        <td>순차 처리</td>
        <td>병렬 처리 (SIMT)</td>
        <td>하드웨어 가속 (FMA)</td>
      </tr>
      <tr>
        <td>FP32 연산</td>
        <td>1회/사이클</td>
        <td>1회/사이클</td>
        <td>64회/사이클 (A100)</td>
      </tr>
      <tr>
        <td>FP16 연산</td>
        <td>1회/사이클</td>
        <td>2회/사이클</td>
        <td>512회/사이클 (A100)</td>
      </tr>
      <tr>
        <td>주요 용도</td>
        <td>일반 연산</td>
        <td>그래픽, 범용 병렬</td>
        <td>딥러닝 행렬 연산</td>
      </tr>
    </tbody>
  </table>

  <div class="diagram-container">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 700 320"
         style="width:100%;max-width:700px;display:block;margin:1.5em auto;">
      <defs>
        <marker id="tc-arrow" viewBox="0 0 10 7" refX="10" refY="3.5"
                markerWidth="10" markerHeight="7" orient="auto">
          <polygon points="0 0, 10 3.5, 0 7" fill="var(--diagram-arrow)"/>
        </marker>
      </defs>

      <!-- CPU Layer -->
      <rect x="20" y="20" width="500" height="50" rx="6"
            fill="var(--bg-secondary)" stroke="var(--border-color)" stroke-width="1.5"/>
      <text x="150" y="50" text-anchor="middle" fill="var(--diagram-text)" font-size="14" font-weight="bold">
        CPU (Scalar Processing)
      </text>
      <text x="450" y="50" text-anchor="middle" fill="var(--text-secondary)" font-size="11">
        Performance: 1x
      </text>

      <line x1="270" y1="70" x2="270" y2="95"
            stroke="var(--diagram-arrow)" stroke-width="2" marker-end="url(#tc-arrow)"/>

      <!-- CUDA Core Layer -->
      <rect x="20" y="95" width="500" height="50" rx="6"
            fill="var(--bg-secondary)" stroke="var(--accent-primary)" stroke-width="2"/>
      <text x="150" y="125" text-anchor="middle" fill="var(--diagram-text)" font-size="14" font-weight="bold">
        CUDA Core (SIMT Parallel)
      </text>
      <text x="450" y="125" text-anchor="middle" fill="var(--diagram-accent)" font-size="11" font-weight="bold">
        Performance: ~10x
      </text>

      <line x1="270" y1="145" x2="270" y2="170"
            stroke="var(--diagram-arrow)" stroke-width="2" marker-end="url(#tc-arrow)"/>

      <!-- Tensor Core Layer -->
      <rect x="20" y="170" width="500" height="50" rx="6"
            fill="var(--accent-primary)" fill-opacity="0.2"
            stroke="var(--accent-primary)" stroke-width="3"/>
      <text x="150" y="200" text-anchor="middle" fill="var(--diagram-text)" font-size="14" font-weight="bold">
        Tensor Core (Matrix FMA Hardware)
      </text>
      <text x="450" y="200" text-anchor="middle" fill="var(--diagram-accent)" font-size="11" font-weight="bold">
        Performance: ~100x
      </text>

      <line x1="270" y1="220" x2="270" y2="245"
            stroke="var(--diagram-arrow)" stroke-width="2" marker-end="url(#tc-arrow)"/>

      <!-- Operation Flow -->
      <rect x="20" y="245" width="500" height="50" rx="6"
            fill="var(--bg-secondary)" stroke="var(--border-color)" stroke-width="1.5"/>
      <text x="150" y="275" text-anchor="middle" fill="var(--diagram-text)" font-size="13">
        Input A (M×K) × Input B (K×N) → Output C (M×N)
      </text>

      <!-- Legend Box -->
      <rect x="540" y="20" width="140" height="275" rx="6"
            fill="var(--diagram-accent)" fill-opacity="0.1"
            stroke="var(--diagram-accent)" stroke-width="1.5" stroke-dasharray="5,3"/>
      <text x="610" y="40" text-anchor="middle" fill="var(--diagram-accent)" font-size="11" font-weight="bold">
        Performance
      </text>
      <text x="610" y="70" text-anchor="middle" fill="var(--text-secondary)" font-size="9">
        (FLOPS 기준)
      </text>
      <text x="610" y="105" text-anchor="middle" fill="var(--diagram-text)" font-size="10">
        CPU: 1x
      </text>
      <text x="610" y="145" text-anchor="middle" fill="var(--diagram-accent)" font-size="10">
        CUDA: ~10x
      </text>
      <text x="610" y="185" text-anchor="middle" fill="var(--diagram-accent)" font-size="10" font-weight="bold">
        Tensor: ~100x
      </text>
      <text x="610" y="210" text-anchor="middle" fill="var(--text-secondary)" font-size="8">
        (FP16 MMA 기준)
      </text>
      <text x="610" y="240" text-anchor="middle" fill="var(--text-secondary)" font-size="9">
        A100 16×16
      </text>
      <text x="610" y="260" text-anchor="middle" fill="var(--text-secondary)" font-size="9">
        행렬 연산
      </text>
    </svg>
    <p class="diagram-caption">그림 1: CPU에서 Tensor Core로의 연산 처리 방식 변화</p>
  </div>
</section>

<section class="content-section">
  <h2 id="how-it-works">동작 원리</h2>
  <p>Tensor Core는 WMMA(Warp-level Matrix Multiply-Accumulate) 연산을 통해 행렬 곱셈을 가속화합니다. 한 번의 명령어로 M×K × K×N 행렬 곱셈을 수행하고 결과를 M×N 행렬에 누산합니다.</p>

  <h3 id="wmma-operation">WMMA 연산 구조</h3>
  <p>Tensor Core의 핵심은 Fused Multiply-Add (FMA) 연산을 행렬 단위로 확장한 것입니다:</p>
  <pre><code><span class="cmt">// CUDA에서 WMMA 연산 예시 (Pseudo-code)</span>
<span class="kw">using</span> wmma = <span class="fn">nvidia::wmma</span>;

<span class="cmt">// 16×16 행렬 조각 로드</span>
fragment&lt;matrix_a&gt; <span class="fn">a_frag</span>;
fragment&lt;matrix_b&gt; <span class="fn">b_frag</span>;
fragment&lt;accumulator&gt; <span class="fn">c_frag</span>;

<span class="fn">load_matrix_sync</span>(a_frag, a_data, <span class="num">16</span>);
<span class="fn">load_matrix_sync</span>(b_frag, b_data, <span class="num">16</span>);
<span class="fn">load_matrix_sync</span>(c_frag, c_data, <span class="num">16</span>);

<span class="cmt">// 행렬 곱셈 누산: C += A × B</span>
<span class="fn">mma_sync</span>(c_frag, a_frag, b_frag, c_frag);

<span class="cmt">// 결과 저장</span>
<span class="fn">store_matrix_sync</span>(c_data, c_frag, <span class="num">16</span>, mem_row_major);</code></pre>

  <div class="diagram-container">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 700 360"
         style="width:100%;max-width:700px;display:block;margin:1.5em auto;">
      <defs>
        <marker id="tc-flow-arrow" viewBox="0 0 10 7" refX="10" refY="3.5"
                markerWidth="10" markerHeight="7" orient="auto">
          <polygon points="0 0, 10 3.5, 0 7" fill="var(--diagram-arrow)"/>
        </marker>
      </defs>

      <!-- Matrix A -->
      <rect x="30" y="30" width="120" height="80" rx="4"
            fill="var(--diagram-accent)" fill-opacity="0.3"
            stroke="var(--diagram-accent)" stroke-width="2"/>
      <text x="90" y="55" text-anchor="middle" fill="var(--diagram-text)" font-size="11">
        Matrix A
      </text>
      <text x="90" y="75" text-anchor="middle" fill="var(--diagram-text)" font-size="10">
        (M × K)
      </text>

      <!-- Matrix B -->
      <rect x="30" y="130" width="120" height="80" rx="4"
            fill="var(--diagram-accent)" fill-opacity="0.3"
            stroke="var(--diagram-accent)" stroke-width="2"/>
      <text x="90" y="155" text-anchor="middle" fill="var(--diagram-text)" font-size="11">
        Matrix B
      </text>
      <text x="90" y="175" text-anchor="middle" fill="var(--diagram-text)" font-size="10">
        (K × N)
      </text>

      <!-- Tensor Core Operation -->
      <rect x="250" y="80" width="160" height="100" rx="8"
            fill="var(--accent-primary)" fill-opacity="0.2"
            stroke="var(--accent-primary)" stroke-width="3"/>
      <text x="330" y="115" text-anchor="middle" fill="var(--diagram-text)" font-size="13" font-weight="bold">
        Tensor Core
      </text>
      <text x="330" y="135" text-anchor="middle" fill="var(--diagram-text)" font-size="11">
        MMA (FMA × Matrix)
      </text>
      <text x="330" y="155" text-anchor="middle" fill="var(--diagram-text)" font-size="10">
        C += A × B
      </text>

      <!-- Arrows A → TC -->
      <line x1="150" y1="70" x2="250" y2="100"
            stroke="var(--diagram-arrow)" stroke-width="2" marker-end="url(#tc-flow-arrow)"/>

      <!-- Arrow B → TC -->
      <line x1="150" y1="170" x2="250" y2="160"
            stroke="var(--diagram-arrow)" stroke-width="2" marker-end="url(#tc-flow-arrow)"/>

      <!-- Matrix C -->
      <rect x="520" y="80" width="150" height="100" rx="4"
            fill="var(--diagram-accent)" fill-opacity="0.2"
            stroke="var(--diagram-accent)" stroke-width="2"/>
      <text x="595" y="115" text-anchor="middle" fill="var(--diagram-text)" font-size="11" font-weight="bold">
        Matrix C
      </text>
      <text x="595" y="135" text-anchor="middle" fill="var(--diagram-text)" font-size="10">
        Accumulator
      </text>
      <text x="595" y="155" text-anchor="middle" fill="var(--diagram-text)" font-size="10">
        (M × N)
      </text>

      <!-- Arrow TC → C -->
      <line x1="410" y1="130" x2="520" y2="130"
            stroke="var(--diagram-arrow)" stroke-width="2" marker-end="url(#tc-flow-arrow)"/>

      <!-- Dimension Explanation Box -->
      <rect x="30" y="240" width="640" height="100" rx="6"
            fill="var(--bg-secondary)" stroke="var(--border-color)" stroke-width="1.5"/>
      <text x="350" y="265" text-anchor="middle" fill="var(--diagram-text)" font-size="11" font-weight="bold">
        행렬 곱셈 차원 설명
      </text>
      <text x="350" y="290" text-anchor="middle" fill="var(--diagram-text)" font-size="10">
        M = 출력 행 수 | K = 입력 차원 (공통) | N = 출력 열 수
      </text>
      <text x="350" y="310" text-anchor="middle" fill="var(--text-secondary)" font-size="9">
        예: A(M×K) × B(K×N) → C(M×N) | M=-batch, K=embedding, N=vocab
      </text>
    </svg>
    <p class="diagram-caption">그림 2: Tensor Core WMMA 연산 데이터 플로우</p>
  </div>

  <h3 id="precision-formats">지원 정밀도 형식</h3>
  <p>Tensor Core는 세대에 따라 다양한 정밀도를 지원합니다. 특히 Blackwell Ultra의 NVFP4는 NVIDIA 특화 포맷으로, 기존 FP4보다 높은 정확도를 제공합니다.</p>

  <div class="info-box info">
    <strong>NVFP4란?</strong> NVIDIA가 Blackwell Ultra에서 도입한 특화 4비트 부동소수점 포맷입니다. 2단계 스케일링(E4M3 FP8 마이크로블록 스케일 + 텐서 레벨 FP32 스케일)을 사용하여 하드웨어 가속 양자화를 구현합니다. FP8과 유사한 정확도(약 1% 이내 차이)를 유지하면서 메모리 사용량을 크게 줄입니다.
  </div>
  <table>
    <thead>
      <tr>
        <th>정밀도</th>
        <th>비트</th>
        <th>주요 용도</th>
        <th>Tensor Core 지원</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>FP32</td>
        <td>32</td>
        <td>학습, 정밀 연산</td>
        <td>✓ (Hopper)</td>
      </tr>
      <tr>
        <td>TF32</td>
        <td>32</td>
        <td>학습 (자동 변환)</td>
        <td>✓ (Ampere+)</td>
      </tr>
      <tr>
        <td>FP16</td>
        <td>16</td>
        <td>학습, 추론</td>
        <td>✓ (Volta+)</td>
      </tr>
      <tr>
        <td>BF16</td>
        <td>16</td>
        <td>학습 (동적 범위)</td>
        <td>✓ (Ampere+)</td>
      </tr>
      <tr>
        <td>FP8</td>
        <td>8</td>
        <td>추론 (Hopper)</td>
        <td>✓ (Hopper)</td>
      </tr>
      <tr>
        <td>INT8</td>
        <td>8</td>
        <td>추론</td>
        <td>✓ (Turing+)</td>
      </tr>
      <tr>
        <td>INT4</td>
        <td>4</td>
        <td>추론 (양자화)</td>
        <td>✓ (Hopper+)</td>
      </tr>
      <tr>
        <td>FP6</td>
        <td>6</td>
        <td>추론 (Blackwell+)</td>
        <td>✓ (Blackwell)</td>
      </tr>
      <tr>
        <td>NVFP4</td>
        <td>4</td>
        <td>추론 (Blackwell Ultra)</td>
        <td>✓ (Blackwell Ultra)</td>
      </tr>
    </tbody>
  </table>
</section>

<section class="content-section">
  <h2 id="generation-evolution">세대별 발전</h2>

  <h3 id="volta-v100">Volta (V100, 2017)</h3>
  <p>최초의 Tensor Core가 도입된 세대입니다. FP16 정밀도를 최초로 지원하며, 딥러닝 학습에서 혁신적인 성능 향상을 제공했습니다.</p>
  <ul>
    <li>640개의 Tensor Core (V100)</li>
    <li>FP16 행렬 연산 지원</li>
    <li>CUDA 9.0 이상 필요</li>
  </ul>

  <h3 id="turing-t4">Turing (T4, 2018)</h3>
  <p>추론을 위해 설계된 첫 세대로, INT8/INT4 양자화를 하드웨어 수준에서 지원했습니다.</p>
  <ul>
    <li>256개의 Tensor Core (T4)</li>
    <li>INT8/INT4 추론 가속</li>
    <li>RTX 시리즈 게임 GPU에도 적용</li>
  </ul>

  <h3 id="ampere-a100">Ampere (A100, 2020)</h3>
  <p>데이터센터 표준이 된 세대로, TF32, BF16, sparsity 지원이 추가되었습니다.</p>
  <ul>
    <li>432개의 Tensor Core (A100)</li>
    <li>TF32/BF16 자동 변환</li>
    <li>Sparsity (구조적 가지치기) 지원</li>
    <li>MIG (Multi-Instance GPU) partitioning</li>
  </ul>

  <h3 id="hopper-transformer-engine">Hopper Transformer Engine (H100, 2022)</h3>
  <p>Hopper에서 도입된 Transformer Engine은 LLM 추론과 학습을 혁신적으로 가속화합니다.</p>
  <ul>
    <li>동적 FP8/FP16 정밀도 자동 전환</li>
    <li>Transformer 레이어별 최적 정밀도 자동 선택</li>
    <li>TF32, FP64, FP16 대비 3배 성능 향상</li>
    <li>LLM 학습에서 A100 대비 9배, 추론에서 30배 성능 향상</li>
  </ul>

  <div class="diagram-container">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 700 360"
         style="width:100%;max-width:700px;display:block;margin:1.5em auto;">
      <defs>
        <marker id="te-arrow" viewBox="0 0 10 7" refX="10" refY="3.5"
                markerWidth="10" markerHeight="7" orient="auto">
          <polygon points="0 0, 10 3.5, 0 7" fill="var(--diagram-arrow)"/>
        </marker>
      </defs>

      <!-- Main Flow: Input → Engine → Tensor Core → Output -->
      <!-- Input -->
      <rect x="30" y="30" width="100" height="45" rx="6"
            fill="var(--bg-secondary)" stroke="var(--border-color)" stroke-width="1.5"/>
      <text x="80" y="58" text-anchor="middle" fill="var(--diagram-text)" font-size="11" font-weight="bold">
        입력 텐서
      </text>

      <!-- Transformer Engine -->
      <rect x="160" y="25" width="140" height="55" rx="6"
            fill="var(--accent-primary)" fill-opacity="0.25"
            stroke="var(--accent-primary)" stroke-width="2.5"/>
      <text x="230" y="45" text-anchor="middle" fill="var(--diagram-text)" font-size="11" font-weight="bold">
        Transformer Engine
      </text>
      <text x="230" y="63" text-anchor="middle" fill="var(--text-secondary)" font-size="9">
        동적 정밀도: FP8 ↔ FP16
      </text>

      <line x1="130" y1="52" x2="160" y2="52"
            stroke="var(--diagram-arrow)" stroke-width="2" marker-end="url(#te-arrow)"/>

      <!-- Tensor Core -->
      <rect x="330" y="25" width="140" height="55" rx="6"
            fill="var(--diagram-accent)" fill-opacity="0.2"
            stroke="var(--diagram-accent)" stroke-width="2"/>
      <text x="400" y="45" text-anchor="middle" fill="var(--diagram-text)" font-size="11" font-weight="bold">
        Tensor Core
      </text>
      <text x="400" y="63" text-anchor="middle" fill="var(--text-secondary)" font-size="9">
        MMA 연산 수행
      </text>

      <line x1="300" y1="52" x2="330" y2="52"
            stroke="var(--diagram-arrow)" stroke-width="2" marker-end="url(#te-arrow)"/>

      <!-- Output -->
      <rect x="500" y="30" width="100" height="45" rx="6"
            fill="var(--bg-secondary)" stroke="var(--border-color)" stroke-width="1.5"/>
      <text x="550" y="58" text-anchor="middle" fill="var(--diagram-text)" font-size="11" font-weight="bold">
        출력 텐서
      </text>

      <line x1="470" y1="52" x2="500" y2="52"
            stroke="var(--diagram-arrow)" stroke-width="2" marker-end="url(#te-arrow)"/>

      <!-- Arrow from Engine to Tensor Core (direct) -->
      <line x1="230" y1="80" x2="230" y2="110"
            stroke="var(--diagram-arrow)" stroke-width="2" stroke-dasharray="4,2"/>
      <line x1="230" y1="110" x2="400" y2="110"
            stroke="var(--diagram-arrow)" stroke-width="2" stroke-dasharray="4,2"/>
      <text x="315" y="105" text-anchor="middle" fill="var(--text-secondary)" font-size="8">
        정밀도 정보 전달
      </text>

      <!-- Attention Box -->
      <rect x="30" y="140" width="200" height="60" rx="6"
            fill="var(--bg-secondary)" stroke="var(--border-color)" stroke-width="1.5"/>
      <text x="130" y="162" text-anchor="middle" fill="var(--diagram-text)" font-size="10" font-weight="bold">
        Attention
      </text>
      <text x="130" y="182" text-anchor="middle" fill="var(--diagram-text)" font-size="9">
        Q × Kᵀ → Softmax → V
      </text>

      <!-- FFN Box -->
      <rect x="250" y="140" width="200" height="60" rx="6"
            fill="var="bg-secondary)" stroke="var(--border-color)" stroke-width="1.5"/>
      <text x="350" y="162" text-anchor="middle" fill="var(--diagram-text)" font-size="10" font-weight="bold">
        Feed-Forward Network
      </text>
      <text x="350" y="182" text-anchor="middle" fill="var(--diagram-text)" font-size="9">
        GELU + Linear
      </text>

      <!-- Precision Info Box -->
      <rect x="470" y="140" width="200" height="60" rx="6"
            fill="var(--accent-primary)" fill-opacity="0.15"
            stroke="var(--accent-primary)" stroke-width="1.5"/>
      <text x="570" y="162" text-anchor="middle" fill="var(--diagram-text)" font-size="10" font-weight="bold">
        정밀도 선택
      </text>
      <text x="570" y="182" text-anchor="middle" fill="var(--text-secondary)" font-size="9">
        FP8: 2× 처리량
      </text>
      <text x="570" y="194" text-anchor="middle" fill="var(--text-secondary)" font-size="8">
        FP16: 높은 정확도
      </text>

      <!-- Performance Box -->
      <rect x="30" y="240" width="640" height="100" rx="6"
            fill="var(--accent-primary)" fill-opacity="0.15"
            stroke="var(--accent-primary)" stroke-width="1.5"/>
      <text x="350" y="268" text-anchor="middle" fill="var(--diagram-text)" font-size="12" font-weight="bold">
        Hopper Transformer Engine 성능 향상
      </text>
      <text x="350" y="295" text-anchor="middle" fill="var(--diagram-text)" font-size="10">
        LLM 학습: 9배 | 추론: 30배 (vs A100)
      </text>
      <text x="350" y="318" text-anchor="middle" fill="var(--text-secondary)" font-size="9">
        FP8 정밀도로 처리량 2배 향상
      </text>
    </svg>
    <p class="diagram-caption">그림 3-1: Hopper Transformer Engine 동작 원리</p>
  </div>

  <h3 id="hopper-h100">Hopper with Ada Lovelace (H100, 2022)</h3>
  <p>H100 GPU는 NVIDIA Hopper 아키텍처(마이크로아키텍처는 Ada Lovelace)를 기반으로 하며, Transformer 엔진이 추가되어 LLM 추론에 최적화되었습니다.</p>
  <ul>
    <li>132개의 Tensor Core 클러스터 × 2 (H100)</li>
    <li>Transformer 엔진 (FP8 동적 정밀도)</li>
    <li>FP8 추론 지원</li>
    <li>NVLink 4세대 (900GB/s)</li>
    <li>동적 프로그래밍, Collective Operations 지원</li>
  </ul>

  <h3 id="blackwell-b100">Blackwell (B100/B200, 2024)</h3>
  <p>가장 최신 세대로, FP4/NVFP4 양자화와 대폭 향상된 추론 성능을 제공합니다.</p>
  <ul>
    <li>제5세대 Tensor Core (FP4/FP6/FP8 지원)</li>
    <li>NVFP4 정밀도 (NVIDIA 특화 4비트 포맷)</li>
    <li>FP8 대비 2배 효율, FP16 대비 3.5배 메모리 절약</li>
    <li>NVLink 5세대 (1.8TB/s)</li>
    <li>Dual-die 설계 (208B 트랜지스터)</li>
  </ul>

  <h3 id="blackwell-ultra">Blackwell Ultra (B200 Ultra, GB300, 2025)</h3>
  <p>Blackwell의 진화형으로, NVFP4 정밀도와 2세대 Transformer Engine이 적용된 최신 제품입니다.</p>
  <ul>
    <li>NVFP4 dense 연산: 최대 15 PetaFLOPS</li>
    <li>2세대 Transformer Engine (micro-tensor scaling)</li>
    <li>256KB Tensor Memory (TMEM) 내장</li>
    <li>Attention 레이어 가속화 2배 향상</li>
    <li>HBM3E 최대 288GB 지원</li>
    <li>NVLink 5세대 (1.8TB/s)</li>
  </ul>

  <div class="diagram-container">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 700 320"
         style="width:100%;max-width:700px;display:block;margin:1.5em auto;">
      <defs>
        <marker id="tc-gen-arrow" viewBox="0 0 10 7" refX="10" refY="3.5"
                markerWidth="10" markerHeight="7" orient="auto">
          <polygon points="0 0, 10 3.5, 0 7" fill="var(--diagram-arrow)"/>
        </marker>
      </defs>

      <!-- First Row: Architecture Evolution -->
      <!-- Volta -->
      <rect x="20" y="20" width="100" height="50" rx="6"
            fill="var(--bg-secondary)" stroke="var(--border-color)" stroke-width="1.5"/>
      <text x="70" y="40" text-anchor="middle" fill="var(--diagram-text)" font-size="11" font-weight="bold">
        Volta
      </text>
      <text x="70" y="56" text-anchor="middle" fill="var(--text-secondary)" font-size="9">
        2017
      </text>

      <line x1="120" y1="45" x2="160" y2="45"
            stroke="var(--diagram-arrow)" stroke-width="2" marker-end="url(#tc-gen-arrow)"/>

      <!-- Turing -->
      <rect x="160" y="20" width="100" height="50" rx="6"
            fill="var(--bg-secondary)" stroke="var(--border-color)" stroke-width="1.5"/>
      <text x="210" y="40" text-anchor="middle" fill="var(--diagram-text)" font-size="11" font-weight="bold">
        Turing
      </text>
      <text x="210" y="56" text-anchor="middle" fill="var(--text-secondary)" font-size="9">
        2018
      </text>

      <line x1="260" y1="45" x2="300" y2="45"
            stroke="var(--diagram-arrow)" stroke-width="2" marker-end="url(#tc-gen-arrow)"/>

      <!-- Ampere -->
      <rect x="300" y="20" width="100" height="50" rx="6"
            fill="var(--accent-primary)" fill-opacity="0.2"
            stroke="var(--accent-primary)" stroke-width="2"/>
      <text x="350" y="40" text-anchor="middle" fill="var(--diagram-text)" font-size="11" font-weight="bold">
        Ampere
      </text>
      <text x="350" y="56" text-anchor="middle" fill="var(--text-secondary)" font-size="9">
        2020
      </text>

      <line x1="400" y1="45" x2="440" y2="45"
            stroke="var(--diagram-arrow)" stroke-width="2" marker-end="url(#tc-gen-arrow)"/>

      <!-- Hopper -->
      <rect x="440" y="20" width="100" height="50" rx="6"
            fill="var(--accent-primary)" fill-opacity="0.3"
            stroke="var(--accent-primary)" stroke-width="2"/>
      <text x="490" y="40" text-anchor="middle" fill="var(--diagram-text)" font-size="11" font-weight="bold">
        Hopper
      </text>
      <text x="490" y="56" text-anchor="middle" fill="var(--text-secondary)" font-size="9">
        2022
      </text>

      <line x1="540" y1="45" x2="580" y2="45"
            stroke="var(--diagram-arrow)" stroke-width="2" marker-end="url(#tc-gen-arrow)"/>

      <!-- Blackwell -->
      <rect x="580" y="20" width="100" height="50" rx="6"
            fill="var(--accent-primary)" fill-opacity="0.4"
            stroke="var(--accent-primary)" stroke-width="2"/>
      <text x="630" y="40" text-anchor="middle" fill="var(--diagram-text)" font-size="11" font-weight="bold">
        Blackwell
      </text>
      <text x="630" y="56" text-anchor="middle" fill="var(--text-secondary)" font-size="9">
        2024
      </text>

      <!-- Second Row: Supported Precision -->
      <rect x="20" y="90" width="100" height="50" rx="6"
            fill="var(--bg-secondary)" stroke="var(--border-color)" stroke-width="1"/>
      <text x="70" y="115" text-anchor="middle" fill="var(--text-secondary)" font-size="10">
        FP16 only
      </text>

      <rect x="160" y="90" width="100" height="50" rx="6"
            fill="var(--bg-secondary)" stroke="var(--border-color)" stroke-width="1"/>
      <text x="210" y="115" text-anchor="middle" fill="var(--text-secondary)" font-size="10">
        + INT8/4
      </text>

      <rect x="300" y="90" width="100" height="50" rx="6"
            fill="var(--accent-primary)" fill-opacity="0.15"
            stroke="var(--accent-primary)" stroke-width="1"/>
      <text x="350" y="115" text-anchor="middle" fill="var(--text-secondary)" font-size="10">
        + TF32/BF16
      </text>

      <rect x="440" y="90" width="100" height="50" rx="6"
            fill="var(--accent-primary)" fill-opacity="0.2"
            stroke="var(--accent-primary)" stroke-width="1"/>
      <text x="490" y="115" text-anchor="middle" fill="var(--text-secondary)" font-size="10">
        + FP8/TE
      </text>

      <rect x="580" y="90" width="100" height="50" rx="6"
            fill="var(--accent-primary)" fill-opacity="0.3"
            stroke="var(--accent-primary)" stroke-width="2"/>
      <text x="630" y="115" text-anchor="middle" fill="var(--diagram-text)" font-size="10" font-weight="bold">
        + FP4/FP6
      </text>

      <!-- Performance Comparison -->
      <rect x="20" y="165" width="660" height="120" rx="6"
            fill="var(--bg-secondary)" stroke="var(--border-color)" stroke-width="1.5"/>
      <text x="350" y="190" text-anchor="middle" fill="var(--diagram-text)" font-size="12" font-weight="bold">
        상대적 추론 성능 (FP16/BF16 기준)
      </text>

      <!-- V100 bar -->
      <rect x="40" y="210" width="80" height="30" rx="4"
            fill="var(--border-color)"/>
      <text x="80" y="230" text-anchor="middle" fill="var(--bg-secondary)" font-size="10">
        V100
      </text>
      <text x="80" y="250" text-anchor="middle" fill="var(--text-secondary)" font-size="9">
        1x
      </text>

      <!-- A100 bar -->
      <rect x="150" y="210" width="90" height="30" rx="4"
            fill="var(--accent-primary)" fill-opacity="0.4"/>
      <text x="195" y="230" text-anchor="middle" fill="var(--bg-secondary)" font-size="10">
        A100
      </text>
      <text x="195" y="250" text-anchor="middle" fill="var(--text-secondary)" font-size="9">
        ~3x
      </text>

      <!-- H100 bar -->
      <rect x="270" y="210" width="110" height="30" rx="4"
            fill="var(--accent-primary)" fill-opacity="0.6"/>
      <text x="325" y="230" text-anchor="middle" fill="var(--bg-secondary)" font-size="10">
        H100
      </text>
      <text x="325" y="250" text-anchor="middle" fill="var(--text-secondary)" font-size="9">
        ~9x
      </text>

      <!-- H100 FP8 bar -->
      <rect x="400" y="210" width="80" height="30" rx="4"
            fill="var(--accent-primary)" fill-opacity="0.6"/>
      <text x="440" y="230" text-anchor="middle" fill="var(--bg-secondary)" font-size="9">
        H100 FP8
      </text>
      <text x="440" y="250" text-anchor="middle" fill="var(--text-secondary)" font-size="9">
        ~30x
      </text>

      <!-- B200 bar -->
      <rect x="500" y="210" width="80" height="30" rx="4"
            fill="var(--accent-primary)" fill-opacity="0.7"/>
      <text x="540" y="230" text-anchor="middle" fill="var(--bg-secondary)" font-size="10">
        B200
      </text>
      <text x="540" y="250" text-anchor="middle" fill="var(--text-secondary)" font-size="9">
        ~30x
      </text>

      <!-- B200 Ultra bar -->
      <rect x="600" y="210" width="80" height="30" rx="4"
            fill="var(--accent-primary)" fill-opacity="0.9"/>
      <text x="640" y="230" text-anchor="middle" fill="var(--bg-secondary)" font-size="9">
        B200 Ultra
      </text>
      <text x="640" y="250" text-anchor="middle" fill="var(--text-secondary)" font-size="9">
        ~40x
      </text>

      <!-- Performance notes -->
      <text x="350" y="280" text-anchor="middle" fill="var(--text-secondary)" font-size="8">
        FP4 양자화 시 추가 2배 향상 (B200: ~60x, Ultra: ~80x)
      </text>
    </svg>
    <p class="diagram-caption">그림 3: Tensor Core 세대별 발전과 성능 비교</p>
  </div>

  <h3 id="blackwell-5th-gen">Blackwell 5세대 Tensor Core 기술</h3>
  <p>Blackwell 아키텍처의 5세대 Tensor Core는 이전 세대와는 근본적으로 다른 설계 철학을 가집니다.</p>

  <h4 id="warp-async-mma">비동기 MMA (Warp-group MMA)</h4>
  <p>이전 세대(Volta, Ampere, Hopper)까지는 warp 내 32개 스레드가 동기화되어 행렬 곱셈을 수행하는 warp-synchronous 모델을 사용했습니다. Blackwell에서는 <code>tcgen05.mma</code> 명령어를 통해 각 스레드가 독립적으로 MMA를 수행할 수 있어 유연성이 크게 향상되었습니다.</p>

  <div class="info-box info">
    <strong>tcgen05 명령어:</strong> Blackwell에서 도입된 새로운 PTX 명령어로, 단일 스레드에서 Tensor Core 행렬 연산을 수행합니다. 이전 세대의 <code>mma.sync</code>, <code>wgmma</code>와 달리 스레드 간 동기화 오버헤드가 없어 더 효율적인 스케줄링이 가능합니다.
  </div>

  <h4 id="micro-tensor-scaling">Micro-Tensor Scaling</h4>
  <p>Blackwell 2세대 Transformer Engine은 micro-tensor scaling 기술을 사용하여 FP4 정밀도의 정확도 손실을 최소화합니다. 이 기술은 16개 값 블록마다 FP8(E4M3) 마이크로 스케일을 적용하고, 텐서 레벨에서 FP32 스케일을 추가로 적용하는 2단계 스케일링 방식입니다.</p>

  <h4 id="tmem-tensor-memory">Tensor Memory (TMEM)</h4>
  <p>Blackwell SM당 256KB의 Tensor Memory가 내장되어 있습니다. 이는 warp 동기적으로 중간 결과를 저장하여 off-chip 메모리 트래픽을 줄이고 데이터 재사용을 극대화합니다.</p>

  <div class="diagram-container">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 700 340"
         style="width:100%;max-width:700px;display:block;margin:1.5em auto;">
      <defs>
        <marker id="bb-arrow" viewBox="0 0 10 7" refX="10" refY="3.5"
                markerWidth="10" markerHeight="7" orient="auto">
          <polygon points="0 0, 10 3.5, 0 7" fill="var(--diagram-arrow)"/>
        </marker>
      </defs>

      <!-- Hopper vs Blackwell Comparison -->
      <rect x="30" y="20" width="310" height="90" rx="8"
            fill="var(--bg-secondary)" stroke="var(--border-color)" stroke-width="1.5"/>
      <text x="185" y="42" text-anchor="middle" fill="var(--diagram-text)" font-size="12" font-weight="bold">
        Hopper (4th Gen Tensor Core)
      </text>
      <text x="185" y="60" text-anchor="middle" fill="var(--text-secondary)" font-size="9">
        Warp-synchronous MMA (wgmma)
      </text>
      <text x="185" y="75" text-anchor="middle" fill="var(--text-secondary)" font-size="9">
        FP8 / FP16 / BF16 / TF32
      </text>
      <text x="185" y="90" text-anchor="middle" fill="var(--text-secondary)" font-size="9">
        1세대 Transformer Engine
      </text>

      <line x1="340" y1="65" x2="370" y2="65"
            stroke="var(--diagram-arrow)" stroke-width="3" marker-end="url(#bb-arrow)"/>

      <!-- Blackwell -->
      <rect x="370" y="20" width="300" height="90" rx="8"
            fill="var(--accent-primary)" fill-opacity="0.2"
            stroke="var(--accent-primary)" stroke-width="2.5"/>
      <text x="520" y="42" text-anchor="middle" fill="var(--diagram-text)" font-size="12" font-weight="bold">
        Blackwell (5th Gen Tensor Core)
      </text>
      <text x="520" y="60" text-anchor="middle" fill="var(--diagram-text)" font-size="9">
        Asynchronous MMA (tcgen05)
      </text>
      <text x="520" y="75" text-anchor="middle" fill="var(--diagram-text)" font-size="9">
        FP4 / FP6 / FP8 / FP16 / BF32
      </text>
      <text x="520" y="90" text-anchor="middle" fill="var(--diagram-text)" font-size="9">
        2세대 Transformer Engine
      </text>

      <!-- NVFP4 Direct Conversion -->
      <rect x="30" y="140" width="640" height="80" rx="8"
            fill="var(--bg-secondary)" stroke="var(--border-color)" stroke-width="1.5"/>
      <text x="350" y="162" text-anchor="middle" fill="var(--diagram-text)" font-size="12" font-weight="bold">
        NVFP4 양자화 (Direct Conversion)
      </text>
      <text x="350" y="182" text-anchor="middle" fill="var(--text-secondary)" font-size="10">
        FP16/FP32 → NVFP4 (2단계 스케일링)
      </text>

      <!-- Block 1: Input -->
      <rect x="50" y="200" width="120" height="30" rx="4"
            fill="var(--diagram-accent)" fill-opacity="0.3" stroke="var(--diagram-accent)" stroke-width="1.5"/>
      <text x="110" y="220" text-anchor="middle" fill="var(--diagram-text)" font-size="9">
        FP16/FP32
      </text>

      <line x1="170" y1="215" x2="210" y2="215"
            stroke="var(--diagram-arrow)" stroke-width="2" marker-end="url(#bb-arrow)"/>

      <!-- Block 2: NVFP4 Quantization -->
      <rect x="210" y="200" width="180" height="30" rx="4"
            fill="var(--accent-primary)" fill-opacity="0.3" stroke="var(--accent-primary)" stroke-width="2"/>
      <text x="300" y="220" text-anchor="middle" fill="var(--diagram-text)" font-size="9" font-weight="bold">
        NVFP4 양자화
      </text>
      <text x="300" y="232" text-anchor="middle" fill="var(--text-secondary)" font-size="8">
        (마이크로 + 텐서 스케일)
      </text>

      <line x1="390" y1="215" x2="430" y2="215"
            stroke="var(--diagram-arrow)" stroke-width="2" marker-end="url(#bb-arrow)"/>

      <!-- Block 3: Tensor Core -->
      <rect x="430" y="200" width="80" height="30" rx="4"
            fill="var(--accent-primary)" fill-opacity="0.5" stroke="var(--accent-primary)" stroke-width="2"/>
      <text x="470" y="220" text-anchor="middle" fill="var(--diagram-text)" font-size="9" font-weight="bold">
        Tensor Core
      </text>

      <line x1="510" y1="215" x2="550" y2="215"
            stroke="var(--diagram-arrow)" stroke-width="2" marker-end="url(#bb-arrow)"/>

      <!-- Block 4: Output -->
      <rect x="550" y="200" width="100" height="30" rx="4"
            fill="var(--diagram-accent)" fill-opacity="0.2" stroke="var(--diagram-accent)" stroke-width="1.5"/>
      <text x="600" y="220" text-anchor="middle" fill="var(--diagram-text)" font-size="9">
        결과 출력
      </text>

      <!-- Benefits Box -->
      <rect x="30" y="255" width="640" height="70" rx="6"
            fill="var(--diagram-accent)" fill-opacity="0.1"
            stroke="var(--diagram-accent)" stroke-width="1.5"/>
      <text x="350" y="280" text-anchor="middle" fill="var(--diagram-text)" font-size="11" font-weight="bold">
        NVFP4 이점
      </text>
      <text x="350" y="300" text-anchor="middle" fill="var(--diagram-text)" font-size="10">
        FP8 유사 정확도 | FP8 대비 1.8×, FP16 대비 3.5× 메모리 절약
      </text>
      <text x="350" y="316" text-anchor="middle" fill="var(--text-secondary)" font-size="9">
        하드웨어 가속 양자화 (정확도 손실 1% 이내)
      </text>
    </svg>
    <p class="diagram-caption">그림 3-2: Blackwell 5세대 Tensor Core vs Hopper + NVFP4 양자화</p>
  </div>
</section>

<section class="content-section">
  <h2 id="llm-optimization">LLM 최적화 활용</h2>
  <p>Tensor Core는 LLM의 학습과 추론에서 핵심 역할을 합니다. 최신 GPU들은 Transformer 특화 최적화를 통해 LLM 성능을 극대화합니다.</p>

  <h3 id="inference-optimization">추론 최적화 기법</h3>

  <h4 id="kv-cache-optimization">KV Cache 최적화</h4>
  <p>추론 시 이전 토큰의 Key-Value 행렬을 캐싱하여 중복 계산을 방지합니다. Tensor Core는 이 KV 캐시 행렬 곱셈을 가속화합니다. Blackwell에서는 TMEM(Tensor Memory)을 통해 KV 캐시를 더 효율적으로 관리합니다.</p>

  <h4 id="blackwell-inference">Blackwell Inference 최적화</h4>
  <p>Blackwell GPU는 추론에 최적화된 여러 기능을 제공합니다:</p>
  <ul>
    <li><strong>FP4/FP6 양자화:</strong> KV 캐시와 가중치를 4비트로 양자화하여 메모리 사용량 최소화</li>
    <li><strong>NVFP4:</strong> Blackwell Ultra 특화 포맷으로 FP8 유사 정확도 달성</li>
    <li><strong>2세대 Transformer Engine:</strong> 동적 정밀도 선택으로 각 레이어에 최적 정밀도 적용</li>
    <li><strong>Tensor Memory 활용:</strong> TMEM에서 KV 캐시 직접 처리로 메모리 대역폭 절약</li>
  </ul>

  <h4 id="quantization">양자화 (Quantization)</h4>
  <p>FP16/BF16에서 INT8/INT4로 모델을 양자화하여 메모리와 대역폭을 절약하면서 추론 속도를 향상시킵니다.</p>
  <pre><code><span class="cmt"># PyTorch에서 INT8 양자화 예시</span>
<span class="kw">import</span> torch.quantization

<span class="cmt"># 동적 양자화 (Dynamic Quantization)</span>
model_int8 = torch.quantization.quantize_dynamic(
    model,
    {torch.nn.Linear: torch.qint8},
    dtype=torch.qint8
)

<span class="cmt"># 추론</span> 
<span class="kw">with</span> torch.no_grad():
    output = <span class="fn">model_int8</span>(input_ids)</code></pre>

  <div class="info-box tip">
    <strong>💡 팁:</strong> H100의 FP8 동적 정밀도는 모델의 각 레이어별 연산 특성에 따라 자동으로 정밀도를 조절합니다.
  </div>

  <h4 id="paged-attention">Paged Attention (vLLM)</h4>
  <p>KV 캐시를 페이지 단위로 관리하여 메모리 단편화를 줄이고, 더 큰 배치 크기로 처리량을 향상시킵니다.</p>

  <h3 id="training-optimization">학습 최적화 기법</h3>

  <h4 id="mixed-precision">Mixed Precision Training</h4>
  <p>FP16/BF16 혼합 정밀도 학습으로 메모리를 절약하면서도 FP32 대비 유사한 품질을 유지합니다.</p>
  <pre><code><span class="cmt"># PyTorch Mixed Precision 학습</span>
<span class="kw">from</span> torch.cuda.amp <span class="kw">import</span> autocast, GradScaler

scaler = <span class="fn">GradScaler</span>()

<span class="kw">for</span> data, target <span class="kw">in</span> dataloader:
    optimizer.zero_grad()
    
    <span class="cmt"># 자동 mixed precision</span>
    <span class="kw">with</span> <span class="fn">autocast</span>(device_type=<span class="str">'cuda'</span>):
        output = <span class="fn">model</span>(data)
        loss = <span class="fn">criterion</span>(output, target)
    
    <span class="cmt"># 그래디언트 스케일링</span>
    scaler.scale(loss).backward()
    scaler.step(optimizer)
    scaler.update()</code></pre>

  <h4 id="gradient-accumulation">Gradient Accumulation</h4>
  <p>메모리 제약으로 큰 배치 크기를 사용하지 못하는 경우, 작은 배치의 그래디언트를 누적하여 동일한 효과를 얻습니다.</p>

  <div class="diagram-container">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 700 480"
         style="width:100%;max-width:700px;display:block;margin:1.5em auto;">
      <defs>
        <marker id="llm-arrow" viewBox="0 0 10 7" refX="10" refY="3.5"
                markerWidth="10" markerHeight="7" orient="auto">
          <polygon points="0 0, 10 3.5, 0 7" fill="var(--diagram-arrow)"/>
        </marker>
      </defs>

      <!-- Top: LLM Forward Pass -->
      <!-- Input Tokens -->
      <rect x="30" y="20" width="100" height="40" rx="6"
            fill="var(--bg-secondary)" stroke="var(--border-color)" stroke-width="1.5"/>
      <text x="80" y="45" text-anchor="middle" fill="var(--diagram-text)" font-size="10">
        Input
      </text>

      <!-- Embedding -->
      <rect x="150" y="20" width="90" height="40" rx="6"
            fill="var(--bg-secondary)" stroke="var(--border-color)" stroke-width="1.5"/>
      <text x="195" y="45" text-anchor="middle" fill="var(--diagram-text)" font-size="10">
        Embedding
      </text>

      <line x1="130" y1="40" x2="150" y2="40"
            stroke="var(--diagram-arrow)" stroke-width="2" marker-end="url(#llm-arrow)"/>

      <!-- Transformer Block -->
      <rect x="260" y="10" width="160" height="60" rx="8"
            fill="var(--accent-primary)" fill-opacity="0.15"
            stroke="var(--accent-primary)" stroke-width="2"/>
      <text x="340" y="32" text-anchor="middle" fill="var(--diagram-text)" font-size="10" font-weight="bold">
        Transformer Block
      </text>
      <text x="340" y="48" text-anchor="middle" fill="var(--diagram-text)" font-size="9">
        Attention + FFN
      </text>
      <text x="340" y="62" text-anchor="middle" fill="var(--diagram-accent)" font-size="8">
        Tensor Core 연산
      </text>

      <line x1="240" y1="40" x2="260" y2="40"
            stroke="var(--diagram-arrow)" stroke-width="2" marker-end="url(#llm-arrow)"/>

      <!-- Output -->
      <rect x="440" y="20" width="80" height="40" rx="6"
            fill="var(--bg-secondary)" stroke="var(--border-color)" stroke-width="1.5"/>
      <text x="480" y="45" text-anchor="middle" fill="var(--diagram-text)" font-size="10">
        Output
      </text>

      <line x1="420" y1="40" x2="440" y2="40"
            stroke="var(--diagram-arrow)" stroke-width="2" marker-end="url(#llm-arrow)"/>

      <!-- Connection: Transformer ↔ Memory -->
      <line x1="340" y1="70" x2="340" y2="100"
            stroke="var(--diagram-arrow)" stroke-width="2"/>
      <text x="350" y="88" text-anchor="middle" fill="var(--diagram-arrow)" font-size="8" font-weight="bold">
        데이터 읽기/쓰기
      </text>

      <!-- Middle: Legend -->
      <rect x="30" y="100" width="640" height="22" rx="4"
            fill="var(--diagram-accent)" fill-opacity="0.1"
            stroke="var(--diagram-accent)" stroke-width="1" stroke-dasharray="4,2"/>
      <text x="350" y="116" text-anchor="middle" fill="var(--diagram-accent)" font-size="9" font-weight="bold">
        GPU HBM Memory ↔ Tensor Core 데이터 흐름
      </text>

      <!-- Bottom: Memory Layout -->
      <rect x="30" y="135" width="640" height="160" rx="6"
            fill="var(--bg-secondary)" stroke="var(--border-color)" stroke-width="1.5"/>
      <text x="350" y="158" text-anchor="middle" fill="var(--diagram-text)" font-size="11" font-weight="bold">
        GPU HBM (High Bandwidth Memory)
      </text>

      <!-- Model Weights -->
      <rect x="50" y="175" width="120" height="50" rx="4"
            fill="var(--diagram-accent)" fill-opacity="0.3" stroke="var(--diagram-accent)" stroke-width="1.5"/>
      <text x="110" y="198" text-anchor="middle" fill="var(--diagram-text)" font-size="10" font-weight="bold">
        Model Weights
      </text>
      <text x="110" y="216" text-anchor="middle" fill="var(--text-secondary)" font-size="8">
        (읽기 전용)
      </text>

      <!-- Arrow: Weights → Transformer -->
      <line x1="110" y1="175" x2="110" y2="120"
            stroke="var(--diagram-arrow)" stroke-width="2"/>
      <text x="118" y="150" text-anchor="middle" fill="var(--diagram-arrow)" font-size="7">
        읽기
      </text>

      <!-- KV Cache -->
      <rect x="200" y="175" width="120" height="50" rx="4"
            fill="var(--diagram-accent)" fill-opacity="0.2" stroke="var(--diagram-accent)" stroke-width="1.5"/>
      <text x="260" y="198" text-anchor="middle" fill="var(--diagram-text)" font-size="10" font-weight="bold">
        KV Cache
      </text>
      <text x="260" y="216" text-anchor="middle" fill="var(--text-secondary)" font-size="8">
        (읽기/쓰기)
      </text>

      <!-- Arrow: KV Cache ↔ Transformer (bidirectional) -->
      <line x1="260" y1="175" x2="260" y2="120"
            stroke="var(--diagram-arrow)" stroke-width="2"/>
      <text x="268" y="150" text-anchor="middle" fill="var(--diagram-arrow)" font-size="7">
        읽기
      </text>
      <line x1="260" y1="295" x2="260" y2="330"
            stroke="var(--diagram-accent)" stroke-width="2"/>
      <text x="268" y="315" text-anchor="middle" fill="var(--diagram-accent)" font-size="7">
        쓰기
      </text>

      <!-- Activations -->
      <rect x="350" y="175" width="120" height="50" rx="4"
            fill="var(--bg-secondary)" stroke="var(--border-color)" stroke-width="1.5"/>
      <text x="410" y="198" text-anchor="middle" fill="var(--diagram-text)" font-size="10" font-weight="bold">
        Activations
      </text>
      <text x="410" y="216" text-anchor="middle" fill="var(--text-secondary)" font-size="8">
        (읽기/쓰기)
      </text>

      <!-- Arrow: Activations ↔ Transformer -->
      <line x1="410" y1="175" x2="410" y2="120"
            stroke="var(--diagram-arrow)" stroke-width="2"/>
      <text x="418" y="150" text-anchor="middle" fill="var(--diagram-arrow)" font-size="7">
        읽기
      </text>
      <line x1="410" y1="295" x2="410" y2="330"
            stroke="var(--diagram-accent)" stroke-width="2"/>
      <text x="418" y="315" text-anchor="middle" fill="var(--diagram-accent)" font-size="7">
        쓰기
      </text>

      <!-- Tensor Core Operations -->
      <rect x="500" y="170" width="140" height="60" rx="6"
            fill="var(--accent-primary)" fill-opacity="0.3"
            stroke="var(--accent-primary)" stroke-width="2"/>
      <text x="570" y="195" text-anchor="middle" fill="var(--diagram-text)" font-size="10" font-weight="bold">
        Tensor Core
      </text>
      <text x="570" y="213" text-anchor="middle" fill="var(--diagram-text)" font-size="9">
        MMA 연산 수행
      </text>

      <!-- Arrow: Tensor Core result → Memory -->
      <line x1="570" y1="230" x2="570" y2="265"
            stroke="var(--diagram-accent)" stroke-width="2"/>
      <text x="578" y="250" text-anchor="middle" fill="var(--diagram-accent)" font-size="7">
        결과 저장
      </text>
    </svg>
    <p class="diagram-caption">그림 4: LLM 추론에서 Tensor Core와 GPU Memory 데이터 흐름</p>
  </div>
</section>

<section class="content-section">
  <h2 id="practical-usage">실전 사용법</h2>

  <h3 id="cuda-programming">CUDA 프로그래밍</h3>
  <p>Tensor Core를 직접 프로그래밍하려면 CUDA와 cuBLAS 라이브러리를 사용합니다.</p>

  <h4 id="cublas-usage">cuBLAS 사용</h4>
  <pre><code><span class="cmt">// cuBLAS Tensor Core 행렬 곱셈</span>
<span class="pp">#include</span> <span class="str">&lt;cublas_v2.h&gt;</span>

<span class="type">void</span> <span class="fn">tensor_core_gemm</span>(cublasHandle_t handle,
    <span class="type">float</span>* A, <span class="type">float</span>* B, <span class="type">float</span>* C,
    <span class="type">int</span> m, <span class="type">int</span> n, <span class="type">int</span> k) {
    
    <span class="type">float</span> alpha = <span class="num">1.0f</span>;
    <span class="type">float</span> beta = <span class="num">0.0f</span>;
    
    <span class="cmt">// FP16 Tensor Core 사용</span>
    cublasSetMathMode(handle, CUBLAS_TENSOR_OP_MATH);
    
    <span class="fn">cublasGemmEx</span>(handle,
        CUBLAS_OP_N, CUBLAS_OP_N,
        n, m, k,
        &alpha, B, CUDA_R_16F, n,
                 A, CUDA_R_16F, k,
        &beta,  C, CUDA_R_16F, n,
        CUBLAS_GEMM_DEFAULT_TENSOR_OP);
}</code></pre>

  <h3 id="pytorch-usage">PyTorch에서 활용</h3>
  <pre><code><span class="cmt"># PyTorch Tensor Core 활성화 확인</span>
<span class="kw">import</span> torch

<span class="cmt"># Tensor Core 사용 가능한지 확인</span>
<span class="fn">print</span>(torch.cuda.is_available())
<span class="fn">print</span>(torch.cuda.get_device_capability())

<span class="cmt"># TF32 활성화 (Ampere+)</span>
torch.backends.cuda.matmul.allow_tf32 = <span class="kw">True</span>
torch.backends.cudnn.allow_tf32 = <span class="kw">True</span>

<span class="cmt"># cuDNN 자동 튜닝</span>
torch.backends.cudnn.benchmark = <span class="kw">True</span>

<span class="cmt"># FP8 지원 확인 (Hopper+)</span>
<span class="kw">if</span> torch.cuda.get_device_capability() >= (<span class="num">9</span>, <span class="num">0</span>):
    <span class="fn">print</span>(<span class="str">"FP8 Tensor Core available"</span>)</code></pre>

  <h3 id="vllm-usage">vLLM으로 추론 가속화</h3>
  <pre><code><span class="cmt"># vLLM으로 Tensor Core 추론</span>
<span class="kw">from</span> vllm <span class="kw">import</span> LLM, SamplingParams

<span class="cmt"># H100에서 FP8 추론</span>
llm = <span class="fn">LLM</span>(
    model=<span class="str">"meta-llama/Llama-2-70b-hf"</span>,
    tensor_parallel_size=<span class="num">4</span>,
    dtype=<span class="str">"half"</span>,  <span class="cmt"># FP16</span>
    <span class="cmt"># dtype="float8"  # FP8 (Hopper)</span>
)

sampling_params = <span class="fn">SamplingParams</span>(
    temperature=<span class="num">0.8</span>,
    max_tokens=<span class="num">256</span>
)

outputs = llm.generate(
    [<span class="str">"Explain quantum computing in simple terms"</span>],
    sampling_params
)</code></pre>

  <div class="info-box warning">
    <strong>⚠️ 주의:</strong> Tensor Core를 최대한 활용하려면 CUDA, cuDNN, PyTorch 버전이 GPU를 지원해야 합니다. 드라이버와 라이브러리 호환성을 확인하세요.
  </div>
</section>

<section class="content-section">
  <h2 id="performance-benchmark">성능 벤치마크</h2>
  <p>LLM 추론에서 Tensor Core의 성능 영향을 이해하기 위한 벤치마크 가이드입니다.</p>

  <h3 id="llm-benchmark">추론 성능 비교</h3>
  <p>아래 수치는 70B 파라미터 LLM 기준이며, 실제 성능은 모델 아키텍처, 배치 크기, KV 캐시 크기 등에 따라 달라질 수 있습니다.</p>
  <table>
    <thead>
      <tr>
        <th>GPU</th>
        <th>아키텍처</th>
        <th>정밀도</th>
        <th>Throughput (tokens/s)</th>
        <th>Latency (ms)</th>
        <th>VRAM</th>
        <th>대역폭</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>A100-40GB</td>
        <td>Ampere</td>
        <td>FP16</td>
        <td>~50</td>
        <td>~200</td>
        <td>40GB</td>
        <td>2TB/s</td>
      </tr>
      <tr>
        <td>A100-80GB</td>
        <td>Ampere</td>
        <td>FP16</td>
        <td>~80</td>
        <td>~125</td>
        <td>80GB</td>
        <td>2TB/s</td>
      </tr>
      <tr>
        <td>H100</td>
        <td>Hopper</td>
        <td>FP16</td>
        <td>~200</td>
        <td>~50</td>
        <td>80GB</td>
        <td>3.2TB/s</td>
      </tr>
      <tr>
        <td>H100</td>
        <td>Hopper</td>
        <td>FP8</td>
        <td>~500</td>
        <td>~20</td>
        <td>80GB</td>
        <td>3.2TB/s</td>
      </tr>
      <tr>
        <td>H200</td>
        <td>Hopper</td>
        <td>FP8</td>
        <td>~700</td>
        <td>~14</td>
        <td>141GB</td>
        <td>4.8TB/s</td>
      </tr>
      <tr>
        <td>B200</td>
        <td>Blackwell</td>
        <td>FP8</td>
        <td>~1800</td>
        <td>~5</td>
        <td>192GB</td>
        <td>8TB/s</td>
      </tr>
      <tr>
        <td>B200</td>
        <td>Blackwell</td>
        <td>FP4</td>
        <td>~3500</td>
        <td>~3</td>
        <td>192GB</td>
        <td>8TB/s</td>
      </tr>
      <tr>
        <td>B200 Ultra</td>
        <td>Blackwell Ultra</td>
        <td>NVFP4</td>
        <td>~4000</td>
        <td>~2</td>
        <td>288GB</td>
        <td>8TB/s</td>
      </tr>
    </tbody>
  </table>
  <p class="diagram-caption">표: 70B 파라미터 LLM 추론 벤치마크 (초당 출력 토큰 수, 배치 크기 1)</p>

  <h3 id="memory-bandwidth">메모리 대역폭 중요성</h3>
  <p>Tensor Core 성능을 완전히 활용하려면 충분한 메모리 대역폭이 필요합니다. H100 NVLink는 900GB/s, B200 NVLink 5는 1.8TB/s를 제공합니다.</p>
</section>

<section class="content-section">
  <h2 id="best-practices">모범 사례</h2>
  <ul>
    <li><strong>드라이버 업데이트:</strong> 최신 NVIDIA 드라이버로 Tensor Core 최적화 활용</li>
    <li><strong>CUDA 버전:</strong> CUDA 12.8 이상 (Blackwell 5세대 Tensor Core 완전 지원)</li>
    <li><strong>PyTorch 버전:</strong> 2.4 이상 (FP8, Blackwell 지원)</li>
    <li><strong>정밀도 선택:</strong> 
      <ul>
        <li>학습: BF16 (Ampere+)</li>
        <li>추론 (H100): FP8</li>
        <li>추론 (Blackwell Ultra): NVFP4 또는 FP4</li>
      </ul>
    </li>
    <li><strong>배치 크기:</strong> VRAM 대비 최대 배치 크기 시도</li>
    <li><strong>vLLM/TensorRT-LLM:</strong> 추론 최적화 프레임워크 사용</li>
    <li><strong>Blackwell 활용:</strong> TMEM 활성화, FP4/NVFP4 양자화 적용</li>
    <li><strong>메모리 대역폭:</strong> Blackwell HBM3e 8TB/s 최대한 활용</li>
  </ul>
</section>

  <section class="content-section">
    <h2 id="references">참고자료</h2>
    <ul>
      <li><a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#tensor-cores" target="_blank" rel="noopener">NVIDIA CUDA Tensor Core 문서</a></li>
      <li><a href="https://developer.nvidia.com/blog/nvidia-hopper-architecture-introduction/" target="_blank" rel="noopener">NVIDIA Hopper Architecture</a></li>
      <li><a href="https://www.nvidia.com/en-us/data-center/technologies/blackwell-architecture/" target="_blank" rel="noopener">NVIDIA Blackwell Architecture</a></li>
      <li><a href="https://developer.nvidia.com/blog/inside-nvidia-blackwell-ultra-the-chip-powering-the-ai-factory-era/" target="_blank" rel="noopener">NVIDIA Blackwell Ultra - AI Factory Era</a></li>
      <li><a href="https://pytorch.org/docs/stable/amp.html" target="_blank" rel="noopener">PyTorch Automatic Mixed Precision</a></li>
      <li><a href="https://docs.vllm.ai/" target="_blank" rel="noopener">vLLM 문서</a></li>
      <li><a href="pages/llm-theory.html">LLM 이론 종합</a></li>
      <li><a href="pages/gguf-format.html">GGUF 모델 포맷 (양자화)</a></li>
    </ul>

  <div class="info-box info">
    <strong>다음 학습:</strong>
    <ul>
      <li><a href="pages/llm-theory.html">LLM 이론 종합</a> - 딥러닝 기초 이론</li>
      <li><a href="pages/gguf-format.html">GGUF 모델 포맷</a> - 양자화 기법 상세</li>
      <li><a href="pages/local-api.html">로컬 LLM API</a> - vLLM 실전 활용</li>
    </ul>
  </div>
</section>

<!-- Page Navigation (이전/다음) -->
<div class="page-nav"></div>

</main>

<!-- ===== TOC Sidebar -->
<aside class="inline-toc">
  <div class="toc-title">목차</div>
  <div class="toc-nav"></div>
</aside>

<!-- ===== Footer ===== -->
<footer class="site-footer">
</footer>

</div>

<script src="../js/main.js"></script>
</body>
</html>
