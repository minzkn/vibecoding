<!DOCTYPE html>
<html lang="ko" data-theme="dark-kernel">
<head>
<!-- BEGIN: Google adsense -->
<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-2110881342960271" crossorigin="anonymous"></script>
<!-- END: Google adsense -->
<!-- BEGIN: Google adsense repair -->
<script async src="https://fundingchoicesmessages.google.com/i/pub-2110881342960271?ers=1" nonce="laI6FT8gpRxugDJv5AGJRA"></script><script nonce="laI6FT8gpRxugDJv5AGJRA">(function() {function signalGooglefcPresent() {if (!window.frames['googlefcPresent']) {if (document.body) {const iframe = document.createElement('iframe'); iframe.style = 'width: 0; height: 0; border: none; z-index: -1000; left: -1000px; top: -1000px;'; iframe.style.display = 'none'; iframe.name = 'googlefcPresent'; document.body.appendChild(iframe);} else {setTimeout(signalGooglefcPresent, 0);}}}signalGooglefcPresent();})();</script>
<!-- END: Google adsense repair -->
<!-- BEGIN: Google analytics -->
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-1VWQF060SX"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-1VWQF060SX');
</script>
<!-- END: Google analytics -->

<!-- Flash ë°©ì§€: ì¿ í‚¤ì—ì„œ í…Œë§ˆ ì¦‰ì‹œ ì ìš© -->
<script>
(function(){var m=document.cookie.match(/claude_theme=([^;]+)/);if(m)document.documentElement.setAttribute('data-theme',m[1]);})();
</script>
<meta charset="UTF-8">
<meta property="og:type" content="article">
<meta property="og:site_name" content="AI Vibe Coding ê°€ì´ë“œ /with MINZKN">
<meta property="og:title" content="NVIDIA Tensor Core ì™„ì „ ê°€ì´ë“œ">
<meta property="og:description" content="NVIDIA Tensor Coreì˜ ì›ë¦¬, ì„¸ëŒ€ë³„ ë°œì „, LLM ì¶”ë¡ Â·í•™ìŠµ ìµœì í™” ê¸°ë²•ì„ ìƒì„¸íˆ ì†Œê°œí•©ë‹ˆë‹¤">
<meta property="og:url" content="https://minzkn.com/claude/pages/tensor-core.html">
<meta property="og:image" content="https://minzkn.com/claude/images/og-image.png">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="NVIDIA Tensor Coreì˜ ì›ë¦¬, ì„¸ëŒ€ë³„ ë°œì „, LLM ì¶”ë¡ Â·í•™ìŠµ ìµœì í™” ê¸°ë²•ì„ ìƒì„¸íˆ ì†Œê°œí•©ë‹ˆë‹¤">
<meta name="keywords" content="tensor core nvidia gpu deep learning matrix multiplicationwmma cuda llm ì¶”ë¡  í•™ìŠµ ìµœì í™” volta ampere hopper">
<meta name="author" content="MINZKN">
<title>Tensor Core ì™„ì „ ê°€ì´ë“œ - AI Vibe Coding ê°€ì´ë“œ /with MINZKN</title>
<link rel="icon" type="image/svg+xml" href="../images/favicon.svg">
<link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/joungkyun/font-d2coding/d2coding.css">
<link rel="stylesheet" href="../css/themes.css">
<link rel="stylesheet" href="../css/style.css">
<link rel="stylesheet" href="../css/responsive.css">
</head>
<body>
<div class="page-wrapper">

<!-- ===== Header ===== -->
<header class="site-header">
</header>

<!-- ===== Side Navigation ===== -->
<nav class="side-nav" aria-label="ì‚¬ì´íŠ¸ ë‚´ë¹„ê²Œì´ì…˜">
</nav>

<!-- ===== Main Content ===== -->
<main class="main-content">
<div class="breadcrumb"></div>

<h1 id="top">NVIDIA Tensor Core ì™„ì „ ê°€ì´ë“œ</h1>
<p class="page-description">NVIDIA Tensor Coreì˜ ê¸°ë³¸ ì›ë¦¬ë¶€í„° LLM ìµœì í™”ê¹Œì§€, ë”¥ëŸ¬ë‹ ì¶”ë¡ ê³¼ í•™ìŠµì„ ê°€ì†í™”í•˜ëŠ” GPU í•µì‹¬ ê¸°ìˆ ì˜ ëª¨ë“  ê²ƒ</p>

<section class="content-section">
  <h2 id="what-is-tensor-core">Tensor Coreë€?</h2>
  <p>Tensor CoreëŠ” NVIDIAê°€ 2017ë…„ Volta ì•„í‚¤í…ì²˜ì—ì„œ ì²˜ìŒ ë„ì…í•œ ì „ìš© í–‰ë ¬ ê³±ì…ˆ ê°€ì† í•˜ë“œì›¨ì–´ì…ë‹ˆë‹¤. ì „í†µì ì¸ CUDA ì½”ì–´ ëŒ€ë¹„ í–‰ë ¬ ì—°ì‚°ì—ì„œ ìµœëŒ€ 12ë°°ì˜ ì„±ëŠ¥ í–¥ìƒì„ ì œê³µí•˜ë©°, íŠ¹íˆ ë”¥ëŸ¬ë‹ì˜ í•µì‹¬ ì—°ì‚°ì¸ í–‰ë ¬ ê³±ì…ˆ(Matrix Multiplication)ì„ í•˜ë“œì›¨ì–´ ìˆ˜ì¤€ì—ì„œ ê°€ì†í™”í•©ë‹ˆë‹¤.</p>

  <div class="info-box info">
    <strong>í•µì‹¬ ê°œë…:</strong> Tensor CoreëŠ” FP16, BF16, FP32, TF32, INT8, INT4 ë“± ë‹¤ì–‘í•œ ì •ë°€ë„ë¥¼ ì§€ì›í•˜ëŠ” í–‰ë ¬ ëˆ„ì‚°(Matrix Accumulate) ì—°ì‚°ì„ ë‹¨ì¼ í´ëŸ­ ì‚¬ì´í´ì—ì„œ ìˆ˜í–‰í•©ë‹ˆë‹¤.
  </div>

  <h3 id="cpu-vs-cuda-vs-tensor">CPU, CUDA Core, Tensor Core ë¹„êµ</h3>
  <table>
    <thead>
      <tr>
        <th>íŠ¹ì„±</th>
        <th>CPU</th>
        <th>CUDA Core</th>
        <th>Tensor Core</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>í–‰ë ¬ ê³±ì…ˆ ë°©ì‹</td>
        <td>ìˆœì°¨ ì²˜ë¦¬</td>
        <td>ë³‘ë ¬ ì²˜ë¦¬ (SIMT)</td>
        <td>í•˜ë“œì›¨ì–´ ê°€ì† (FMA)</td>
      </tr>
      <tr>
        <td>FP32 ì—°ì‚°</td>
        <td>1íšŒ/ì‚¬ì´í´</td>
        <td>1íšŒ/ì‚¬ì´í´</td>
        <td>64íšŒ/ì‚¬ì´í´ (A100)</td>
      </tr>
      <tr>
        <td>FP16 ì—°ì‚°</td>
        <td>1íšŒ/ì‚¬ì´í´</td>
        <td>2íšŒ/ì‚¬ì´í´</td>
        <td>512íšŒ/ì‚¬ì´í´ (A100)</td>
      </tr>
      <tr>
        <td>ì£¼ìš” ìš©ë„</td>
        <td>ì¼ë°˜ ì—°ì‚°</td>
        <td>ê·¸ë˜í”½, ë²”ìš© ë³‘ë ¬</td>
        <td>ë”¥ëŸ¬ë‹ í–‰ë ¬ ì—°ì‚°</td>
      </tr>
    </tbody>
  </table>

  <div class="diagram-container">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 700 350"
         style="width:100%;max-width:700px;display:block;margin:1.5em auto;">
      <defs>
        <marker id="tc-arrow" viewBox="0 0 10 7" refX="10" refY="3.5"
                markerWidth="10" markerHeight="7" orient="auto">
          <polygon points="0 0, 10 3.5, 0 7" fill="var(--diagram-arrow)"/>
        </marker>
      </defs>

      <!-- CPU Layer -->
      <rect x="20" y="20" width="660" height="50" rx="6"
            fill="var(--bg-secondary)" stroke="var(--border-color)" stroke-width="1.5"/>
      <text x="350" y="50" text-anchor="middle" fill="var(--diagram-text)" font-size="14" font-weight="bold">
        CPU (Scalar Processing)
      </text>

      <line x1="350" y1="70" x2="350" y2="95"
            stroke="var(--diagram-arrow)" stroke-width="2" marker-end="url(#tc-arrow)"/>

      <!-- CUDA Core Layer -->
      <rect x="20" y="95" width="660" height="50" rx="6"
            fill="var(--bg-secondary)" stroke="var(--accent-primary)" stroke-width="2"/>
      <text x="350" y="125" text-anchor="middle" fill="var(--diagram-text)" font-size="14" font-weight="bold">
        CUDA Core (SIMT Parallel)
      </text>

      <line x1="350" y1="145" x2="350" y2="170"
            stroke="var(--diagram-arrow)" stroke-width="2" marker-end="url(#tc-arrow)"/>

      <!-- Tensor Core Layer -->
      <rect x="20" y="170" width="660" height="50" rx="6"
            fill="var(--accent-primary)" fill-opacity="0.2"
            stroke="var(--accent-primary)" stroke-width="3"/>
      <text x="350" y="200" text-anchor="middle" fill="var(--diagram-text)" font-size="14" font-weight="bold">
        Tensor Core (Matrix FMA Hardware)
      </text>

      <line x1="350" y1="220" x2="350" y2="245"
            stroke="var(--diagram-arrow)" stroke-width="2" marker-end="url(#tc-arrow)"/>

      <!-- Operation Flow -->
      <rect x="20" y="245" width="660" height="50" rx="6"
            fill="var(--bg-secondary)" stroke="var(--border-color)" stroke-width="1.5"/>
      <text x="350" y="275" text-anchor="middle" fill="var(--diagram-text)" font-size="13">
        Input A (MÃ—K) Ã— Input B (KÃ—N) â†’ Output C (MÃ—N)
      </text>

      <!-- Performance Arrow -->
      <rect x="550" y="20" width="100" height="275" rx="6"
            fill="var(--diagram-accent)" fill-opacity="0.15"
            stroke="var(--diagram-accent)" stroke-width="2" stroke-dasharray="5,3"/>
      <text x="600" y="100" text-anchor="middle" fill="var(--diagram-accent)" font-size="11" font-weight="bold">
        Performance
      </text>
      <text x="600" y="130" text-anchor="middle" fill="var(--diagram-accent)" font-size="11">
        1x
      </text>
      <text x="600" y="165" text-anchor="middle" fill="var(--diagram-accent)" font-size="11">
        ~10x
      </text>
      <text x="600" y="215" text-anchor="middle" fill="var(--diagram-accent)" font-size="11">
        ~100x
      </text>

      <!-- Arrow from Tensor Core to Output -->
      <line x1="620" y1="245" x2="620" y2="295"
            stroke="var(--diagram-accent)" stroke-width="3"/>
      <polygon points="620,300 615,290 625,290" fill="var(--diagram-accent)"/>
    </svg>
    <p class="diagram-caption">ê·¸ë¦¼ 1: CPUì—ì„œ Tensor Coreë¡œì˜ ì—°ì‚° ì²˜ë¦¬ ë°©ì‹ ë³€í™”</p>
  </div>
</section>

<section class="content-section">
  <h2 id="how-it-works">ë™ì‘ ì›ë¦¬</h2>
  <p>Tensor CoreëŠ” WMMA(Warp-level Matrix Multiply-Accumulate) ì—°ì‚°ì„ í†µí•´ í–‰ë ¬ ê³±ì…ˆì„ ê°€ì†í™”í•©ë‹ˆë‹¤. í•œ ë²ˆì˜ ëª…ë ¹ì–´ë¡œ MÃ—K Ã— KÃ—N í–‰ë ¬ ê³±ì…ˆì„ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ MÃ—N í–‰ë ¬ì— ëˆ„ì‚°í•©ë‹ˆë‹¤.</p>

  <h3 id="wmma-operation">WMMA ì—°ì‚° êµ¬ì¡°</h3>
  <p>Tensor Coreì˜ í•µì‹¬ì€ Fused Multiply-Add (FMA) ì—°ì‚°ì„ í–‰ë ¬ ë‹¨ìœ„ë¡œ í™•ì¥í•œ ê²ƒì…ë‹ˆë‹¤:</p>
  <pre><code><span class="cmt">// CUDAì—ì„œ WMMA ì—°ì‚° ì˜ˆì‹œ (Pseudo-code)</span>
<span class="kw">using</span> wmma = <span class="fn">nvidia::wmma</span>;

<span class="cmt">// 16Ã—16 í–‰ë ¬ ì¡°ê° ë¡œë“œ</span>
fragment&lt;matrix_a&gt; <span class="fn">a_frag</span>;
fragment&lt;matrix_b&gt; <span class="fn">b_frag</span>;
fragment&lt;accumulator&gt; <span class="fn">c_frag</span>;

<span class="fn">load_matrix_sync</span>(a_frag, a_data, <span class="num">16</span>);
<span class="fn">load_matrix_sync</span>(b_frag, b_data, <span class="num">16</span>);
<span class="fn">load_matrix_sync</span>(c_frag, c_data, <span class="num">16</span>);

<span class="cmt">// í–‰ë ¬ ê³±ì…ˆ ëˆ„ì‚°: C += A Ã— B</span>
<span class="fn">mma_sync</span>(c_frag, a_frag, b_frag, c_frag);

<span class="cmt">// ê²°ê³¼ ì €ì¥</span>
<span class="fn">store_matrix_sync</span>(c_data, c_frag, <span class="num">16</span>, mem_row_major);</code></pre>

  <div class="diagram-container">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 700 280"
         style="width:100%;max-width:700px;display:block;margin:1.5em auto;">
      <defs>
        <marker id="tc-flow-arrow" viewBox="0 0 10 7" refX="10" refY="3.5"
                markerWidth="10" markerHeight="7" orient="auto">
          <polygon points="0 0, 10 3.5, 0 7" fill="var(--diagram-arrow)"/>
        </marker>
      </defs>

      <!-- Matrix A -->
      <rect x="30" y="30" width="120" height="80" rx="4"
            fill="var(--diagram-accent)" fill-opacity="0.3"
            stroke="var(--diagram-accent)" stroke-width="2"/>
      <text x="90" y="55" text-anchor="middle" fill="var(--diagram-text)" font-size="11">
        Matrix A
      </text>
      <text x="90" y="75" text-anchor="middle" fill="var(--diagram-text)" font-size="10">
        (M Ã— K)
      </text>

      <!-- Matrix B -->
      <rect x="30" y="130" width="120" height="80" rx="4"
            fill="var(--diagram-accent)" fill-opacity="0.3"
            stroke="var(--diagram-accent)" stroke-width="2"/>
      <text x="90" y="155" text-anchor="middle" fill="var(--diagram-text)" font-size="11">
        Matrix B
      </text>
      <text x="90" y="175" text-anchor="middle" fill="var(--diagram-text)" font-size="10">
        (K Ã— N)
      </text>

      <!-- Tensor Core Operation -->
      <rect x="250" y="80" width="160" height="100" rx="8"
            fill="var(--accent-primary)" fill-opacity="0.2"
            stroke="var(--accent-primary)" stroke-width="3"/>
      <text x="330" y="115" text-anchor="middle" fill="var(--diagram-text)" font-size="13" font-weight="bold">
        Tensor Core
      </text>
      <text x="330" y="135" text-anchor="middle" fill="var(--diagram-text)" font-size="11">
        MMA (FMA Ã— Matrix)
      </text>
      <text x="330" y="155" text-anchor="middle" fill="var(--diagram-text)" font-size="10">
        C += A Ã— B
      </text>

      <!-- Arrows A â†’ TC -->
      <line x1="150" y1="70" x2="250" y2="100"
            stroke="var(--diagram-arrow)" stroke-width="2" marker-end="url(#tc-flow-arrow)"/>

      <!-- Arrow B â†’ TC -->
      <line x1="150" y1="170" x2="250" y2="160"
            stroke="var(--diagram-arrow)" stroke-width="2" marker-end="url(#tc-flow-arrow)"/>

      <!-- Matrix C -->
      <rect x="520" y="80" width="150" height="100" rx="4"
            fill="var(--diagram-accent)" fill-opacity="0.2"
            stroke="var(--diagram-accent)" stroke-width="2"/>
      <text x="595" y="115" text-anchor="middle" fill="var(--diagram-text)" font-size="11" font-weight="bold">
        Matrix C
      </text>
      <text x="595" y="135" text-anchor="middle" fill="var(--diagram-text)" font-size="10">
        Accumulator
      </text>
      <text x="595" y="155" text-anchor="middle" fill="var(--diagram-text)" font-size="10">
        (M Ã— N)
      </text>

      <!-- Arrow TC â†’ C -->
      <line x1="410" y1="130" x2="520" y2="130"
            stroke="var(--diagram-arrow)" stroke-width="2" marker-end="url(#tc-flow-arrow)"/>
    </svg>
    <p class="diagram-caption">ê·¸ë¦¼ 2: Tensor Core WMMA ì—°ì‚° ë°ì´í„° í”Œë¡œìš°</p>
  </div>

  <h3 id="precision-formats">ì§€ì› ì •ë°€ë„ í˜•ì‹</h3>
  <table>
    <thead>
      <tr>
        <th>ì •ë°€ë„</th>
        <th>ë¹„íŠ¸</th>
        <th>ì£¼ìš” ìš©ë„</th>
        <th>Tensor Core ì§€ì›</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>FP32</td>
        <td>32</td>
        <td>í•™ìŠµ, ì •ë°€ ì—°ì‚°</td>
        <td>âœ“ (Hopper)</td>
      </tr>
      <tr>
        <td>TF32</td>
        <td>32</td>
        <td>í•™ìŠµ (ìë™ ë³€í™˜)</td>
        <td>âœ“ (Ampere+)</td>
      </tr>
      <tr>
        <td>FP16</td>
        <td>16</td>
        <td>í•™ìŠµ, ì¶”ë¡ </td>
        <td>âœ“ (Volta+)</td>
      </tr>
      <tr>
        <td>BF16</td>
        <td>16</td>
        <td>í•™ìŠµ (ë™ì  ë²”ìœ„)</td>
        <td>âœ“ (Ampere+)</td>
      </tr>
      <tr>
        <td>FP8</td>
        <td>8</td>
        <td>ì¶”ë¡  (Hopper)</td>
        <td>âœ“ (Hopper)</td>
      </tr>
      <tr>
        <td>INT8</td>
        <td>8</td>
        <td>ì¶”ë¡ </td>
        <td>âœ“ (Turing+)</td>
      </tr>
      <tr>
        <td>INT4</td>
        <td>4</td>
        <td>ì¶”ë¡  (ì–‘ìí™”)</td>
        <td>âœ“ (Hopper+)</td>
      </tr>
    </tbody>
  </table>
</section>

<section class="content-section">
  <h2 id="generation-evolution">ì„¸ëŒ€ë³„ ë°œì „</h2>

  <h3 id="volta-v100">Volta (V100, 2017)</h3>
  <p>ìµœì´ˆì˜ Tensor Coreê°€ ë„ì…ëœ ì„¸ëŒ€ì…ë‹ˆë‹¤. FP16 ì •ë°€ë„ë¥¼ ìµœì´ˆë¡œ ì§€ì›í•˜ë©°, ë”¥ëŸ¬ë‹ í•™ìŠµì—ì„œ í˜ì‹ ì ì¸ ì„±ëŠ¥ í–¥ìƒì„ ì œê³µí–ˆìŠµë‹ˆë‹¤.</p>
  <ul>
    <li>640ê°œì˜ Tensor Core (V100)</li>
    <li>FP16 í–‰ë ¬ ì—°ì‚° ì§€ì›</li>
    <li>CUDA 9.0 ì´ìƒ í•„ìš”</li>
  </ul>

  <h3 id="turing-t4">Turing (T4, 2018)</h3>
  <p>ì¶”ë¡ ì„ ìœ„í•´ ì„¤ê³„ëœ ì²« ì„¸ëŒ€ë¡œ, INT8/INT4 ì–‘ìí™”ë¥¼ í•˜ë“œì›¨ì–´ ìˆ˜ì¤€ì—ì„œ ì§€ì›í–ˆìŠµë‹ˆë‹¤.</p>
  <ul>
    <li>256ê°œì˜ Tensor Core (T4)</li>
    <li>INT8/INT4 ì¶”ë¡  ê°€ì†</li>
    <li>RTX ì‹œë¦¬ì¦ˆ ê²Œì„ GPUì—ë„ ì ìš©</li>
  </ul>

  <h3 id="ampere-a100">Ampere (A100, 2020)</h3>
  <p>ë°ì´í„°ì„¼í„° í‘œì¤€ì´ ëœ ì„¸ëŒ€ë¡œ, TF32, BF16, sparsity ì§€ì›ì´ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.</p>
  <ul>
    <li>432ê°œì˜ Tensor Core (A100)</li>
    <li>TF32/BF16 ìë™ ë³€í™˜</li>
    <li>Sparsity (êµ¬ì¡°ì  ê°€ì§€ì¹˜ê¸°) ì§€ì›</li>
    <li>MIG (Multi-Instance GPU) partitioning</li>
  </ul>

  <h3 id="ada-lovelace">Ada Lovelace (H100, 2022)</h3>
  <p>Transformer ì—”ì§„ì´ ì¶”ê°€ë˜ì–´ LLM ì¶”ë¡ ì— ìµœì í™”ë˜ì—ˆìŠµë‹ˆë‹¤.</p>
<ul>
    <li>132ê°œì˜ Tensor Core í´ëŸ¬ìŠ¤í„° Ã— 2 (H100)</li>
    <li>Transformer ì—”ì§„ (FP8 ë™ì -precision)</li>
    <li>FP8 ì¶”ë¡  ì§€ì›</li>
    <li>NVLink 4ì„¸ëŒ€ (900GB/s)</li>
  </ul>

  <h3 id="hopper-h100">Hopper (H100, 2023)</h3>
  <p>ìµœì‹  ì„¸ëŒ€ë¡œ, sparsity, FP8, Transformer ìµœì í™”ê°€ ì§‘ì•½ë˜ì—ˆìŠµë‹ˆë‹¤.</p>
  <ul>
    <li>132ê°œì˜ Tensor Core í´ëŸ¬ìŠ¤í„° Ã— 2 (H100)</li>
    <li>Transformer ì—”ì§„ (FP8 ë™ì  ì •ë°€ë„)</li>
    <li>FP8 ì¶”ë¡  ìµœì í™”</li>
    <li>Dynamic Programming, Collective Operations ì§€ì›</li>
  </ul>

  <h3 id="blackwell-b100">Blackwell (B100/B200, 2024)</h3>
  <p>ê°€ì¥ ìµœì‹  ì„¸ëŒ€ë¡œ, FP4 ì–‘ìí™”ì™€ ëŒ€í­ í–¥ìƒëœ ì¶”ë¡  ì„±ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.</p>
  <ul>
    <li>FP4 ì •ë°€ë„ ì§€ì›</li>
    <li>FP8 ëŒ€ë¹„ 2ë°° íš¨ìœ¨</li>
    <li>ì œ3ì„¸ëŒ€ Tensor Core</li>
    <li>NVLink 5ì„¸ëŒ€ (1.8TB/s)</li>
  </ul>

  <div class="diagram-container">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 700 320"
         style="width:100%;max-width:700px;display:block;margin:1.5em auto;">
      <defs>
        <marker id="tc-gen-arrow" viewBox="0 0 10 7" refX="10" refY="3.5"
                markerWidth="10" markerHeight="7" orient="auto">
          <polygon points="0 0, 10 3.5, 0 7" fill="var(--diagram-arrow)"/>
        </marker>
      </defs>

      <!-- Volta -->
      <rect x="30" y="20" width="130" height="60" rx="6"
            fill="var(--bg-secondary)" stroke="var(--border-color)" stroke-width="1.5"/>
      <text x="95" y="45" text-anchor="middle" fill="var(--diagram-text)" font-size="12" font-weight="bold">
        Volta (2017)
      </text>
      <text x="95" y="62" text-anchor="middle" fill="var(--text-secondary)" font-size="10">
        FP16, V100
      </text>

      <line x1="160" y1="50" x2="220" y2="50"
            stroke="var(--diagram-arrow)" stroke-width="2" marker-end="url(#tc-gen-arrow)"/>

      <!-- Turing -->
      <rect x="220" y="20" width="130" height="60" rx="6"
            fill="var(--bg-secondary)" stroke="var(--border-color)" stroke-width="1.5"/>
      <text x="285" y="45" text-anchor="middle" fill="var(--diagram-text)" font-size="12" font-weight="bold">
        Turing (2018)
      </text>
      <text x="285" y="62" text-anchor="middle" fill="var(--text-secondary)" font-size="10">
        INT8/4, T4
      </text>

      <line x1="350" y1="50" x2="410" y2="50"
            stroke="var(--diagram-arrow)" stroke-width="2" marker-end="url(#tc-gen-arrow)"/>

      <!-- Ampere -->
      <rect x="410" y="20" width="130" height="60" rx="6"
            fill="var(--accent-primary)" fill-opacity="0.2"
            stroke="var(--accent-primary)" stroke-width="2"/>
      <text x="475" y="45" text-anchor="middle" fill="var(--diagram-text)" font-size="12" font-weight="bold">
        Ampere (2020)
      </text>
      <text x="475" y="62" text-anchor="middle" fill="var(--text-secondary)" font-size="10">
        TF32/BF16, A100
      </text>

      <line x1="540" y1="50" x2="600" y2="50"
            stroke="var(--diagram-arrow)" stroke-width="2" marker-end="url(#tc-gen-arrow)"/>

      <!-- Ada/Hopper -->
      <rect x="600" y="20" width="80" height="60" rx="6"
            fill="var(--accent-primary)" fill-opacity="0.3"
            stroke="var(--accent-primary)" stroke-width="2"/>
      <text x="640" y="45" text-anchor="middle" fill="var(--diagram-text)" font-size="11" font-weight="bold">
        Hopper
      </text>
      <text x="640" y="62" text-anchor="middle" fill="var(--text-secondary)" font-size="9">
        FP8
      </text>

      <!-- Row 2 -->
      <rect x="30" y="110" width="130" height="60" rx="6"
            fill="var(--bg-secondary)" stroke="var(--border-color)" stroke-width="1.5"/>
      <text x="95" y="135" text-anchor="middle" fill="var(--diagram-text)" font-size="12" font-weight="bold">
        Volta (2017)
      </text>
      <text x="95" y="152" text-anchor="middle" fill="var(--text-secondary)" font-size="10">
        FP16 only
      </text>

      <rect x="220" y="110" width="130" height="60" rx="6"
            fill="var(--bg-secondary)" stroke="var(--border-color)" stroke-width="1.5"/>
      <text x="285" y="135" text-anchor="middle" fill="var(--diagram-text)" font-size="12" font-weight="bold">
        Turing (2018)
      </text>
      <text x="285" y="152" text-anchor="middle" fill="var(--text-secondary)" font-size="10">
        + INT8/4
      </text>

      <rect x="410" y="110" width="130" height="60" rx="6"
            fill="var(--accent-primary)" fill-opacity="0.2"
            stroke="var(--accent-primary)" stroke-width="2"/>
      <text x="475" y="135" text-anchor="middle" fill="var(--diagram-text)" font-size="12" font-weight="bold">
        Ampere (2020)
      </text>
      <text x="475" y="152" text-anchor="middle" fill="var(--text-secondary)" font-size="10">
        + TF32/BF16/Sparse
      </text>

      <rect x="600" y="110" width="80" height="60" rx="6"
            fill="var(--accent-primary)" fill-opacity="0.3"
            stroke="var(--accent-primary)" stroke-width="2"/>
      <text x="640" y="135" text-anchor="middle" fill="var(--diagram-text)" font-size="11" font-weight="bold">
        Hopper
      </text>
      <text x="640" y="152" text-anchor="middle" fill="var(--text-secondary)" font-size="9">
        + FP8/Transformer
      </text>

      <!-- Performance Comparison -->
      <rect x="30" y="200" width="650" height="100" rx="6"
            fill="var(--bg-secondary)" stroke="var(--border-color)" stroke-width="1.5"/>
      <text x="355" y="225" text-anchor="middle" fill="var(--diagram-text)" font-size="12" font-weight="bold">
        ìƒëŒ€ì  ì¶”ë¡  ì„±ëŠ¥ (FP16/BF16 ê¸°ì¤€)
      </text>

      <!-- V100 bar -->
      <rect x="50" y="240" width="80" height="30" rx="4"
            fill="var(--border-color)"/>
      <text x="90" y="260" text-anchor="middle" fill="var(--bg-secondary)" font-size="10">
        V100
      </text>
      <text x="90" y="278" text-anchor="middle" fill="var(--text-secondary)" font-size="9">
        1x
      </text>

      <!-- A100 bar -->
      <rect x="170" y="240" width="100" height="30" rx="4"
            fill="var(--accent-primary)" fill-opacity="0.4"/>
      <text x="220" y="260" text-anchor="middle" fill="var(--bg-secondary)" font-size="10">
        A100
      </text>
      <text x="220" y="278" text-anchor="middle" fill="var(--text-secondary)" font-size="9">
        ~3x
      </text>

      <!-- H100 bar -->
      <rect x="310" y="240" width="140" height="30" rx="4"
            fill="var(--accent-primary)" fill-opacity="0.6"/>
      <text x="380" y="260" text-anchor="middle" fill="var(--bg-secondary)" font-size="10">
        H100
      </text>
      <text x="380" y="278" text-anchor="middle" fill="var(--text-secondary)" font-size="9">
        ~9x (FP8: ~30x)
      </text>

      <!-- B100 bar -->
      <rect x="490" y="240" width="150" height="30" rx="4"
            fill="var(--accent-primary)" fill-opacity="0.8"/>
      <text x="565" y="260" text-anchor="middle" fill="var(--bg-secondary)" font-size="10">
        B200
      </text>
      <text x="565" y="278" text-anchor="middle" fill="var(--text-secondary)" font-size="9">
        ~30x (FP4: ~60x)
      </text>
    </svg>
    <p class="diagram-caption">ê·¸ë¦¼ 3: Tensor Core ì„¸ëŒ€ë³„ ë°œì „ê³¼ ì„±ëŠ¥ ë¹„êµ</p>
  </div>
</section>

<section class="content-section">
  <h2 id="llm-optimization">LLM ìµœì í™” í™œìš©</h2>
  <p>Tensor CoreëŠ” LLMì˜ í•™ìŠµê³¼ ì¶”ë¡ ì—ì„œ í•µì‹¬ ì—­í• ì„ í•©ë‹ˆë‹¤. ìµœì‹  GPUë“¤ì€ Transformer íŠ¹í™” ìµœì í™”ë¥¼ í†µí•´ LLM ì„±ëŠ¥ì„ ê·¹ëŒ€í™”í•©ë‹ˆë‹¤.</p>

  <h3 id="inference-optimization">ì¶”ë¡  ìµœì í™” ê¸°ë²•</h3>

  <h4 id="kv-cache-optimization">KV Cache ìµœì í™”</h4>
  <p>ì¶”ë¡  ì‹œ ì´ì „ í† í°ì˜ Key-Value í–‰ë ¬ì„ ìºì‹±í•˜ì—¬ ì¤‘ë³µ ê³„ì‚°ì„ ë°©ì§€í•©ë‹ˆë‹¤. Tensor CoreëŠ” ì´ KV ìºì‹œ í–‰ë ¬ ê³±ì…ˆì„ ê°€ì†í™”í•©ë‹ˆë‹¤.</p>

  <h4 id="quantization">ì–‘ìí™” (Quantization)</h4>
  <p>FP16/BF16ì—ì„œ INT8/INT4ë¡œ ëª¨ë¸ì„ ì–‘ìí™”í•˜ì—¬ ë©”ëª¨ë¦¬ì™€ ëŒ€ì—­í­ì„ ì ˆì•½í•˜ë©´ì„œ ì¶”ë¡  ì†ë„ë¥¼ í–¥ìƒì‹œí‚µë‹ˆë‹¤.</p>
  <pre><code><span class="cmt"># PyTorchì—ì„œ INT8 ì–‘ìí™” ì˜ˆì‹œ</span>
<span class="kw">import</span> torch.quantization

<span class="cmt"># ë™ì  ì–‘ìí™” (Dynamic Quantization)</span>
model_int8 = torch.quantization.quantize_dynamic(
    model,
    {torch.nn.Linear: torch.qint8},
    dtype=torch.qint8
)

<span class="cmt"># ì¶”ë¡ </span> 
<span class="kw">with</span> torch.no_grad():
    output = <span class="fn">model_int8</span>(input_ids)</code></pre>

  <div class="info-box tip">
    <strong>ğŸ’¡ íŒ:</strong> H100ì˜ FP8 ë™ì  ì •ë°€ë„ëŠ” ëª¨ë¸ì˜ ê° ë ˆì´ì–´ë³„ ì—°ì‚° íŠ¹ì„±ì— ë”°ë¼ ìë™ìœ¼ë¡œ ì •ë°€ë„ë¥¼ ì¡°ì ˆí•©ë‹ˆë‹¤.
  </div>

  <h4 id="paged-attention">Paged Attention (vLLM)</h4>
  <p>KV ìºì‹œë¥¼ í˜ì´ì§€ ë‹¨ìœ„ë¡œ ê´€ë¦¬í•˜ì—¬ ë©”ëª¨ë¦¬ ë‹¨í¸í™”ë¥¼ ì¤„ì´ê³ , ë” í° ë°°ì¹˜ í¬ê¸°ë¡œ ì²˜ë¦¬ëŸ‰ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.</p>

  <h3 id="training-optimization">í•™ìŠµ ìµœì í™” ê¸°ë²•</h3>

  <h4 id="mixed-precision">Mixed Precision Training</h4>
  <p>FP16/BF16 í˜¼í•© ì •ë°€ë„ í•™ìŠµìœ¼ë¡œ ë©”ëª¨ë¦¬ë¥¼ ì ˆì•½í•˜ë©´ì„œë„ FP32 ëŒ€ë¹„ ìœ ì‚¬í•œ í’ˆì§ˆì„ ìœ ì§€í•©ë‹ˆë‹¤.</p>
  <pre><code><span class="cmt"># PyTorch Mixed Precision í•™ìŠµ</span>
<span class="kw">from</span> torch.cuda.amp <span class="kw">import</span> autocast, GradScaler

scaler = <span class="fn">GradScaler</span>()

<span class="kw">for</span> data, target <span class="kw">in</span> dataloader:
    optimizer.zero_grad()
    
    <span class="cmt"># ìë™ mixed precision</span>
    <span class="kw">with</span> <span class="fn">autocast</span>(device_type=<span class="str">'cuda'</span>):
        output = <span class="fn">model</span>(data)
        loss = <span class="fn">criterion</span>(output, target)
    
    <span class="cmt"># ê·¸ë˜ë””ì–¸íŠ¸ ìŠ¤ì¼€ì¼ë§</span>
    scaler.scale(loss).backward()
    scaler.step(optimizer)
    scaler.update()</code></pre>

  <h4 id="gradient-accumulation">Gradient Accumulation</h4>
  <p>ë©”ëª¨ë¦¬ ì œì•½ìœ¼ë¡œ í° ë°°ì¹˜ í¬ê¸°ë¥¼ ì‚¬ìš©í•˜ì§€ ëª»í•˜ëŠ” ê²½ìš°, ì‘ì€ ë°°ì¹˜ì˜ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ëˆ„ì í•˜ì—¬ ë™ì¼í•œ íš¨ê³¼ë¥¼ ì–»ìŠµë‹ˆë‹¤.</p>

  <div class="diagram-container">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 700 380"
         style="width:100%;max-width:700px;display:block;margin:1.5em auto;">
      <defs>
        <marker id="llm-arrow" viewBox="0 0 10 7" refX="10" refY="3.5"
                markerWidth="10" markerHeight="7" orient="auto">
          <polygon points="0 0, 10 3.5, 0 7" fill="var(--diagram-arrow)"/>
        </marker>
      </defs>

      <!-- Input Tokens -->
      <rect x="20" y="30" width="140" height="50" rx="6"
            fill="var(--bg-secondary)" stroke="var(--border-color)" stroke-width="1.5"/>
      <text x="90" y="60" text-anchor="middle" fill="var(--diagram-text)" font-size="12">
        Input Tokens
      </text>

      <!-- Embedding -->
      <rect x="200" y="30" width="120" height="50" rx="6"
            fill="var(--bg-secondary)" stroke="var(--border-color)" stroke-width="1.5"/>
      <text x="260" y="60" text-anchor="middle" fill="var(--diagram-text)" font-size="12">
        Embedding
      </text>

      <line x1="160" y1="55" x2="200" y2="55"
            stroke="var(--diagram-arrow)" stroke-width="2" marker-end="url(#llm-arrow)"/>

      <!-- Transformer Block 1 -->
      <rect x="360" y="10" width="200" height="90" rx="8"
            fill="var(--accent-primary)" fill-opacity="0.15"
            stroke="var(--accent-primary)" stroke-width="2"/>
      <text x="460" y="35" text-anchor="middle" fill="var(--diagram-text)" font-size="11" font-weight="bold">
        Transformer Block 1
      </text>
      <text x="460" y="52" text-anchor="middle" fill="var(--text-secondary)" font-size="10">
        Attention + FFN
      </text>
      <text x="460" y="69" text-anchor="middle" fill="var(--diagram-accent)" font-size="10">
        Tensor Core: QÃ—Káµ€, V
      </text>

      <line x1="320" y1="55" x2="360" y2="55"
            stroke="var(--diagram-arrow)" stroke-width="2" marker-end="url(#llm-arrow)"/>

      <!-- ... More Blocks -->
      <rect x="360" y="110" width="200" height="30" rx="6"
            fill="var(--border-color)" stroke="var(--border-color)" stroke-width="1" stroke-dasharray="4,2"/>
      <text x="460" y="130" text-anchor="middle" fill="var(--text-secondary)" font-size="10">
        ... (N blocks)
      </text>

      <!-- Output -->
      <rect x="600" y="30" width="80" height="50" rx="6"
            fill="var(--bg-secondary)" stroke="var(--border-color)" stroke-width="1.5"/>
      <text x="640" y="60" text-anchor="middle" fill="var(--diagram-text)" font-size="11">
        Output
      </text>

      <line x1="560" y1="55" x2="600" y2="55"
            stroke="var(--diagram-arrow)" stroke-width="2" marker-end="url(#llm-arrow)"/>

      <!-- GPU Memory Layout -->
      <rect x="20" y="180" width="660" height="180" rx="6"
            fill="var(--bg-secondary)" stroke="var(--border-color)" stroke-width="1.5"/>
      <text x="350" y="205" text-anchor="middle" fill="var(--diagram-text)" font-size="12" font-weight="bold">
        GPU Memory & Tensor Core í™œìš©
      </text>

      <!-- Weights -->
      <rect x="40" y="220" width="180" height="50" rx="4"
            fill="var(--diagram-accent)" fill-opacity="0.3" stroke="var(--diagram-accent)" stroke-width="1.5"/>
      <text x="130" y="250" text-anchor="middle" fill="var(--diagram-text)" font-size="11">
        Model Weights
      </text>

      <!-- KV Cache -->
      <rect x="250" y="220" width="180" height="50" rx="4"
            fill="var(--diagram-accent)" fill-opacity="0.2" stroke="var(--diagram-accent)" stroke-width="1.5"/>
      <text x="340" y="250" text-anchor="middle" fill="var(--diagram-text)" font-size="11">
        KV Cache
      </text>

      <!-- Activations -->
      <rect x="460" y="220" width="180" height="50" rx="4"
            fill="var(--bg-secondary)" stroke="var(--border-color)" stroke-width="1.5"/>
      <text x="550" y="250" text-anchor="middle" fill="var(--diagram-text)" font-size="11">
        Activations
      </text>

      <!-- Tensor Core -->
      <rect x="160" y="300" width="200" height="40" rx="6"
            fill="var(--accent-primary)" fill-opacity="0.3"
            stroke="var(--accent-primary)" stroke-width="2"/>
      <text x="260" y="325" text-anchor="middle" fill="var(--diagram-text)" font-size="11" font-weight="bold">
        Tensor Core Operations
      </text>
      <text x="260" y="340" text-anchor="middle" fill="var(--text-secondary)" font-size="9">
        QÃ—Káµ€, Attention, FFN, Output Projection
      </text>

      <!-- Arrows to Tensor Core -->
      <line x1="130" y1="270" x2="130" y2="300"
            stroke="var(--diagram-arrow)" stroke-width="1.5"/>
      <line x1="220" y1="270" x2="220" y2="300"
            stroke="var(--diagram-arrow)" stroke-width="1.5"/>
      <line x1="340" y1="270" x2="340" y2="300"
            stroke="var(--diagram-arrow)" stroke-width="1.5"/>
      <line x1="430" y1="270" x2="430" y2="300"
            stroke="var(--diagram-arrow)" stroke-width="1.5"/>
    </svg>
    <p class="diagram-caption">ê·¸ë¦¼ 4: LLM ì¶”ë¡ ì—ì„œ Tensor Core í™œìš© êµ¬ì¡°</p>
  </div>
</section>

<section class="content-section">
  <h2 id="practical-usage">ì‹¤ì „ ì‚¬ìš©ë²•</h2>

  <h3 id="cuda-programming">CUDA í”„ë¡œê·¸ë˜ë°</h3>
  <p>Tensor Coreë¥¼ ì§ì ‘ í”„ë¡œê·¸ë˜ë°í•˜ë ¤ë©´ CUDAì™€ cuBLAS ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.</p>

  <h4 id="cublas-usage">cuBLAS ì‚¬ìš©</h4>
  <pre><code><span class="cmt">// cuBLAS Tensor Core í–‰ë ¬ ê³±ì…ˆ</span>
<span class="pp">#include</span> <span class="str">&lt;cublas_v2.h&gt;</span>

<span class="type">void</span> <span class="fn">tensor_core_gemm</span>(cublasHandle_t handle,
    <span class="type">float</span>* A, <span class="type">float</span>* B, <span class="type">float</span>* C,
    <span class="type">int</span> m, <span class="type">int</span> n, <span class="type">int</span> k) {
    
    <span class="type">float</span> alpha = <span class="num">1.0f</span>;
    <span class="type">float</span> beta = <span class="num">0.0f</span>;
    
    <span class="cmt">// FP16 Tensor Core ì‚¬ìš©</span>
    cublasSetMathMode(handle, CUBLAS_TENSOR_OP_MATH);
    
    <span class="fn">cublasGemmEx</span>(handle,
        CUBLAS_OP_N, CUBLAS_OP_N,
        n, m, k,
        &alpha, B, CUDA_R_16F, n,
                 A, CUDA_R_16F, k,
        &beta,  C, CUDA_R_16F, n,
        CUBLAS_GEMM_DEFAULT_TENSOR_OP);
}</code></pre>

  <h3 id="pytorch-usage">PyTorchì—ì„œ í™œìš©</h3>
  <pre><code><span class="cmt"># PyTorch Tensor Core í™œì„±í™” í™•ì¸</span>
<span class="kw">import</span> torch

<span class="cmt"># Tensor Core ì‚¬ìš© ê°€ëŠ¥í•œì§€ í™•ì¸</span>
<span class="fn">print</span>(torch.cuda.is_available())
<span class="fn">print</span>(torch.cuda.get_device_capability())

<span class="cmt"># TF32 í™œì„±í™” (Ampere+)</span>
torch.backends.cuda.matmul.allow_tf32 = <span class="kw">True</span>
torch.backends.cudnn.allow_tf32 = <span class="kw">True</span>

<span class="cmt"># cuDNN ìë™ íŠœë‹</span>
torch.backends.cudnn.benchmark = <span class="kw">True</span>

<span class="cmt"># FP8 ì§€ì› í™•ì¸ (Hopper+)</span>
<span class="kw">if</span> torch.cuda.get_device_capability() >= (<span class="num">9</span>, <span class="num">0</span>):
    <span class="fn">print</span>(<span class="str">"FP8 Tensor Core available"</span>)</code></pre>

  <h3 id="vllm-usage">vLLMìœ¼ë¡œ ì¶”ë¡  ê°€ì†í™”</h3>
  <pre><code><span class="cmt"># vLLMìœ¼ë¡œ Tensor Core ì¶”ë¡ </span>
<span class="kw">from</span> vllm <span class="kw">import</span> LLM, SamplingParams

<span class="cmt"># H100ì—ì„œ FP8 ì¶”ë¡ </span>
llm = <span class="fn">LLM</span>(
    model=<span class="str">"meta-llama/Llama-2-70b-hf"</span>,
    tensor_parallel_size=<span class="num">4</span>,
    dtype=<span class="str">"half"</span>,  <span class="cmt"># FP16</span>
    <span class="cmt"># dtype="float8"  # FP8 (Hopper)</span>
)

sampling_params = <span class="fn">SamplingParams</span>(
    temperature=<span class="num">0.8</span>,
    max_tokens=<span class="num">256</span>
)

outputs = llm.generate(
    [<span class="str">"Explain quantum computing in simple terms"</span>],
    sampling_params
)</code></pre>

  <div class="info-box warning">
    <strong>âš ï¸ ì£¼ì˜:</strong> Tensor Coreë¥¼ ìµœëŒ€í•œ í™œìš©í•˜ë ¤ë©´ CUDA, cuDNN, PyTorch ë²„ì „ì´ GPUë¥¼ ì§€ì›í•´ì•¼ í•©ë‹ˆë‹¤. ë“œë¼ì´ë²„ì™€ ë¼ì´ë¸ŒëŸ¬ë¦¬ í˜¸í™˜ì„±ì„ í™•ì¸í•˜ì„¸ìš”.
  </div>
</section>

<section class="content-section">
  <h2 id="performance-benchmark">ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬</h2>
  <p>LLM ì¶”ë¡ ì—ì„œ Tensor Coreì˜ ì„±ëŠ¥ ì˜í–¥ì„ ì´í•´í•˜ê¸° ìœ„í•œ ë²¤ì¹˜ë§ˆí¬ ê°€ì´ë“œì…ë‹ˆë‹¤.</p>

  <h3 id="llm-benchmark">ì¶”ë¡  ì„±ëŠ¥ ë¹„êµ</h3>
  <table>
    <thead>
      <tr>
        <th>GPU</th>
        <th>ì •ë°€ë„</th>
        <th> throughput (tokens/s)</th>
        <th>Latency (ms)</th>
        <th>VRAM</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>A100-40GB</td>
        <td>FP16</td>
        <td>~50</td>
        <td>~200</td>
        <td>40GB</td>
      </tr>
      <tr>
        <td>A100-80GB</td>
        <td>FP16</td>
        <td>~80</td>
        <td>~125</td>
        <td>80GB</td>
      </tr>
      <tr>
        <td>H100</td>
        <td>FP16</td>
        <td>~200</td>
        <td>~50</td>
        <td>80GB</td>
      </tr>
      <tr>
        <td>H100</td>
        <td>FP8</td>
        <td>~500</td>
        <td>~20</td>
        <td>80GB</td>
      </tr>
      <tr>
        <td>B200</td>
        <td>FP8</td>
        <td>~1800</td>
        <td>~5</td>
        <td>192GB</td>
      </tr>
      <tr>
        <td>B200</td>
        <td>FP4</td>
        <td>~3500</td>
        <td>~3</td>
        <td>192GB</td>
      </tr>
    </tbody>
  </table>
  <p class="diagram-caption">í‘œ: 70B íŒŒë¼ë¯¸í„° LLM ì¶”ë¡  ë²¤ì¹˜ë§ˆí¬ (ì´ˆë‹¹ ì¶œë ¥ í† í° ìˆ˜, ë°°ì¹˜ í¬ê¸° 1)</p>

  <h3 id="memory-bandwidth">ë©”ëª¨ë¦¬ ëŒ€ì—­í­ ì¤‘ìš”ì„±</h3>
  <p>Tensor Core ì„±ëŠ¥ì„ ì™„ì „íˆ í™œìš©í•˜ë ¤ë©´ ì¶©ë¶„í•œ ë©”ëª¨ë¦¬ ëŒ€ì—­í­ì´ í•„ìš”í•©ë‹ˆë‹¤. H100 NVLinkëŠ” 900GB/s, B200 NVLink 5ëŠ” 1.8TB/së¥¼ ì œê³µí•©ë‹ˆë‹¤.</p>
</section>

<section class="content-section">
  <h2 id="best-practices">ëª¨ë²” ì‚¬ë¡€</h2>
  <ul>
    <li><strong>ë“œë¼ì´ë²„ ì—…ë°ì´íŠ¸:</strong> ìµœì‹  NVIDIA ë“œë¼ì´ë²„ë¡œ Tensor Core ìµœì í™” í™œìš©</li>
    <li><strong>CUDA ë²„ì „:</strong> CUDA 11.8 ì´ìƒ (Tensor Core ì™„ì „ ì§€ì›)</li>
    <li><strong>PyTorch ë²„ì „:</strong> 1.13 ì´ìƒ (AMP ë° Tensor Core ìë™ í™œìš©)</li>
    <li><strong>ì •ë°€ë„ ì„ íƒ:</strong> í•™ìŠµì€ BF16, ì¶”ë¡ ì€ FP8/INT8</li>
    <li><strong>ë°°ì¹˜ í¬ê¸°:</strong> VRAM ëŒ€ë¹„ ìµœëŒ€ ë°°ì¹˜ í¬ê¸° ì‹œë„</li>
    <li><strong>vLLM/TensorRT-LLM:</strong> ì¶”ë¡  ìµœì í™” í”„ë ˆì„ì›Œí¬ ì‚¬ìš©</li>
  </ul>
</section>

<section class="content-section">
  <h2 id="references">ì°¸ê³ ìë£Œ</h2>
  <ul>
    <li><a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#tensor-cores" target="_blank" rel="noopener">NVIDIA CUDA Tensor Core ë¬¸ì„œ</a></li>
    <li><a href="https://developer.nvidia.com/blog/nvidia-hopper-architecture-introduction/" target="_blank" rel="noopener">NVIDIA Hopper Architecture</a></li>
    <li><a href="https://pytorch.org/docs/stable/amp.html" target="_blank" rel="noopener">PyTorch Automatic Mixed Precision</a></li>
    <li><a href="https://docs.vllm.ai/" target="_blank" rel="noopener">vLLM ë¬¸ì„œ</a></li>
    <li><a href="pages/llm-theory.html">LLM ì´ë¡  ì¢…í•©</a></li>
    <li><a href="pages/gguf-format.html">GGUF ëª¨ë¸ í¬ë§· (ì–‘ìí™”)</a></li>
  </ul>

  <div class="info-box info">
    <strong>ë‹¤ìŒ í•™ìŠµ:</strong>
    <ul>
      <li><a href="pages/llm-theory.html">LLM ì´ë¡  ì¢…í•©</a> - ë”¥ëŸ¬ë‹ ê¸°ì´ˆ ì´ë¡ </li>
      <li><a href="pages/gguf-format.html">GGUF ëª¨ë¸ í¬ë§·</a> - ì–‘ìí™” ê¸°ë²• ìƒì„¸</li>
      <li><a href="pages/local-api.html">ë¡œì»¬ LLM API</a> - vLLM ì‹¤ì „ í™œìš©</li>
    </ul>
  </div>
</section>

<!-- Page Navigation (ì´ì „/ë‹¤ìŒ) -->
<div class="page-nav"></div>

</main>

<!-- ===== TOC Sidebar -->
<aside class="inline-toc">
  <div class="toc-title">ëª©ì°¨</div>
  <div class="toc-nav"></div>
</aside>

<!-- ===== Footer ===== -->
<footer class="site-footer">
</footer>

</div>

<script src="../js/main.js"></script>
</body>
</html>
